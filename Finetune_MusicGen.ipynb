{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0PiDuP48UM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0b77c6-3e6f-4835-f856-25f089025eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount the drive to access data and store models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z8U5-Sb55Kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6aaeeb3-b0f1-48e9-e75b-e593278e87f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: audiocraft in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from audiocraft) (11.0.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from audiocraft) (0.7.0)\n",
            "Requirement already satisfied: flashy>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from audiocraft) (0.0.2)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (from audiocraft) (1.3.2)\n",
            "Requirement already satisfied: hydra-colorlog in /usr/local/lib/python3.10/dist-packages (from audiocraft) (1.2.0)\n",
            "Requirement already satisfied: julius in /usr/local/lib/python3.10/dist-packages (from audiocraft) (0.2.7)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (from audiocraft) (0.5.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from audiocraft) (1.23.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from audiocraft) (0.1.99)\n",
            "Requirement already satisfied: spacy==3.5.2 in /usr/local/lib/python3.10/dist-packages (from audiocraft) (3.5.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from audiocraft) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchaudio>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from audiocraft) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from audiocraft) (0.19.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from audiocraft) (4.66.1)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from audiocraft) (4.35.2)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (from audiocraft) (0.0.22.post7)\n",
            "Requirement already satisfied: demucs in /usr/local/lib/python3.10/dist-packages (from audiocraft) (4.0.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from audiocraft) (0.10.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (from audiocraft) (3.50.2)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from audiocraft) (1.2.1)\n",
            "Requirement already satisfied: encodec in /usr/local/lib/python3.10/dist-packages (from audiocraft) (0.1.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from audiocraft) (3.20.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (2.0.10)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.2->audiocraft) (3.3.0)\n",
            "Requirement already satisfied: dora-search in /usr/local/lib/python3.10/dist-packages (from flashy>=0.0.1->audiocraft) (0.1.12)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from flashy>=0.0.1->audiocraft) (6.8.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->audiocraft) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->audiocraft) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->audiocraft) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->audiocraft) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->audiocraft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->audiocraft) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->audiocraft) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->audiocraft) (2.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->audiocraft) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->audiocraft) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->audiocraft) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->audiocraft) (0.4.1)\n",
            "Requirement already satisfied: lameenc>=1.2 in /usr/local/lib/python3.10/dist-packages (from demucs->audiocraft) (1.7.0)\n",
            "Requirement already satisfied: openunmix in /usr/local/lib/python3.10/dist-packages (from demucs->audiocraft) (1.2.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (0.104.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.6.1 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (0.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (0.25.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (6.1.1)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (3.7.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (3.9.10)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (9.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (0.0.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (2.10.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (0.24.0.post1)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio->audiocraft) (11.0.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->audiocraft) (1.0.7)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words->audiocraft) (0.6.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics->audiocraft) (0.10.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->audiocraft) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->audiocraft) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->audiocraft) (0.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->audiocraft) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->audiocraft) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->audiocraft) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->audiocraft) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->audiocraft) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->audiocraft) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->audiocraft) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->audiocraft) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->audiocraft) (4.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.2->audiocraft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.2->audiocraft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.2->audiocraft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.2->audiocraft) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->audiocraft) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->audiocraft) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.2->audiocraft) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.2->audiocraft) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy==3.5.2->audiocraft) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio->audiocraft) (0.14.0)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from dora-search->flashy>=0.0.1->audiocraft) (1.3.4)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.10/dist-packages (from dora-search->flashy>=0.0.1->audiocraft) (1.5.1)\n",
            "Requirement already satisfied: treetable in /usr/local/lib/python3.10/dist-packages (from dora-search->flashy>=0.0.1->audiocraft) (0.2.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio->audiocraft) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio->audiocraft) (0.27.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->audiocraft) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->audiocraft) (1.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->audiocraft) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio->audiocraft) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->audiocraft) (2.21)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->audiocraft) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->audiocraft) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->audiocraft) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->audiocraft) (0.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio->audiocraft) (1.16.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->dora-search->flashy>=0.0.1->audiocraft) (2.2.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.38.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "# Install addtional requirements\n",
        "!pip install audiocraft\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E13_8NZ51lZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e483eb0-7027-4d80-b8f4-acd1fdd335a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
            "    Python  3.10.13 (you have 3.10.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "from audiocraft.models import MusicGen\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "import random\n",
        "import wandb\n",
        "from torch.utils.data import Dataset\n",
        "from audiocraft.modules.conditioners import ClassifierFreeGuidanceDropout\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py8VfSY76jBb"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, data_dir, no_label=False):\n",
        "        self.data_dir = data_dir\n",
        "        self.data_map = []\n",
        "\n",
        "        dir_map = os.listdir(data_dir)\n",
        "        for d in dir_map:\n",
        "            name, ext = os.path.splitext(d)\n",
        "            if ext == \".wav\":\n",
        "                if no_label:\n",
        "                    self.data_map.append({\"audio\": os.path.join(data_dir, d)})\n",
        "                    continue\n",
        "                if os.path.exists(os.path.join(data_dir, name + \".txt\")):\n",
        "                    self.data_map.append(\n",
        "                        {\n",
        "                            \"audio\": os.path.join(data_dir, d),\n",
        "                            \"label\": os.path.join(data_dir, name + \".txt\"),\n",
        "                        }\n",
        "                    )\n",
        "                else:\n",
        "                    raise ValueError(f\"No label file for {name}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_map)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data_map[idx]\n",
        "        audio = data[\"audio\"]\n",
        "        label = data.get(\"label\", \"\")\n",
        "\n",
        "        return audio, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqNo_5Ay6fal"
      },
      "outputs": [],
      "source": [
        "def count_nans(tensor):\n",
        "    nan_mask = torch.isnan(tensor)\n",
        "    num_nans = torch.sum(nan_mask).item()\n",
        "    return num_nans\n",
        "\n",
        "\n",
        "def preprocess_audio(audio_path, model: MusicGen, duration: int = 30):\n",
        "    wav, sr = torchaudio.load(audio_path)\n",
        "    wav = torchaudio.functional.resample(wav, sr, model.sample_rate)\n",
        "    wav = wav.mean(dim=0, keepdim=True)\n",
        "    if wav.shape[1] < model.sample_rate * duration:\n",
        "        return None\n",
        "    end_sample = int(model.sample_rate * duration)\n",
        "    start_sample = random.randrange(0, max(wav.shape[1] - end_sample, 1))\n",
        "    wav = wav[:, start_sample : start_sample + end_sample]\n",
        "\n",
        "    assert wav.shape[0] == 1\n",
        "\n",
        "    wav = wav.cuda()\n",
        "    wav = wav.unsqueeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_audio = model.compression_model.encode(wav)\n",
        "\n",
        "    codes, scale = gen_audio\n",
        "\n",
        "    assert scale is None\n",
        "\n",
        "    return codes\n",
        "\n",
        "\n",
        "def fixnan(tensor: torch.Tensor):\n",
        "    nan_mask = torch.isnan(tensor)\n",
        "    result = torch.where(nan_mask, torch.zeros_like(tensor), tensor)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def one_hot_encode(tensor, num_classes=2048):\n",
        "    shape = tensor.shape\n",
        "    one_hot = torch.zeros((shape[0], shape[1], num_classes))\n",
        "\n",
        "    for i in range(shape[0]):\n",
        "        for j in range(shape[1]):\n",
        "            index = tensor[i, j].item()\n",
        "            one_hot[i, j, index] = 1\n",
        "\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDAUqFIe5pyn"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    dataset_path: str,\n",
        "    save_path: str,\n",
        "    model_id: str,\n",
        "    lr: float,\n",
        "    epochs: int,\n",
        "    use_wandb: bool,\n",
        "    no_label: bool = False,\n",
        "    tune_text: bool = False,\n",
        "    save_step: int = None,\n",
        "    grad_acc: int = 8,\n",
        "    use_scaler: bool = False,\n",
        "    weight_decay: float = 1e-5,\n",
        "    warmup_steps: int = 10,\n",
        "    batch_size: int = 10,\n",
        "    use_cfg: bool = False,\n",
        "):\n",
        "    if use_wandb:\n",
        "        run = wandb.init(project=\"audiocraft\")\n",
        "\n",
        "    model = MusicGen.get_pretrained(model_id)\n",
        "    model.lm = model.lm.to(torch.float32)  # important\n",
        "\n",
        "    dataset = AudioDataset(dataset_path, no_label=no_label)\n",
        "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    learning_rate = lr\n",
        "    model.lm.train()\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    if tune_text:\n",
        "        print(\"Tuning text\")\n",
        "    else:\n",
        "        print(\"Tuning everything\")\n",
        "\n",
        "    # from paper\n",
        "    optimizer = AdamW(\n",
        "        model.lm.condition_provider.parameters()\n",
        "        if tune_text\n",
        "        else model.lm.parameters(),\n",
        "        lr=learning_rate,\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "    scheduler = get_scheduler(\n",
        "        \"cosine\",\n",
        "        optimizer,\n",
        "        warmup_steps,\n",
        "        int(epochs * len(train_dataloader) / grad_acc),\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    num_epochs = epochs\n",
        "\n",
        "    save_step = save_step\n",
        "    save_models = False if save_step is None else True\n",
        "\n",
        "    # save_path = \"models/\"\n",
        "\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    current_step = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (audio, label) in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            all_codes = []\n",
        "            texts = []\n",
        "\n",
        "            # where audio and label are just paths\n",
        "            for inner_audio, l in zip(audio, label):\n",
        "            # for inner_audio in audio:\n",
        "                inner_audio = preprocess_audio(inner_audio, model)  # returns tensor\n",
        "                if inner_audio is None:\n",
        "                    continue\n",
        "\n",
        "                if use_cfg:\n",
        "                    codes = torch.cat([inner_audio, inner_audio], dim=0)\n",
        "                else:\n",
        "                    codes = inner_audio\n",
        "\n",
        "                all_codes.append(codes)\n",
        "                texts.append(open(l, \"r\").read().strip())\n",
        "\n",
        "            attributes, _ = model._prepare_tokens_and_attributes(texts, None)\n",
        "            conditions = attributes\n",
        "            if use_cfg:\n",
        "                null_conditions = ClassifierFreeGuidanceDropout(p=1.0)(conditions)\n",
        "                conditions = conditions + null_conditions\n",
        "            tokenized = model.lm.condition_provider.tokenize(conditions)\n",
        "            cfg_conditions = model.lm.condition_provider(tokenized)\n",
        "            condition_tensors = cfg_conditions\n",
        "\n",
        "            if len(all_codes) == 0:\n",
        "                continue\n",
        "\n",
        "            codes = torch.cat(all_codes, dim=0)\n",
        "\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                lm_output = model.lm.compute_predictions(\n",
        "                    codes=codes, conditions=[], condition_tensors=condition_tensors\n",
        "                )\n",
        "                # lm_output = model.lm.compute_predictions(\n",
        "                #     codes=codes, conditions=[], condition_tensors=None\n",
        "                # )\n",
        "\n",
        "                codes = codes[0]\n",
        "                logits = lm_output.logits[0]\n",
        "                mask = lm_output.mask[0]\n",
        "\n",
        "                codes = one_hot_encode(codes, num_classes=2048)\n",
        "\n",
        "                codes = codes.cuda()\n",
        "                logits = logits.cuda()\n",
        "                mask = mask.cuda()\n",
        "\n",
        "                mask = mask.view(-1)\n",
        "                masked_logits = logits.view(-1, 2048)[mask]\n",
        "                masked_codes = codes.view(-1, 2048)[mask]\n",
        "\n",
        "                loss = criterion(masked_logits, masked_codes)\n",
        "\n",
        "            current_step += 1 / grad_acc\n",
        "\n",
        "            # assert count_nans(masked_logits) == 0\n",
        "\n",
        "            (scaler.scale(loss) if use_scaler else loss).backward()\n",
        "\n",
        "            total_norm = 0\n",
        "            for p in model.lm.condition_provider.parameters():\n",
        "                try:\n",
        "                    param_norm = p.grad.data.norm(2)\n",
        "                    total_norm += param_norm.item() ** 2\n",
        "                except AttributeError:\n",
        "                    pass\n",
        "            total_norm = total_norm ** (1.0 / 2)\n",
        "\n",
        "            if use_wandb:\n",
        "                run.log(\n",
        "                    {\n",
        "                        \"loss\": loss.item(),\n",
        "                        \"total_norm\": total_norm,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            print(\n",
        "                f\"Epoch: {epoch}/{num_epochs}, Batch: {batch_idx}/{len(train_dataloader)}, Loss: {loss.item()}\"\n",
        "            )\n",
        "\n",
        "            if batch_idx % grad_acc != grad_acc - 1:\n",
        "                continue\n",
        "\n",
        "            if use_scaler:\n",
        "                scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.lm.parameters(), 0.5)\n",
        "\n",
        "            if use_scaler:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if save_models:\n",
        "                if (\n",
        "                    current_step == int(current_step)\n",
        "                    and int(current_step) % save_step == 0\n",
        "                ):\n",
        "                    torch.save(\n",
        "                        model.lm.state_dict(), f\"{save_path}/lm_{current_step}.pt\"\n",
        "                    )\n",
        "        torch.save(\n",
        "             model.lm.state_dict(), f\"{save_path}/lm_epoch_{epoch}.pt\"\n",
        "        )\n",
        "\n",
        "    torch.save(model.lm.state_dict(), f\"{save_path}/lm_final.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLTnVLDg7LEA"
      },
      "outputs": [],
      "source": [
        "experiment_args = {\n",
        "    \"dataset_path\":\"/content/drive/MyDrive/jazz dataset/wav_output\",\n",
        "    \"save_path\": \"/content/drive/MyDrive/jazz dataset/models_output\",\n",
        "    \"model_id\":\"small\",\n",
        "    \"lr\":1e-5,\n",
        "    \"epochs\":30,\n",
        "    \"use_wandb\":False,\n",
        "    \"save_step\":91,\n",
        "    \"no_label\":False,\n",
        "    \"tune_text\":False,\n",
        "    \"weight_decay\":1e-5,\n",
        "    \"grad_acc\":2,\n",
        "    \"warmup_steps\":16,\n",
        "    \"batch_size\":2,\n",
        "    \"use_cfg\":False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_wHRx0d7FMh",
        "outputId": "e909360f-7907-4f24-b99e-4a94af05935f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/audiocraft/models/musicgen.py:116: UserWarning: MusicGen pretrained model relying on deprecated checkpoint mapping. Please use full pre-trained id instead: facebook/musicgen-small\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning everything\n",
            "Epoch: 0/30, Batch: 0/91, Loss: 3.841639280319214\n",
            "Epoch: 0/30, Batch: 1/91, Loss: 3.8847475051879883\n",
            "Epoch: 0/30, Batch: 2/91, Loss: 3.0169878005981445\n",
            "Epoch: 0/30, Batch: 3/91, Loss: 4.431493282318115\n",
            "Epoch: 0/30, Batch: 4/91, Loss: 3.6072967052459717\n",
            "Epoch: 0/30, Batch: 5/91, Loss: 4.216796398162842\n",
            "Epoch: 0/30, Batch: 6/91, Loss: 3.468733549118042\n",
            "Epoch: 0/30, Batch: 7/91, Loss: 3.5692131519317627\n",
            "Epoch: 0/30, Batch: 8/91, Loss: 4.021340847015381\n",
            "Epoch: 0/30, Batch: 9/91, Loss: 4.509680271148682\n",
            "Epoch: 0/30, Batch: 10/91, Loss: 3.8295698165893555\n",
            "Epoch: 0/30, Batch: 11/91, Loss: 2.893702268600464\n",
            "Epoch: 0/30, Batch: 12/91, Loss: 4.7225518226623535\n",
            "Epoch: 0/30, Batch: 13/91, Loss: 4.01603889465332\n",
            "Epoch: 0/30, Batch: 14/91, Loss: 4.493042469024658\n",
            "Epoch: 0/30, Batch: 15/91, Loss: 3.9741151332855225\n",
            "Epoch: 0/30, Batch: 16/91, Loss: 5.119402885437012\n",
            "Epoch: 0/30, Batch: 17/91, Loss: 4.419198513031006\n",
            "Epoch: 0/30, Batch: 18/91, Loss: 4.255405426025391\n",
            "Epoch: 0/30, Batch: 19/91, Loss: 4.1274333000183105\n",
            "Epoch: 0/30, Batch: 20/91, Loss: 4.071962356567383\n",
            "Epoch: 0/30, Batch: 21/91, Loss: 3.802748680114746\n",
            "Epoch: 0/30, Batch: 22/91, Loss: 4.371151447296143\n",
            "Epoch: 0/30, Batch: 23/91, Loss: 5.339971542358398\n",
            "Epoch: 0/30, Batch: 24/91, Loss: 4.5439887046813965\n",
            "Epoch: 0/30, Batch: 25/91, Loss: 4.702807903289795\n",
            "Epoch: 0/30, Batch: 26/91, Loss: 4.379505634307861\n",
            "Epoch: 0/30, Batch: 27/91, Loss: 4.885584354400635\n",
            "Epoch: 0/30, Batch: 28/91, Loss: 4.288511753082275\n",
            "Epoch: 0/30, Batch: 29/91, Loss: 4.0648064613342285\n",
            "Epoch: 0/30, Batch: 30/91, Loss: 3.276003360748291\n",
            "Epoch: 0/30, Batch: 31/91, Loss: 5.146886348724365\n",
            "Epoch: 0/30, Batch: 32/91, Loss: 4.501717567443848\n",
            "Epoch: 0/30, Batch: 33/91, Loss: 4.3786516189575195\n",
            "Epoch: 0/30, Batch: 34/91, Loss: 3.9658994674682617\n",
            "Epoch: 0/30, Batch: 35/91, Loss: 4.947749137878418\n",
            "Epoch: 0/30, Batch: 36/91, Loss: 4.10096549987793\n",
            "Epoch: 0/30, Batch: 37/91, Loss: 3.546050548553467\n",
            "Epoch: 0/30, Batch: 38/91, Loss: 3.872133255004883\n",
            "Epoch: 0/30, Batch: 39/91, Loss: 3.824493169784546\n",
            "Epoch: 0/30, Batch: 40/91, Loss: 4.446794509887695\n",
            "Epoch: 0/30, Batch: 41/91, Loss: 2.1503052711486816\n",
            "Epoch: 0/30, Batch: 42/91, Loss: 4.115926742553711\n",
            "Epoch: 0/30, Batch: 43/91, Loss: 4.414968490600586\n",
            "Epoch: 0/30, Batch: 44/91, Loss: 5.012418746948242\n",
            "Epoch: 0/30, Batch: 45/91, Loss: 3.9832563400268555\n",
            "Epoch: 0/30, Batch: 46/91, Loss: 4.179959297180176\n",
            "Epoch: 0/30, Batch: 47/91, Loss: 4.067443370819092\n",
            "Epoch: 0/30, Batch: 48/91, Loss: 3.6076583862304688\n",
            "Epoch: 0/30, Batch: 49/91, Loss: 3.825099229812622\n",
            "Epoch: 0/30, Batch: 50/91, Loss: 4.443595886230469\n",
            "Epoch: 0/30, Batch: 51/91, Loss: 4.410455226898193\n",
            "Epoch: 0/30, Batch: 52/91, Loss: 3.859323263168335\n",
            "Epoch: 0/30, Batch: 53/91, Loss: 5.026913166046143\n",
            "Epoch: 0/30, Batch: 54/91, Loss: 4.125977516174316\n",
            "Epoch: 0/30, Batch: 55/91, Loss: 4.986186981201172\n",
            "Epoch: 0/30, Batch: 56/91, Loss: 4.379823684692383\n",
            "Epoch: 0/30, Batch: 57/91, Loss: 4.054049968719482\n",
            "Epoch: 0/30, Batch: 58/91, Loss: 3.923508882522583\n",
            "Epoch: 0/30, Batch: 59/91, Loss: 4.600826263427734\n",
            "Epoch: 0/30, Batch: 60/91, Loss: 3.4176957607269287\n",
            "Epoch: 0/30, Batch: 61/91, Loss: 3.0841212272644043\n",
            "Epoch: 0/30, Batch: 62/91, Loss: 3.789271116256714\n",
            "Epoch: 0/30, Batch: 63/91, Loss: 3.8302948474884033\n",
            "Epoch: 0/30, Batch: 64/91, Loss: 4.610965728759766\n",
            "Epoch: 0/30, Batch: 65/91, Loss: 3.239264726638794\n",
            "Epoch: 0/30, Batch: 66/91, Loss: 4.333242893218994\n",
            "Epoch: 0/30, Batch: 67/91, Loss: 4.460886478424072\n",
            "Epoch: 0/30, Batch: 68/91, Loss: 4.498374938964844\n",
            "Epoch: 0/30, Batch: 69/91, Loss: 4.039809226989746\n",
            "Epoch: 0/30, Batch: 70/91, Loss: 4.799944877624512\n",
            "Epoch: 0/30, Batch: 71/91, Loss: 4.789745807647705\n",
            "Epoch: 0/30, Batch: 72/91, Loss: 3.537238121032715\n",
            "Epoch: 0/30, Batch: 73/91, Loss: 4.0121169090271\n",
            "Epoch: 0/30, Batch: 74/91, Loss: 3.8217508792877197\n",
            "Epoch: 0/30, Batch: 75/91, Loss: 3.9279396533966064\n",
            "Epoch: 0/30, Batch: 76/91, Loss: 3.6693732738494873\n",
            "Epoch: 0/30, Batch: 77/91, Loss: 4.116117000579834\n",
            "Epoch: 0/30, Batch: 78/91, Loss: 4.71580696105957\n",
            "Epoch: 0/30, Batch: 79/91, Loss: 4.829145431518555\n",
            "Epoch: 0/30, Batch: 80/91, Loss: 4.866973876953125\n",
            "Epoch: 0/30, Batch: 81/91, Loss: 4.233583450317383\n",
            "Epoch: 0/30, Batch: 82/91, Loss: 3.670844078063965\n",
            "Epoch: 0/30, Batch: 83/91, Loss: 4.326568126678467\n",
            "Epoch: 0/30, Batch: 84/91, Loss: 4.705073356628418\n",
            "Epoch: 0/30, Batch: 85/91, Loss: 4.554031848907471\n",
            "Epoch: 0/30, Batch: 86/91, Loss: 3.759801149368286\n",
            "Epoch: 0/30, Batch: 87/91, Loss: 4.570055961608887\n",
            "Epoch: 0/30, Batch: 88/91, Loss: 4.286117076873779\n",
            "Epoch: 0/30, Batch: 89/91, Loss: 4.690544128417969\n",
            "Epoch: 0/30, Batch: 90/91, Loss: 4.492106914520264\n",
            "Epoch: 1/30, Batch: 0/91, Loss: 3.82914137840271\n",
            "Epoch: 1/30, Batch: 1/91, Loss: 3.9231417179107666\n",
            "Epoch: 1/30, Batch: 2/91, Loss: 3.8634517192840576\n",
            "Epoch: 1/30, Batch: 3/91, Loss: 4.175173282623291\n",
            "Epoch: 1/30, Batch: 4/91, Loss: 5.0707011222839355\n",
            "Epoch: 1/30, Batch: 5/91, Loss: 4.810079097747803\n",
            "Epoch: 1/30, Batch: 6/91, Loss: 4.104226589202881\n",
            "Epoch: 1/30, Batch: 7/91, Loss: 4.869865417480469\n",
            "Epoch: 1/30, Batch: 8/91, Loss: 3.8912360668182373\n",
            "Epoch: 1/30, Batch: 9/91, Loss: 4.356477737426758\n",
            "Epoch: 1/30, Batch: 10/91, Loss: 4.451723098754883\n",
            "Epoch: 1/30, Batch: 11/91, Loss: 5.034727573394775\n",
            "Epoch: 1/30, Batch: 12/91, Loss: 4.770328998565674\n",
            "Epoch: 1/30, Batch: 13/91, Loss: 4.189921855926514\n",
            "Epoch: 1/30, Batch: 14/91, Loss: 4.201640605926514\n",
            "Epoch: 1/30, Batch: 15/91, Loss: 4.480502605438232\n",
            "Epoch: 1/30, Batch: 16/91, Loss: 3.5124969482421875\n",
            "Epoch: 1/30, Batch: 17/91, Loss: 4.1241044998168945\n",
            "Epoch: 1/30, Batch: 18/91, Loss: 4.8094000816345215\n",
            "Epoch: 1/30, Batch: 19/91, Loss: 3.3186135292053223\n",
            "Epoch: 1/30, Batch: 20/91, Loss: 4.581486225128174\n",
            "Epoch: 1/30, Batch: 21/91, Loss: 4.540952682495117\n",
            "Epoch: 1/30, Batch: 22/91, Loss: 4.047223091125488\n",
            "Epoch: 1/30, Batch: 23/91, Loss: 4.593244552612305\n",
            "Epoch: 1/30, Batch: 24/91, Loss: 4.452507972717285\n",
            "Epoch: 1/30, Batch: 25/91, Loss: 5.064719200134277\n",
            "Epoch: 1/30, Batch: 26/91, Loss: 3.802994728088379\n",
            "Epoch: 1/30, Batch: 27/91, Loss: 4.5782999992370605\n",
            "Epoch: 1/30, Batch: 28/91, Loss: 3.7524709701538086\n",
            "Epoch: 1/30, Batch: 29/91, Loss: 2.311610221862793\n",
            "Epoch: 1/30, Batch: 30/91, Loss: 4.3250885009765625\n",
            "Epoch: 1/30, Batch: 31/91, Loss: 4.462810039520264\n",
            "Epoch: 1/30, Batch: 32/91, Loss: 3.663480043411255\n",
            "Epoch: 1/30, Batch: 33/91, Loss: 4.156571865081787\n",
            "Epoch: 1/30, Batch: 34/91, Loss: 4.115370750427246\n",
            "Epoch: 1/30, Batch: 35/91, Loss: 5.013522148132324\n",
            "Epoch: 1/30, Batch: 36/91, Loss: 4.53674840927124\n",
            "Epoch: 1/30, Batch: 37/91, Loss: 4.692728519439697\n",
            "Epoch: 1/30, Batch: 38/91, Loss: 4.313212871551514\n",
            "Epoch: 1/30, Batch: 39/91, Loss: 4.9141435623168945\n",
            "Epoch: 1/30, Batch: 40/91, Loss: 4.501908779144287\n",
            "Epoch: 1/30, Batch: 41/91, Loss: 4.543106555938721\n",
            "Epoch: 1/30, Batch: 42/91, Loss: 4.571359634399414\n",
            "Epoch: 1/30, Batch: 43/91, Loss: 3.998792886734009\n",
            "Epoch: 1/30, Batch: 44/91, Loss: 4.239432334899902\n",
            "Epoch: 1/30, Batch: 45/91, Loss: 4.736468315124512\n",
            "Epoch: 1/30, Batch: 46/91, Loss: 4.163188457489014\n",
            "Epoch: 1/30, Batch: 47/91, Loss: 2.913964033126831\n",
            "Epoch: 1/30, Batch: 48/91, Loss: 3.8548338413238525\n",
            "Epoch: 1/30, Batch: 49/91, Loss: 3.89491605758667\n",
            "Epoch: 1/30, Batch: 50/91, Loss: 4.328690052032471\n",
            "Epoch: 1/30, Batch: 51/91, Loss: 3.895540237426758\n",
            "Epoch: 1/30, Batch: 52/91, Loss: 4.04153299331665\n",
            "Epoch: 1/30, Batch: 53/91, Loss: 4.718746662139893\n",
            "Epoch: 1/30, Batch: 54/91, Loss: 4.597723960876465\n",
            "Epoch: 1/30, Batch: 55/91, Loss: 3.702892303466797\n",
            "Epoch: 1/30, Batch: 56/91, Loss: 4.001441478729248\n",
            "Epoch: 1/30, Batch: 57/91, Loss: 4.4027419090271\n",
            "Epoch: 1/30, Batch: 58/91, Loss: 4.727598667144775\n",
            "Epoch: 1/30, Batch: 59/91, Loss: 3.9492807388305664\n",
            "Epoch: 1/30, Batch: 60/91, Loss: 4.301480770111084\n",
            "Epoch: 1/30, Batch: 61/91, Loss: 4.351208686828613\n",
            "Epoch: 1/30, Batch: 62/91, Loss: 4.241401672363281\n",
            "Epoch: 1/30, Batch: 63/91, Loss: 3.6230931282043457\n",
            "Epoch: 1/30, Batch: 64/91, Loss: 5.00438928604126\n",
            "Epoch: 1/30, Batch: 65/91, Loss: 3.8273212909698486\n",
            "Epoch: 1/30, Batch: 66/91, Loss: 4.535102844238281\n",
            "Epoch: 1/30, Batch: 67/91, Loss: 3.5940427780151367\n",
            "Epoch: 1/30, Batch: 68/91, Loss: 4.5931077003479\n",
            "Epoch: 1/30, Batch: 69/91, Loss: 4.131443500518799\n",
            "Epoch: 1/30, Batch: 70/91, Loss: 4.240200996398926\n",
            "Epoch: 1/30, Batch: 71/91, Loss: 4.565814018249512\n",
            "Epoch: 1/30, Batch: 72/91, Loss: 4.248465538024902\n",
            "Epoch: 1/30, Batch: 73/91, Loss: 5.121610164642334\n",
            "Epoch: 1/30, Batch: 74/91, Loss: 3.8525729179382324\n",
            "Epoch: 1/30, Batch: 75/91, Loss: 4.369060039520264\n",
            "Epoch: 1/30, Batch: 76/91, Loss: 5.02210807800293\n",
            "Epoch: 1/30, Batch: 77/91, Loss: 5.264712810516357\n",
            "Epoch: 1/30, Batch: 78/91, Loss: 3.1355934143066406\n",
            "Epoch: 1/30, Batch: 79/91, Loss: 3.4018797874450684\n",
            "Epoch: 1/30, Batch: 80/91, Loss: 4.634230613708496\n",
            "Epoch: 1/30, Batch: 81/91, Loss: 2.5067551136016846\n",
            "Epoch: 1/30, Batch: 82/91, Loss: 4.337657451629639\n",
            "Epoch: 1/30, Batch: 83/91, Loss: 4.337010860443115\n",
            "Epoch: 1/30, Batch: 84/91, Loss: 4.369187831878662\n",
            "Epoch: 1/30, Batch: 85/91, Loss: 3.7504923343658447\n",
            "Epoch: 1/30, Batch: 86/91, Loss: 3.8324949741363525\n",
            "Epoch: 1/30, Batch: 87/91, Loss: 4.322261333465576\n",
            "Epoch: 1/30, Batch: 88/91, Loss: 4.978550434112549\n",
            "Epoch: 1/30, Batch: 89/91, Loss: 4.312444686889648\n",
            "Epoch: 1/30, Batch: 90/91, Loss: 4.412748336791992\n",
            "Epoch: 2/30, Batch: 0/91, Loss: 3.702573776245117\n",
            "Epoch: 2/30, Batch: 1/91, Loss: 4.950681686401367\n",
            "Epoch: 2/30, Batch: 2/91, Loss: 4.321622371673584\n",
            "Epoch: 2/30, Batch: 3/91, Loss: 4.780255317687988\n",
            "Epoch: 2/30, Batch: 4/91, Loss: 4.470676898956299\n",
            "Epoch: 2/30, Batch: 5/91, Loss: 4.433986663818359\n",
            "Epoch: 2/30, Batch: 6/91, Loss: 2.8598899841308594\n",
            "Epoch: 2/30, Batch: 7/91, Loss: 4.165221214294434\n",
            "Epoch: 2/30, Batch: 8/91, Loss: 4.509207248687744\n",
            "Epoch: 2/30, Batch: 9/91, Loss: 4.8277587890625\n",
            "Epoch: 2/30, Batch: 10/91, Loss: 4.585398197174072\n",
            "Epoch: 2/30, Batch: 11/91, Loss: 4.368871688842773\n",
            "Epoch: 2/30, Batch: 12/91, Loss: 3.839057207107544\n",
            "Epoch: 2/30, Batch: 13/91, Loss: 4.020135402679443\n",
            "Epoch: 2/30, Batch: 14/91, Loss: 3.9127213954925537\n",
            "Epoch: 2/30, Batch: 15/91, Loss: 4.493128776550293\n",
            "Epoch: 2/30, Batch: 16/91, Loss: 2.9963674545288086\n",
            "Epoch: 2/30, Batch: 17/91, Loss: 4.338552951812744\n",
            "Epoch: 2/30, Batch: 18/91, Loss: 3.701432228088379\n",
            "Epoch: 2/30, Batch: 19/91, Loss: 4.546468257904053\n",
            "Epoch: 2/30, Batch: 20/91, Loss: 4.530128002166748\n",
            "Epoch: 2/30, Batch: 21/91, Loss: 4.215737342834473\n",
            "Epoch: 2/30, Batch: 22/91, Loss: 4.414452075958252\n",
            "Epoch: 2/30, Batch: 23/91, Loss: 4.0948920249938965\n",
            "Epoch: 2/30, Batch: 24/91, Loss: 3.7811243534088135\n",
            "Epoch: 2/30, Batch: 25/91, Loss: 4.478942394256592\n",
            "Epoch: 2/30, Batch: 26/91, Loss: 5.130602836608887\n",
            "Epoch: 2/30, Batch: 27/91, Loss: 4.411677360534668\n",
            "Epoch: 2/30, Batch: 28/91, Loss: 4.221008777618408\n",
            "Epoch: 2/30, Batch: 29/91, Loss: 3.8938448429107666\n",
            "Epoch: 2/30, Batch: 30/91, Loss: 4.652340412139893\n",
            "Epoch: 2/30, Batch: 31/91, Loss: 4.196187973022461\n",
            "Epoch: 2/30, Batch: 32/91, Loss: 4.35469388961792\n",
            "Epoch: 2/30, Batch: 33/91, Loss: 3.6817970275878906\n",
            "Epoch: 2/30, Batch: 34/91, Loss: 3.9037983417510986\n",
            "Epoch: 2/30, Batch: 35/91, Loss: 2.847888946533203\n",
            "Epoch: 2/30, Batch: 36/91, Loss: 4.096766471862793\n",
            "Epoch: 2/30, Batch: 37/91, Loss: 4.211550712585449\n",
            "Epoch: 2/30, Batch: 38/91, Loss: 4.997036933898926\n",
            "Epoch: 2/30, Batch: 39/91, Loss: 4.852345943450928\n",
            "Epoch: 2/30, Batch: 40/91, Loss: 4.773013114929199\n",
            "Epoch: 2/30, Batch: 41/91, Loss: 4.4497389793396\n",
            "Epoch: 2/30, Batch: 42/91, Loss: 4.116868495941162\n",
            "Epoch: 2/30, Batch: 43/91, Loss: 4.72808313369751\n",
            "Epoch: 2/30, Batch: 44/91, Loss: 4.822929859161377\n",
            "Epoch: 2/30, Batch: 45/91, Loss: 3.6491899490356445\n",
            "Epoch: 2/30, Batch: 46/91, Loss: 4.569197654724121\n",
            "Epoch: 2/30, Batch: 47/91, Loss: 4.069584846496582\n",
            "Epoch: 2/30, Batch: 48/91, Loss: 4.564182758331299\n",
            "Epoch: 2/30, Batch: 49/91, Loss: 5.350852012634277\n",
            "Epoch: 2/30, Batch: 50/91, Loss: 4.058516979217529\n",
            "Epoch: 2/30, Batch: 51/91, Loss: 4.27238655090332\n",
            "Epoch: 2/30, Batch: 52/91, Loss: 4.449300765991211\n",
            "Epoch: 2/30, Batch: 53/91, Loss: 4.15356969833374\n",
            "Epoch: 2/30, Batch: 54/91, Loss: 4.743388652801514\n",
            "Epoch: 2/30, Batch: 55/91, Loss: 4.005427837371826\n",
            "Epoch: 2/30, Batch: 56/91, Loss: 3.6083824634552\n",
            "Epoch: 2/30, Batch: 57/91, Loss: 4.939876079559326\n",
            "Epoch: 2/30, Batch: 58/91, Loss: 3.111736536026001\n",
            "Epoch: 2/30, Batch: 59/91, Loss: 4.561490058898926\n",
            "Epoch: 2/30, Batch: 60/91, Loss: 4.819705963134766\n",
            "Epoch: 2/30, Batch: 61/91, Loss: 4.530627250671387\n",
            "Epoch: 2/30, Batch: 62/91, Loss: 4.2816386222839355\n",
            "Epoch: 2/30, Batch: 63/91, Loss: 4.357692718505859\n",
            "Epoch: 2/30, Batch: 64/91, Loss: 3.7859878540039062\n",
            "Epoch: 2/30, Batch: 65/91, Loss: 3.9543848037719727\n",
            "Epoch: 2/30, Batch: 66/91, Loss: 5.111216068267822\n",
            "Epoch: 2/30, Batch: 67/91, Loss: 4.411963939666748\n",
            "Epoch: 2/30, Batch: 68/91, Loss: 4.1854071617126465\n",
            "Epoch: 2/30, Batch: 69/91, Loss: 4.574305534362793\n",
            "Epoch: 2/30, Batch: 70/91, Loss: 2.949462413787842\n",
            "Epoch: 2/30, Batch: 71/91, Loss: 4.991304397583008\n",
            "Epoch: 2/30, Batch: 72/91, Loss: 2.1434123516082764\n",
            "Epoch: 2/30, Batch: 73/91, Loss: 4.89037561416626\n",
            "Epoch: 2/30, Batch: 74/91, Loss: 4.039083957672119\n",
            "Epoch: 2/30, Batch: 75/91, Loss: 4.661258220672607\n",
            "Epoch: 2/30, Batch: 76/91, Loss: 4.354897499084473\n",
            "Epoch: 2/30, Batch: 77/91, Loss: 3.9129650592803955\n",
            "Epoch: 2/30, Batch: 78/91, Loss: 4.252211093902588\n",
            "Epoch: 2/30, Batch: 79/91, Loss: 3.9143741130828857\n",
            "Epoch: 2/30, Batch: 80/91, Loss: 4.471622943878174\n",
            "Epoch: 2/30, Batch: 81/91, Loss: 3.7930307388305664\n",
            "Epoch: 2/30, Batch: 82/91, Loss: 4.312441349029541\n",
            "Epoch: 2/30, Batch: 83/91, Loss: 3.8115010261535645\n",
            "Epoch: 2/30, Batch: 84/91, Loss: 4.336472988128662\n",
            "Epoch: 2/30, Batch: 85/91, Loss: 4.3265862464904785\n",
            "Epoch: 2/30, Batch: 86/91, Loss: 3.5870063304901123\n",
            "Epoch: 2/30, Batch: 87/91, Loss: 4.730311870574951\n",
            "Epoch: 2/30, Batch: 88/91, Loss: 4.811784744262695\n",
            "Epoch: 2/30, Batch: 89/91, Loss: 4.365828037261963\n",
            "Epoch: 2/30, Batch: 90/91, Loss: 4.614877700805664\n",
            "Epoch: 3/30, Batch: 0/91, Loss: 4.449127674102783\n",
            "Epoch: 3/30, Batch: 1/91, Loss: 5.071221351623535\n",
            "Epoch: 3/30, Batch: 2/91, Loss: 4.547269821166992\n",
            "Epoch: 3/30, Batch: 3/91, Loss: 4.566206455230713\n",
            "Epoch: 3/30, Batch: 4/91, Loss: 2.801995038986206\n",
            "Epoch: 3/30, Batch: 5/91, Loss: 4.778924942016602\n",
            "Epoch: 3/30, Batch: 6/91, Loss: 2.836292028427124\n",
            "Epoch: 3/30, Batch: 7/91, Loss: 4.245996952056885\n",
            "Epoch: 3/30, Batch: 8/91, Loss: 4.513186931610107\n",
            "Epoch: 3/30, Batch: 9/91, Loss: 3.7506296634674072\n",
            "Epoch: 3/30, Batch: 10/91, Loss: 3.773737907409668\n",
            "Epoch: 3/30, Batch: 11/91, Loss: 4.083475589752197\n",
            "Epoch: 3/30, Batch: 12/91, Loss: 4.568129062652588\n",
            "Epoch: 3/30, Batch: 13/91, Loss: 3.151477336883545\n",
            "Epoch: 3/30, Batch: 14/91, Loss: 5.185914993286133\n",
            "Epoch: 3/30, Batch: 15/91, Loss: 4.742825031280518\n",
            "Epoch: 3/30, Batch: 16/91, Loss: 4.247695446014404\n",
            "Epoch: 3/30, Batch: 17/91, Loss: 3.013343572616577\n",
            "Epoch: 3/30, Batch: 18/91, Loss: 4.242353439331055\n",
            "Epoch: 3/30, Batch: 19/91, Loss: 4.0762834548950195\n",
            "Epoch: 3/30, Batch: 20/91, Loss: 4.342703342437744\n",
            "Epoch: 3/30, Batch: 21/91, Loss: 4.344797134399414\n",
            "Epoch: 3/30, Batch: 22/91, Loss: 4.359683990478516\n",
            "Epoch: 3/30, Batch: 23/91, Loss: 4.3584303855896\n",
            "Epoch: 3/30, Batch: 24/91, Loss: 4.062657356262207\n",
            "Epoch: 3/30, Batch: 25/91, Loss: 4.543335437774658\n",
            "Epoch: 3/30, Batch: 26/91, Loss: 4.292847633361816\n",
            "Epoch: 3/30, Batch: 27/91, Loss: 4.434638977050781\n",
            "Epoch: 3/30, Batch: 28/91, Loss: 3.5105814933776855\n",
            "Epoch: 3/30, Batch: 29/91, Loss: 4.6960673332214355\n",
            "Epoch: 3/30, Batch: 30/91, Loss: 4.023820400238037\n",
            "Epoch: 3/30, Batch: 31/91, Loss: 4.379000186920166\n",
            "Epoch: 3/30, Batch: 32/91, Loss: 4.110514163970947\n",
            "Epoch: 3/30, Batch: 33/91, Loss: 3.6988773345947266\n",
            "Epoch: 3/30, Batch: 34/91, Loss: 4.811489105224609\n",
            "Epoch: 3/30, Batch: 35/91, Loss: 4.835900783538818\n",
            "Epoch: 3/30, Batch: 36/91, Loss: 4.715290069580078\n",
            "Epoch: 3/30, Batch: 37/91, Loss: 3.9351541996002197\n",
            "Epoch: 3/30, Batch: 38/91, Loss: 3.043410062789917\n",
            "Epoch: 3/30, Batch: 39/91, Loss: 3.972142219543457\n",
            "Epoch: 3/30, Batch: 40/91, Loss: 4.549255847930908\n",
            "Epoch: 3/30, Batch: 41/91, Loss: 4.850461959838867\n",
            "Epoch: 3/30, Batch: 42/91, Loss: 4.859856605529785\n",
            "Epoch: 3/30, Batch: 43/91, Loss: 4.799323558807373\n",
            "Epoch: 3/30, Batch: 44/91, Loss: 4.143578052520752\n",
            "Epoch: 3/30, Batch: 45/91, Loss: 3.3093602657318115\n",
            "Epoch: 3/30, Batch: 46/91, Loss: 4.917226791381836\n",
            "Epoch: 3/30, Batch: 47/91, Loss: 3.7803380489349365\n",
            "Epoch: 3/30, Batch: 48/91, Loss: 3.587902784347534\n",
            "Epoch: 3/30, Batch: 49/91, Loss: 5.180966377258301\n",
            "Epoch: 3/30, Batch: 50/91, Loss: 4.517220497131348\n",
            "Epoch: 3/30, Batch: 51/91, Loss: 2.6781976222991943\n",
            "Epoch: 3/30, Batch: 52/91, Loss: 4.623232841491699\n",
            "Epoch: 3/30, Batch: 53/91, Loss: 3.919877052307129\n",
            "Epoch: 3/30, Batch: 54/91, Loss: 3.826251983642578\n",
            "Epoch: 3/30, Batch: 55/91, Loss: 4.325402736663818\n",
            "Epoch: 3/30, Batch: 56/91, Loss: 4.364445209503174\n",
            "Epoch: 3/30, Batch: 57/91, Loss: 4.076935291290283\n",
            "Epoch: 3/30, Batch: 58/91, Loss: 4.4167890548706055\n",
            "Epoch: 3/30, Batch: 59/91, Loss: 4.400815963745117\n",
            "Epoch: 3/30, Batch: 60/91, Loss: 4.036232948303223\n",
            "Epoch: 3/30, Batch: 61/91, Loss: 4.667723655700684\n",
            "Epoch: 3/30, Batch: 62/91, Loss: 4.678609848022461\n",
            "Epoch: 3/30, Batch: 63/91, Loss: 4.27168083190918\n",
            "Epoch: 3/30, Batch: 64/91, Loss: 4.499499797821045\n",
            "Epoch: 3/30, Batch: 65/91, Loss: 4.954026699066162\n",
            "Epoch: 3/30, Batch: 66/91, Loss: 4.765773773193359\n",
            "Epoch: 3/30, Batch: 67/91, Loss: 4.684591293334961\n",
            "Epoch: 3/30, Batch: 68/91, Loss: 4.467616081237793\n",
            "Epoch: 3/30, Batch: 69/91, Loss: 4.234896183013916\n",
            "Epoch: 3/30, Batch: 70/91, Loss: 3.7697153091430664\n",
            "Epoch: 3/30, Batch: 71/91, Loss: 3.918311834335327\n",
            "Epoch: 3/30, Batch: 72/91, Loss: 3.663841962814331\n",
            "Epoch: 3/30, Batch: 73/91, Loss: 5.138258934020996\n",
            "Epoch: 3/30, Batch: 74/91, Loss: 4.054906368255615\n",
            "Epoch: 3/30, Batch: 75/91, Loss: 4.492632865905762\n",
            "Epoch: 3/30, Batch: 76/91, Loss: 4.1397857666015625\n",
            "Epoch: 3/30, Batch: 77/91, Loss: 4.551623821258545\n",
            "Epoch: 3/30, Batch: 78/91, Loss: 3.737093687057495\n",
            "Epoch: 3/30, Batch: 79/91, Loss: 4.049780368804932\n",
            "Epoch: 3/30, Batch: 80/91, Loss: 4.380824089050293\n",
            "Epoch: 3/30, Batch: 81/91, Loss: 4.248839378356934\n",
            "Epoch: 3/30, Batch: 82/91, Loss: 3.833369255065918\n",
            "Epoch: 3/30, Batch: 83/91, Loss: 3.9052069187164307\n",
            "Epoch: 3/30, Batch: 84/91, Loss: 3.5390567779541016\n",
            "Epoch: 3/30, Batch: 85/91, Loss: 3.9119112491607666\n",
            "Epoch: 3/30, Batch: 86/91, Loss: 4.5300211906433105\n",
            "Epoch: 3/30, Batch: 87/91, Loss: 4.702425003051758\n",
            "Epoch: 3/30, Batch: 88/91, Loss: 4.2551140785217285\n",
            "Epoch: 3/30, Batch: 89/91, Loss: 3.5407392978668213\n",
            "Epoch: 3/30, Batch: 90/91, Loss: 4.478235244750977\n",
            "Epoch: 4/30, Batch: 0/91, Loss: 4.081140041351318\n",
            "Epoch: 4/30, Batch: 1/91, Loss: 4.2800798416137695\n",
            "Epoch: 4/30, Batch: 2/91, Loss: 3.824047327041626\n",
            "Epoch: 4/30, Batch: 3/91, Loss: 2.957925796508789\n",
            "Epoch: 4/30, Batch: 4/91, Loss: 3.4857983589172363\n",
            "Epoch: 4/30, Batch: 5/91, Loss: 4.721897602081299\n",
            "Epoch: 4/30, Batch: 6/91, Loss: 4.337028980255127\n",
            "Epoch: 4/30, Batch: 7/91, Loss: 4.273090362548828\n",
            "Epoch: 4/30, Batch: 8/91, Loss: 4.730096817016602\n",
            "Epoch: 4/30, Batch: 9/91, Loss: 3.942418336868286\n",
            "Epoch: 4/30, Batch: 10/91, Loss: 4.5628533363342285\n",
            "Epoch: 4/30, Batch: 11/91, Loss: 3.8823652267456055\n",
            "Epoch: 4/30, Batch: 12/91, Loss: 5.02006196975708\n",
            "Epoch: 4/30, Batch: 13/91, Loss: 3.052856206893921\n",
            "Epoch: 4/30, Batch: 14/91, Loss: 3.7153942584991455\n",
            "Epoch: 4/30, Batch: 15/91, Loss: 4.332299709320068\n",
            "Epoch: 4/30, Batch: 16/91, Loss: 3.8794784545898438\n",
            "Epoch: 4/30, Batch: 17/91, Loss: 4.384772300720215\n",
            "Epoch: 4/30, Batch: 18/91, Loss: 2.233586549758911\n",
            "Epoch: 4/30, Batch: 19/91, Loss: 4.243979454040527\n",
            "Epoch: 4/30, Batch: 20/91, Loss: 3.860389471054077\n",
            "Epoch: 4/30, Batch: 21/91, Loss: 4.7909746170043945\n",
            "Epoch: 4/30, Batch: 22/91, Loss: 3.4815213680267334\n",
            "Epoch: 4/30, Batch: 23/91, Loss: 4.118599891662598\n",
            "Epoch: 4/30, Batch: 24/91, Loss: 3.651050090789795\n",
            "Epoch: 4/30, Batch: 25/91, Loss: 4.165886878967285\n",
            "Epoch: 4/30, Batch: 26/91, Loss: 4.153606414794922\n",
            "Epoch: 4/30, Batch: 27/91, Loss: 4.040962219238281\n",
            "Epoch: 4/30, Batch: 28/91, Loss: 3.7407889366149902\n",
            "Epoch: 4/30, Batch: 29/91, Loss: 4.735838413238525\n",
            "Epoch: 4/30, Batch: 30/91, Loss: 4.437757968902588\n",
            "Epoch: 4/30, Batch: 31/91, Loss: 5.00063419342041\n",
            "Epoch: 4/30, Batch: 32/91, Loss: 3.5866310596466064\n",
            "Epoch: 4/30, Batch: 33/91, Loss: 4.654372692108154\n",
            "Epoch: 4/30, Batch: 34/91, Loss: 3.984203577041626\n",
            "Epoch: 4/30, Batch: 35/91, Loss: 4.154867172241211\n",
            "Epoch: 4/30, Batch: 36/91, Loss: 4.82609748840332\n",
            "Epoch: 4/30, Batch: 37/91, Loss: 4.616489410400391\n",
            "Epoch: 4/30, Batch: 38/91, Loss: 4.575915336608887\n",
            "Epoch: 4/30, Batch: 39/91, Loss: 2.8526413440704346\n",
            "Epoch: 4/30, Batch: 40/91, Loss: 4.886548042297363\n",
            "Epoch: 4/30, Batch: 41/91, Loss: 3.500316858291626\n",
            "Epoch: 4/30, Batch: 42/91, Loss: 4.654677391052246\n",
            "Epoch: 4/30, Batch: 43/91, Loss: 4.713732719421387\n",
            "Epoch: 4/30, Batch: 44/91, Loss: 4.995017051696777\n",
            "Epoch: 4/30, Batch: 45/91, Loss: 4.320405006408691\n",
            "Epoch: 4/30, Batch: 46/91, Loss: 3.724621057510376\n",
            "Epoch: 4/30, Batch: 47/91, Loss: 3.856034755706787\n",
            "Epoch: 4/30, Batch: 48/91, Loss: 3.9109675884246826\n",
            "Epoch: 4/30, Batch: 49/91, Loss: 4.04863166809082\n",
            "Epoch: 4/30, Batch: 50/91, Loss: 4.82694149017334\n",
            "Epoch: 4/30, Batch: 51/91, Loss: 5.353224754333496\n",
            "Epoch: 4/30, Batch: 52/91, Loss: 4.661312580108643\n",
            "Epoch: 4/30, Batch: 53/91, Loss: 4.045360565185547\n",
            "Epoch: 4/30, Batch: 54/91, Loss: 4.467232704162598\n",
            "Epoch: 4/30, Batch: 55/91, Loss: 3.7965505123138428\n",
            "Epoch: 4/30, Batch: 56/91, Loss: 3.566317081451416\n",
            "Epoch: 4/30, Batch: 57/91, Loss: 4.9293999671936035\n",
            "Epoch: 4/30, Batch: 58/91, Loss: 4.048355579376221\n",
            "Epoch: 4/30, Batch: 59/91, Loss: 4.331725597381592\n",
            "Epoch: 4/30, Batch: 60/91, Loss: 4.284818172454834\n",
            "Epoch: 4/30, Batch: 61/91, Loss: 3.780500888824463\n",
            "Epoch: 4/30, Batch: 62/91, Loss: 4.35030460357666\n",
            "Epoch: 4/30, Batch: 63/91, Loss: 4.49505615234375\n",
            "Epoch: 4/30, Batch: 64/91, Loss: 3.7918460369110107\n",
            "Epoch: 4/30, Batch: 65/91, Loss: 4.459192752838135\n",
            "Epoch: 4/30, Batch: 66/91, Loss: 4.468682765960693\n",
            "Epoch: 4/30, Batch: 67/91, Loss: 5.022953033447266\n",
            "Epoch: 4/30, Batch: 68/91, Loss: 4.9652862548828125\n",
            "Epoch: 4/30, Batch: 69/91, Loss: 4.997673511505127\n",
            "Epoch: 4/30, Batch: 70/91, Loss: 4.619213581085205\n",
            "Epoch: 4/30, Batch: 71/91, Loss: 4.062429904937744\n",
            "Epoch: 4/30, Batch: 72/91, Loss: 3.924342393875122\n",
            "Epoch: 4/30, Batch: 73/91, Loss: 3.026942729949951\n",
            "Epoch: 4/30, Batch: 74/91, Loss: 4.54111909866333\n",
            "Epoch: 4/30, Batch: 75/91, Loss: 4.71537971496582\n",
            "Epoch: 4/30, Batch: 76/91, Loss: 4.368427276611328\n",
            "Epoch: 4/30, Batch: 77/91, Loss: 2.8616244792938232\n",
            "Epoch: 4/30, Batch: 78/91, Loss: 4.525240898132324\n",
            "Epoch: 4/30, Batch: 79/91, Loss: 4.086641311645508\n",
            "Epoch: 4/30, Batch: 80/91, Loss: 3.6265621185302734\n",
            "Epoch: 4/30, Batch: 81/91, Loss: 4.439205646514893\n",
            "Epoch: 4/30, Batch: 82/91, Loss: 4.743729114532471\n",
            "Epoch: 4/30, Batch: 83/91, Loss: 4.222635269165039\n",
            "Epoch: 4/30, Batch: 84/91, Loss: 5.132340908050537\n",
            "Epoch: 4/30, Batch: 85/91, Loss: 4.5239763259887695\n",
            "Epoch: 4/30, Batch: 86/91, Loss: 3.941206932067871\n",
            "Epoch: 4/30, Batch: 87/91, Loss: 3.7969608306884766\n",
            "Epoch: 4/30, Batch: 88/91, Loss: 4.187635898590088\n",
            "Epoch: 4/30, Batch: 89/91, Loss: 3.7780392169952393\n",
            "Epoch: 4/30, Batch: 90/91, Loss: 4.547132968902588\n",
            "Epoch: 5/30, Batch: 0/91, Loss: 4.753815174102783\n",
            "Epoch: 5/30, Batch: 1/91, Loss: 3.557701587677002\n",
            "Epoch: 5/30, Batch: 2/91, Loss: 4.6153082847595215\n",
            "Epoch: 5/30, Batch: 3/91, Loss: 4.277919769287109\n",
            "Epoch: 5/30, Batch: 4/91, Loss: 4.3790411949157715\n",
            "Epoch: 5/30, Batch: 5/91, Loss: 4.563941478729248\n",
            "Epoch: 5/30, Batch: 6/91, Loss: 4.455447673797607\n",
            "Epoch: 5/30, Batch: 7/91, Loss: 4.723606586456299\n",
            "Epoch: 5/30, Batch: 8/91, Loss: 3.6734962463378906\n",
            "Epoch: 5/30, Batch: 9/91, Loss: 4.544623374938965\n",
            "Epoch: 5/30, Batch: 10/91, Loss: 4.041531562805176\n",
            "Epoch: 5/30, Batch: 11/91, Loss: 4.94649600982666\n",
            "Epoch: 5/30, Batch: 12/91, Loss: 4.771467685699463\n",
            "Epoch: 5/30, Batch: 13/91, Loss: 4.014243125915527\n",
            "Epoch: 5/30, Batch: 14/91, Loss: 4.555549621582031\n",
            "Epoch: 5/30, Batch: 15/91, Loss: 2.8094215393066406\n",
            "Epoch: 5/30, Batch: 16/91, Loss: 4.1987409591674805\n",
            "Epoch: 5/30, Batch: 17/91, Loss: 2.9686431884765625\n",
            "Epoch: 5/30, Batch: 18/91, Loss: 4.206797122955322\n",
            "Epoch: 5/30, Batch: 19/91, Loss: 3.568793535232544\n",
            "Epoch: 5/30, Batch: 20/91, Loss: 3.5683093070983887\n",
            "Epoch: 5/30, Batch: 21/91, Loss: 3.0001375675201416\n",
            "Epoch: 5/30, Batch: 22/91, Loss: 4.34112024307251\n",
            "Epoch: 5/30, Batch: 23/91, Loss: 4.564526557922363\n",
            "Epoch: 5/30, Batch: 24/91, Loss: 4.423100471496582\n",
            "Epoch: 5/30, Batch: 25/91, Loss: 4.669631004333496\n",
            "Epoch: 5/30, Batch: 26/91, Loss: 3.6852118968963623\n",
            "Epoch: 5/30, Batch: 27/91, Loss: 4.356024265289307\n",
            "Epoch: 5/30, Batch: 28/91, Loss: 3.974620819091797\n",
            "Epoch: 5/30, Batch: 29/91, Loss: 5.070333480834961\n",
            "Epoch: 5/30, Batch: 30/91, Loss: 4.431309223175049\n",
            "Epoch: 5/30, Batch: 31/91, Loss: 4.07180118560791\n",
            "Epoch: 5/30, Batch: 32/91, Loss: 5.020959377288818\n",
            "Epoch: 5/30, Batch: 33/91, Loss: 3.869645833969116\n",
            "Epoch: 5/30, Batch: 34/91, Loss: 4.639350891113281\n",
            "Epoch: 5/30, Batch: 35/91, Loss: 4.185227394104004\n",
            "Epoch: 5/30, Batch: 36/91, Loss: 4.974669933319092\n",
            "Epoch: 5/30, Batch: 37/91, Loss: 2.172041893005371\n",
            "Epoch: 5/30, Batch: 38/91, Loss: 3.7886767387390137\n",
            "Epoch: 5/30, Batch: 39/91, Loss: 3.6260147094726562\n",
            "Epoch: 5/30, Batch: 40/91, Loss: 4.122321605682373\n",
            "Epoch: 5/30, Batch: 41/91, Loss: 4.199875831604004\n",
            "Epoch: 5/30, Batch: 42/91, Loss: 4.32573938369751\n",
            "Epoch: 5/30, Batch: 43/91, Loss: 4.071685791015625\n",
            "Epoch: 5/30, Batch: 44/91, Loss: 4.315436363220215\n",
            "Epoch: 5/30, Batch: 45/91, Loss: 4.008513927459717\n",
            "Epoch: 5/30, Batch: 46/91, Loss: 4.854851245880127\n",
            "Epoch: 5/30, Batch: 47/91, Loss: 3.939136505126953\n",
            "Epoch: 5/30, Batch: 48/91, Loss: 4.78638219833374\n",
            "Epoch: 5/30, Batch: 49/91, Loss: 4.212252140045166\n",
            "Epoch: 5/30, Batch: 50/91, Loss: 4.661023139953613\n",
            "Epoch: 5/30, Batch: 51/91, Loss: 4.419084548950195\n",
            "Epoch: 5/30, Batch: 52/91, Loss: 5.194247245788574\n",
            "Epoch: 5/30, Batch: 53/91, Loss: 3.877065896987915\n",
            "Epoch: 5/30, Batch: 54/91, Loss: 3.860300064086914\n",
            "Epoch: 5/30, Batch: 55/91, Loss: 2.9118762016296387\n",
            "Epoch: 5/30, Batch: 56/91, Loss: 5.862886905670166\n",
            "Epoch: 5/30, Batch: 57/91, Loss: 3.839893341064453\n",
            "Epoch: 5/30, Batch: 58/91, Loss: 3.88085675239563\n",
            "Epoch: 5/30, Batch: 59/91, Loss: 4.3301825523376465\n",
            "Epoch: 5/30, Batch: 60/91, Loss: 4.51998233795166\n",
            "Epoch: 5/30, Batch: 61/91, Loss: 4.760234355926514\n",
            "Epoch: 5/30, Batch: 62/91, Loss: 3.877490282058716\n",
            "Epoch: 5/30, Batch: 63/91, Loss: 3.958031415939331\n",
            "Epoch: 5/30, Batch: 64/91, Loss: 4.360102653503418\n",
            "Epoch: 5/30, Batch: 65/91, Loss: 5.128380298614502\n",
            "Epoch: 5/30, Batch: 66/91, Loss: 4.216107368469238\n",
            "Epoch: 5/30, Batch: 67/91, Loss: 4.480329513549805\n",
            "Epoch: 5/30, Batch: 68/91, Loss: 4.514316558837891\n",
            "Epoch: 5/30, Batch: 69/91, Loss: 4.500129222869873\n",
            "Epoch: 5/30, Batch: 70/91, Loss: 4.412313938140869\n",
            "Epoch: 5/30, Batch: 71/91, Loss: 5.122028827667236\n",
            "Epoch: 5/30, Batch: 72/91, Loss: 4.705602169036865\n",
            "Epoch: 5/30, Batch: 73/91, Loss: 4.180374622344971\n",
            "Epoch: 5/30, Batch: 74/91, Loss: 3.068398952484131\n",
            "Epoch: 5/30, Batch: 75/91, Loss: 4.347769737243652\n",
            "Epoch: 5/30, Batch: 76/91, Loss: 3.9468159675598145\n",
            "Epoch: 5/30, Batch: 77/91, Loss: 4.356057167053223\n",
            "Epoch: 5/30, Batch: 78/91, Loss: 4.267611503601074\n",
            "Epoch: 5/30, Batch: 79/91, Loss: 3.6867401599884033\n",
            "Epoch: 5/30, Batch: 80/91, Loss: 3.7263782024383545\n",
            "Epoch: 5/30, Batch: 81/91, Loss: 4.124567031860352\n",
            "Epoch: 5/30, Batch: 82/91, Loss: 3.6134448051452637\n",
            "Epoch: 5/30, Batch: 83/91, Loss: 4.499328136444092\n",
            "Epoch: 5/30, Batch: 84/91, Loss: 4.161582946777344\n",
            "Epoch: 5/30, Batch: 85/91, Loss: 4.0978264808654785\n",
            "Epoch: 5/30, Batch: 86/91, Loss: 4.03940486907959\n",
            "Epoch: 5/30, Batch: 87/91, Loss: 4.234991073608398\n",
            "Epoch: 5/30, Batch: 88/91, Loss: 5.0445990562438965\n",
            "Epoch: 5/30, Batch: 89/91, Loss: 4.84473180770874\n",
            "Epoch: 5/30, Batch: 90/91, Loss: 4.339627265930176\n",
            "Epoch: 6/30, Batch: 0/91, Loss: 3.923412322998047\n",
            "Epoch: 6/30, Batch: 1/91, Loss: 4.531630992889404\n",
            "Epoch: 6/30, Batch: 2/91, Loss: 4.803159713745117\n",
            "Epoch: 6/30, Batch: 3/91, Loss: 4.228733539581299\n",
            "Epoch: 6/30, Batch: 4/91, Loss: 3.508960247039795\n",
            "Epoch: 6/30, Batch: 5/91, Loss: 5.14307975769043\n",
            "Epoch: 6/30, Batch: 6/91, Loss: 4.616811752319336\n",
            "Epoch: 6/30, Batch: 7/91, Loss: 3.878117084503174\n",
            "Epoch: 6/30, Batch: 8/91, Loss: 4.972270965576172\n",
            "Epoch: 6/30, Batch: 9/91, Loss: 4.8602070808410645\n",
            "Epoch: 6/30, Batch: 10/91, Loss: 4.1109089851379395\n",
            "Epoch: 6/30, Batch: 11/91, Loss: 4.437494277954102\n",
            "Epoch: 6/30, Batch: 12/91, Loss: 3.9313554763793945\n",
            "Epoch: 6/30, Batch: 13/91, Loss: 4.61893367767334\n",
            "Epoch: 6/30, Batch: 14/91, Loss: 4.225446701049805\n",
            "Epoch: 6/30, Batch: 15/91, Loss: 5.02485990524292\n",
            "Epoch: 6/30, Batch: 16/91, Loss: 4.562977313995361\n",
            "Epoch: 6/30, Batch: 17/91, Loss: 4.476449012756348\n",
            "Epoch: 6/30, Batch: 18/91, Loss: 3.04219126701355\n",
            "Epoch: 6/30, Batch: 19/91, Loss: 3.640263080596924\n",
            "Epoch: 6/30, Batch: 20/91, Loss: 4.6650004386901855\n",
            "Epoch: 6/30, Batch: 21/91, Loss: 3.9686100482940674\n",
            "Epoch: 6/30, Batch: 22/91, Loss: 3.3305158615112305\n",
            "Epoch: 6/30, Batch: 23/91, Loss: 3.9811410903930664\n",
            "Epoch: 6/30, Batch: 24/91, Loss: 4.645752429962158\n",
            "Epoch: 6/30, Batch: 25/91, Loss: 3.5368056297302246\n",
            "Epoch: 6/30, Batch: 26/91, Loss: 4.025415897369385\n",
            "Epoch: 6/30, Batch: 27/91, Loss: 4.156013011932373\n",
            "Epoch: 6/30, Batch: 28/91, Loss: 4.471673011779785\n",
            "Epoch: 6/30, Batch: 29/91, Loss: 4.280295372009277\n",
            "Epoch: 6/30, Batch: 30/91, Loss: 4.515031337738037\n",
            "Epoch: 6/30, Batch: 31/91, Loss: 3.1591553688049316\n",
            "Epoch: 6/30, Batch: 32/91, Loss: 2.9218387603759766\n",
            "Epoch: 6/30, Batch: 33/91, Loss: 4.721096992492676\n",
            "Epoch: 6/30, Batch: 34/91, Loss: 4.193665027618408\n",
            "Epoch: 6/30, Batch: 35/91, Loss: 4.202227592468262\n",
            "Epoch: 6/30, Batch: 36/91, Loss: 3.9809601306915283\n",
            "Epoch: 6/30, Batch: 37/91, Loss: 2.875650644302368\n",
            "Epoch: 6/30, Batch: 38/91, Loss: 4.196137428283691\n",
            "Epoch: 6/30, Batch: 39/91, Loss: 3.9407548904418945\n",
            "Epoch: 6/30, Batch: 40/91, Loss: 4.469335079193115\n",
            "Epoch: 6/30, Batch: 41/91, Loss: 4.337767601013184\n",
            "Epoch: 6/30, Batch: 42/91, Loss: 4.000948429107666\n",
            "Epoch: 6/30, Batch: 43/91, Loss: 4.7313008308410645\n",
            "Epoch: 6/30, Batch: 44/91, Loss: 3.806199550628662\n",
            "Epoch: 6/30, Batch: 45/91, Loss: 4.406097412109375\n",
            "Epoch: 6/30, Batch: 46/91, Loss: 4.243861198425293\n",
            "Epoch: 6/30, Batch: 47/91, Loss: 4.995904922485352\n",
            "Epoch: 6/30, Batch: 48/91, Loss: 3.7569918632507324\n",
            "Epoch: 6/30, Batch: 49/91, Loss: 4.07244873046875\n",
            "Epoch: 6/30, Batch: 50/91, Loss: 3.8552613258361816\n",
            "Epoch: 6/30, Batch: 51/91, Loss: 4.0417633056640625\n",
            "Epoch: 6/30, Batch: 52/91, Loss: 4.453603267669678\n",
            "Epoch: 6/30, Batch: 53/91, Loss: 4.522375106811523\n",
            "Epoch: 6/30, Batch: 54/91, Loss: 2.8405511379241943\n",
            "Epoch: 6/30, Batch: 55/91, Loss: 4.283522605895996\n",
            "Epoch: 6/30, Batch: 56/91, Loss: 4.473919868469238\n",
            "Epoch: 6/30, Batch: 57/91, Loss: 4.938347816467285\n",
            "Epoch: 6/30, Batch: 58/91, Loss: 4.12895393371582\n",
            "Epoch: 6/30, Batch: 59/91, Loss: 3.9279303550720215\n",
            "Epoch: 6/30, Batch: 60/91, Loss: 3.6463983058929443\n",
            "Epoch: 6/30, Batch: 61/91, Loss: 5.004070281982422\n",
            "Epoch: 6/30, Batch: 62/91, Loss: 4.558046340942383\n",
            "Epoch: 6/30, Batch: 63/91, Loss: 4.689389228820801\n",
            "Epoch: 6/30, Batch: 64/91, Loss: 2.1026172637939453\n",
            "Epoch: 6/30, Batch: 65/91, Loss: 4.740204334259033\n",
            "Epoch: 6/30, Batch: 66/91, Loss: 3.860684633255005\n",
            "Epoch: 6/30, Batch: 67/91, Loss: 4.719751358032227\n",
            "Epoch: 6/30, Batch: 68/91, Loss: 4.566330432891846\n",
            "Epoch: 6/30, Batch: 69/91, Loss: 4.731810092926025\n",
            "Epoch: 6/30, Batch: 70/91, Loss: 4.157939910888672\n",
            "Epoch: 6/30, Batch: 71/91, Loss: 4.418737888336182\n",
            "Epoch: 6/30, Batch: 72/91, Loss: 4.255166530609131\n",
            "Epoch: 6/30, Batch: 73/91, Loss: 4.4637250900268555\n",
            "Epoch: 6/30, Batch: 74/91, Loss: 4.953916549682617\n",
            "Epoch: 6/30, Batch: 75/91, Loss: 3.9760968685150146\n",
            "Epoch: 6/30, Batch: 76/91, Loss: 4.379688739776611\n",
            "Epoch: 6/30, Batch: 77/91, Loss: 4.4811177253723145\n",
            "Epoch: 6/30, Batch: 78/91, Loss: 4.283638954162598\n",
            "Epoch: 6/30, Batch: 79/91, Loss: 4.496348857879639\n",
            "Epoch: 6/30, Batch: 80/91, Loss: 3.8505804538726807\n",
            "Epoch: 6/30, Batch: 81/91, Loss: 4.795190811157227\n",
            "Epoch: 6/30, Batch: 82/91, Loss: 4.351559638977051\n",
            "Epoch: 6/30, Batch: 83/91, Loss: 4.0214457511901855\n",
            "Epoch: 6/30, Batch: 84/91, Loss: 4.187747001647949\n",
            "Epoch: 6/30, Batch: 85/91, Loss: 4.051299571990967\n",
            "Epoch: 6/30, Batch: 86/91, Loss: 3.7469632625579834\n",
            "Epoch: 6/30, Batch: 87/91, Loss: 4.555636405944824\n",
            "Epoch: 6/30, Batch: 88/91, Loss: 2.9030423164367676\n",
            "Epoch: 6/30, Batch: 89/91, Loss: 4.696019172668457\n",
            "Epoch: 6/30, Batch: 90/91, Loss: 4.334939002990723\n",
            "Epoch: 7/30, Batch: 0/91, Loss: 4.617809772491455\n",
            "Epoch: 7/30, Batch: 1/91, Loss: 3.797752857208252\n",
            "Epoch: 7/30, Batch: 2/91, Loss: 3.665074586868286\n",
            "Epoch: 7/30, Batch: 3/91, Loss: 3.6132774353027344\n",
            "Epoch: 7/30, Batch: 4/91, Loss: 2.161315679550171\n",
            "Epoch: 7/30, Batch: 5/91, Loss: 4.322070121765137\n",
            "Epoch: 7/30, Batch: 6/91, Loss: 4.0418806076049805\n",
            "Epoch: 7/30, Batch: 7/91, Loss: 4.024091720581055\n",
            "Epoch: 7/30, Batch: 8/91, Loss: 4.051971435546875\n",
            "Epoch: 7/30, Batch: 9/91, Loss: 4.122920036315918\n",
            "Epoch: 7/30, Batch: 10/91, Loss: 4.791042327880859\n",
            "Epoch: 7/30, Batch: 11/91, Loss: 4.662671089172363\n",
            "Epoch: 7/30, Batch: 12/91, Loss: 4.195900917053223\n",
            "Epoch: 7/30, Batch: 13/91, Loss: 4.349170207977295\n",
            "Epoch: 7/30, Batch: 14/91, Loss: 4.201328277587891\n",
            "Epoch: 7/30, Batch: 15/91, Loss: 5.235255718231201\n",
            "Epoch: 7/30, Batch: 16/91, Loss: 4.363439083099365\n",
            "Epoch: 7/30, Batch: 17/91, Loss: 4.114484786987305\n",
            "Epoch: 7/30, Batch: 18/91, Loss: 4.0981926918029785\n",
            "Epoch: 7/30, Batch: 19/91, Loss: 3.628046751022339\n",
            "Epoch: 7/30, Batch: 20/91, Loss: 4.084824562072754\n",
            "Epoch: 7/30, Batch: 21/91, Loss: 4.015578746795654\n",
            "Epoch: 7/30, Batch: 22/91, Loss: 3.7982327938079834\n",
            "Epoch: 7/30, Batch: 23/91, Loss: 4.053008079528809\n",
            "Epoch: 7/30, Batch: 24/91, Loss: 3.806326150894165\n",
            "Epoch: 7/30, Batch: 25/91, Loss: 4.3890485763549805\n",
            "Epoch: 7/30, Batch: 26/91, Loss: 4.253349304199219\n",
            "Epoch: 7/30, Batch: 27/91, Loss: 4.133205413818359\n",
            "Epoch: 7/30, Batch: 28/91, Loss: 4.032631874084473\n",
            "Epoch: 7/30, Batch: 29/91, Loss: 4.4278388023376465\n",
            "Epoch: 7/30, Batch: 30/91, Loss: 4.392730236053467\n",
            "Epoch: 7/30, Batch: 31/91, Loss: 3.9788148403167725\n",
            "Epoch: 7/30, Batch: 32/91, Loss: 4.927272319793701\n",
            "Epoch: 7/30, Batch: 33/91, Loss: 3.6563432216644287\n",
            "Epoch: 7/30, Batch: 34/91, Loss: 4.581403732299805\n",
            "Epoch: 7/30, Batch: 35/91, Loss: 4.622746467590332\n",
            "Epoch: 7/30, Batch: 36/91, Loss: 2.701761484146118\n",
            "Epoch: 7/30, Batch: 37/91, Loss: 4.427304744720459\n",
            "Epoch: 7/30, Batch: 38/91, Loss: 4.144557476043701\n",
            "Epoch: 7/30, Batch: 39/91, Loss: 4.651095390319824\n",
            "Epoch: 7/30, Batch: 40/91, Loss: 4.278939247131348\n",
            "Epoch: 7/30, Batch: 41/91, Loss: 4.869864463806152\n",
            "Epoch: 7/30, Batch: 42/91, Loss: 4.866629123687744\n",
            "Epoch: 7/30, Batch: 43/91, Loss: 4.061861515045166\n",
            "Epoch: 7/30, Batch: 44/91, Loss: 4.334655284881592\n",
            "Epoch: 7/30, Batch: 45/91, Loss: 4.043304443359375\n",
            "Epoch: 7/30, Batch: 46/91, Loss: 4.756708145141602\n",
            "Epoch: 7/30, Batch: 47/91, Loss: 3.8939783573150635\n",
            "Epoch: 7/30, Batch: 48/91, Loss: 4.306030750274658\n",
            "Epoch: 7/30, Batch: 49/91, Loss: 4.467403888702393\n",
            "Epoch: 7/30, Batch: 50/91, Loss: 3.926955223083496\n",
            "Epoch: 7/30, Batch: 51/91, Loss: 4.314380645751953\n",
            "Epoch: 7/30, Batch: 52/91, Loss: 3.5890021324157715\n",
            "Epoch: 7/30, Batch: 53/91, Loss: 4.707252025604248\n",
            "Epoch: 7/30, Batch: 54/91, Loss: 4.900887966156006\n",
            "Epoch: 7/30, Batch: 55/91, Loss: 4.802640438079834\n",
            "Epoch: 7/30, Batch: 56/91, Loss: 4.8013529777526855\n",
            "Epoch: 7/30, Batch: 57/91, Loss: 4.924126148223877\n",
            "Epoch: 7/30, Batch: 58/91, Loss: 4.382564067840576\n",
            "Epoch: 7/30, Batch: 59/91, Loss: 4.581316947937012\n",
            "Epoch: 7/30, Batch: 60/91, Loss: 3.851292848587036\n",
            "Epoch: 7/30, Batch: 61/91, Loss: 4.530550003051758\n",
            "Epoch: 7/30, Batch: 62/91, Loss: 4.756199836730957\n",
            "Epoch: 7/30, Batch: 63/91, Loss: 4.37476921081543\n",
            "Epoch: 7/30, Batch: 64/91, Loss: 4.020742893218994\n",
            "Epoch: 7/30, Batch: 65/91, Loss: 4.745241641998291\n",
            "Epoch: 7/30, Batch: 66/91, Loss: 3.5636885166168213\n",
            "Epoch: 7/30, Batch: 67/91, Loss: 4.515285491943359\n",
            "Epoch: 7/30, Batch: 68/91, Loss: 3.9853219985961914\n",
            "Epoch: 7/30, Batch: 69/91, Loss: 4.575002193450928\n",
            "Epoch: 7/30, Batch: 70/91, Loss: 4.629947662353516\n",
            "Epoch: 7/30, Batch: 71/91, Loss: 4.526290416717529\n",
            "Epoch: 7/30, Batch: 72/91, Loss: 4.330789089202881\n",
            "Epoch: 7/30, Batch: 73/91, Loss: 4.602078914642334\n",
            "Epoch: 7/30, Batch: 74/91, Loss: 3.8956410884857178\n",
            "Epoch: 7/30, Batch: 75/91, Loss: 4.079850196838379\n",
            "Epoch: 7/30, Batch: 76/91, Loss: 3.6097114086151123\n",
            "Epoch: 7/30, Batch: 77/91, Loss: 3.018876075744629\n",
            "Epoch: 7/30, Batch: 78/91, Loss: 4.428242206573486\n",
            "Epoch: 7/30, Batch: 79/91, Loss: 4.4977498054504395\n",
            "Epoch: 7/30, Batch: 80/91, Loss: 3.8456125259399414\n",
            "Epoch: 7/30, Batch: 81/91, Loss: 3.4061484336853027\n",
            "Epoch: 7/30, Batch: 82/91, Loss: 3.577894449234009\n",
            "Epoch: 7/30, Batch: 83/91, Loss: 4.121886730194092\n",
            "Epoch: 7/30, Batch: 84/91, Loss: 4.945397853851318\n",
            "Epoch: 7/30, Batch: 85/91, Loss: 5.101808547973633\n",
            "Epoch: 7/30, Batch: 86/91, Loss: 4.4499359130859375\n",
            "Epoch: 7/30, Batch: 87/91, Loss: 4.050049304962158\n",
            "Epoch: 7/30, Batch: 88/91, Loss: 4.585132122039795\n",
            "Epoch: 7/30, Batch: 89/91, Loss: 3.6970057487487793\n",
            "Epoch: 7/30, Batch: 90/91, Loss: 4.634201526641846\n",
            "Epoch: 8/30, Batch: 0/91, Loss: 4.959708213806152\n",
            "Epoch: 8/30, Batch: 1/91, Loss: 2.3637876510620117\n",
            "Epoch: 8/30, Batch: 2/91, Loss: 2.2094764709472656\n",
            "Epoch: 8/30, Batch: 3/91, Loss: 4.160574913024902\n",
            "Epoch: 8/30, Batch: 4/91, Loss: 4.045166969299316\n",
            "Epoch: 8/30, Batch: 5/91, Loss: 4.359560012817383\n",
            "Epoch: 8/30, Batch: 6/91, Loss: 4.53996467590332\n",
            "Epoch: 8/30, Batch: 7/91, Loss: 4.435306549072266\n",
            "Epoch: 8/30, Batch: 8/91, Loss: 2.896085023880005\n",
            "Epoch: 8/30, Batch: 9/91, Loss: 4.275279998779297\n",
            "Epoch: 8/30, Batch: 10/91, Loss: 4.858015060424805\n",
            "Epoch: 8/30, Batch: 11/91, Loss: 3.89849853515625\n",
            "Epoch: 8/30, Batch: 12/91, Loss: 4.614572525024414\n",
            "Epoch: 8/30, Batch: 13/91, Loss: 3.8206875324249268\n",
            "Epoch: 8/30, Batch: 14/91, Loss: 3.4440033435821533\n",
            "Epoch: 8/30, Batch: 15/91, Loss: 4.535398006439209\n",
            "Epoch: 8/30, Batch: 16/91, Loss: 3.701054573059082\n",
            "Epoch: 8/30, Batch: 17/91, Loss: 4.559141635894775\n",
            "Epoch: 8/30, Batch: 18/91, Loss: 3.8914754390716553\n",
            "Epoch: 8/30, Batch: 19/91, Loss: 4.364526748657227\n",
            "Epoch: 8/30, Batch: 20/91, Loss: 4.814703941345215\n",
            "Epoch: 8/30, Batch: 21/91, Loss: 4.006560802459717\n",
            "Epoch: 8/30, Batch: 22/91, Loss: 4.439243793487549\n",
            "Epoch: 8/30, Batch: 23/91, Loss: 2.675347328186035\n",
            "Epoch: 8/30, Batch: 24/91, Loss: 4.642520427703857\n",
            "Epoch: 8/30, Batch: 25/91, Loss: 4.633193016052246\n",
            "Epoch: 8/30, Batch: 26/91, Loss: 4.9172234535217285\n",
            "Epoch: 8/30, Batch: 27/91, Loss: 4.059567928314209\n",
            "Epoch: 8/30, Batch: 28/91, Loss: 4.291118621826172\n",
            "Epoch: 8/30, Batch: 29/91, Loss: 3.9956109523773193\n",
            "Epoch: 8/30, Batch: 30/91, Loss: 3.1390535831451416\n",
            "Epoch: 8/30, Batch: 31/91, Loss: 3.995809555053711\n",
            "Epoch: 8/30, Batch: 32/91, Loss: 4.6621623039245605\n",
            "Epoch: 8/30, Batch: 33/91, Loss: 4.371476173400879\n",
            "Epoch: 8/30, Batch: 34/91, Loss: 4.372916221618652\n",
            "Epoch: 8/30, Batch: 35/91, Loss: 4.48622989654541\n",
            "Epoch: 8/30, Batch: 36/91, Loss: 3.9893312454223633\n",
            "Epoch: 8/30, Batch: 37/91, Loss: 3.544462203979492\n",
            "Epoch: 8/30, Batch: 38/91, Loss: 3.85571026802063\n",
            "Epoch: 8/30, Batch: 39/91, Loss: 4.494325160980225\n",
            "Epoch: 8/30, Batch: 40/91, Loss: 4.529808521270752\n",
            "Epoch: 8/30, Batch: 41/91, Loss: 4.458527088165283\n",
            "Epoch: 8/30, Batch: 42/91, Loss: 3.2826309204101562\n",
            "Epoch: 8/30, Batch: 43/91, Loss: 3.9615795612335205\n",
            "Epoch: 8/30, Batch: 44/91, Loss: 4.277193069458008\n",
            "Epoch: 8/30, Batch: 45/91, Loss: 3.828683614730835\n",
            "Epoch: 8/30, Batch: 46/91, Loss: 4.571341037750244\n",
            "Epoch: 8/30, Batch: 47/91, Loss: 4.3309197425842285\n",
            "Epoch: 8/30, Batch: 48/91, Loss: 4.428234100341797\n",
            "Epoch: 8/30, Batch: 49/91, Loss: 4.311497211456299\n",
            "Epoch: 8/30, Batch: 50/91, Loss: 3.8594202995300293\n",
            "Epoch: 8/30, Batch: 51/91, Loss: 3.7675070762634277\n",
            "Epoch: 8/30, Batch: 52/91, Loss: 4.5490336418151855\n",
            "Epoch: 8/30, Batch: 53/91, Loss: 4.289240837097168\n",
            "Epoch: 8/30, Batch: 54/91, Loss: 3.9922749996185303\n",
            "Epoch: 8/30, Batch: 55/91, Loss: 4.0076823234558105\n",
            "Epoch: 8/30, Batch: 56/91, Loss: 3.660752058029175\n",
            "Epoch: 8/30, Batch: 57/91, Loss: 4.072479248046875\n",
            "Epoch: 8/30, Batch: 58/91, Loss: 4.787581443786621\n",
            "Epoch: 8/30, Batch: 59/91, Loss: 4.956812858581543\n",
            "Epoch: 8/30, Batch: 60/91, Loss: 5.2130842208862305\n",
            "Epoch: 8/30, Batch: 61/91, Loss: 3.552513837814331\n",
            "Epoch: 8/30, Batch: 62/91, Loss: 4.912108898162842\n",
            "Epoch: 8/30, Batch: 63/91, Loss: 3.89697265625\n",
            "Epoch: 8/30, Batch: 64/91, Loss: 4.519355773925781\n",
            "Epoch: 8/30, Batch: 65/91, Loss: 4.6155524253845215\n",
            "Epoch: 8/30, Batch: 66/91, Loss: 4.656225681304932\n",
            "Epoch: 8/30, Batch: 67/91, Loss: 4.167845249176025\n",
            "Epoch: 8/30, Batch: 68/91, Loss: 4.872467041015625\n",
            "Epoch: 8/30, Batch: 69/91, Loss: 3.7027151584625244\n",
            "Epoch: 8/30, Batch: 70/91, Loss: 4.437535762786865\n",
            "Epoch: 8/30, Batch: 71/91, Loss: 4.030766010284424\n",
            "Epoch: 8/30, Batch: 72/91, Loss: 3.252466917037964\n",
            "Epoch: 8/30, Batch: 73/91, Loss: 3.8958637714385986\n",
            "Epoch: 8/30, Batch: 74/91, Loss: 3.4026246070861816\n",
            "Epoch: 8/30, Batch: 75/91, Loss: 4.421105861663818\n",
            "Epoch: 8/30, Batch: 76/91, Loss: 3.4209091663360596\n",
            "Epoch: 8/30, Batch: 77/91, Loss: 2.9810452461242676\n",
            "Epoch: 8/30, Batch: 78/91, Loss: 3.7919371128082275\n",
            "Epoch: 8/30, Batch: 79/91, Loss: 3.904059648513794\n",
            "Epoch: 8/30, Batch: 80/91, Loss: 4.363986968994141\n",
            "Epoch: 8/30, Batch: 81/91, Loss: 4.779193878173828\n",
            "Epoch: 8/30, Batch: 82/91, Loss: 4.682214736938477\n",
            "Epoch: 8/30, Batch: 83/91, Loss: 4.028017520904541\n",
            "Epoch: 8/30, Batch: 84/91, Loss: 3.364288091659546\n",
            "Epoch: 8/30, Batch: 85/91, Loss: 3.8342437744140625\n",
            "Epoch: 8/30, Batch: 86/91, Loss: 4.28328800201416\n",
            "Epoch: 8/30, Batch: 87/91, Loss: 2.992267608642578\n",
            "Epoch: 8/30, Batch: 88/91, Loss: 3.882836103439331\n",
            "Epoch: 8/30, Batch: 89/91, Loss: 4.705966949462891\n",
            "Epoch: 8/30, Batch: 90/91, Loss: 3.2003891468048096\n",
            "Epoch: 9/30, Batch: 0/91, Loss: 4.306814670562744\n",
            "Epoch: 9/30, Batch: 1/91, Loss: 4.28350830078125\n",
            "Epoch: 9/30, Batch: 2/91, Loss: 3.90393328666687\n",
            "Epoch: 9/30, Batch: 3/91, Loss: 4.623167037963867\n",
            "Epoch: 9/30, Batch: 4/91, Loss: 4.5434393882751465\n",
            "Epoch: 9/30, Batch: 5/91, Loss: 4.3443474769592285\n",
            "Epoch: 9/30, Batch: 6/91, Loss: 4.252043724060059\n",
            "Epoch: 9/30, Batch: 7/91, Loss: 4.409780979156494\n",
            "Epoch: 9/30, Batch: 8/91, Loss: 4.180464267730713\n",
            "Epoch: 9/30, Batch: 9/91, Loss: 2.859910726547241\n",
            "Epoch: 9/30, Batch: 10/91, Loss: 4.4332170486450195\n",
            "Epoch: 9/30, Batch: 11/91, Loss: 3.7383904457092285\n",
            "Epoch: 9/30, Batch: 12/91, Loss: 4.526701927185059\n",
            "Epoch: 9/30, Batch: 13/91, Loss: 3.9861326217651367\n",
            "Epoch: 9/30, Batch: 14/91, Loss: 4.284197807312012\n",
            "Epoch: 9/30, Batch: 15/91, Loss: 2.8670506477355957\n",
            "Epoch: 9/30, Batch: 16/91, Loss: 4.49598503112793\n",
            "Epoch: 9/30, Batch: 17/91, Loss: 3.953542709350586\n",
            "Epoch: 9/30, Batch: 18/91, Loss: 3.8905768394470215\n",
            "Epoch: 9/30, Batch: 19/91, Loss: 5.095633506774902\n",
            "Epoch: 9/30, Batch: 20/91, Loss: 4.328213214874268\n",
            "Epoch: 9/30, Batch: 21/91, Loss: 3.390496015548706\n",
            "Epoch: 9/30, Batch: 22/91, Loss: 3.731888771057129\n",
            "Epoch: 9/30, Batch: 23/91, Loss: 4.655924320220947\n",
            "Epoch: 9/30, Batch: 24/91, Loss: 4.9485764503479\n",
            "Epoch: 9/30, Batch: 25/91, Loss: 3.800612449645996\n",
            "Epoch: 9/30, Batch: 26/91, Loss: 5.20095157623291\n",
            "Epoch: 9/30, Batch: 27/91, Loss: 3.813350200653076\n",
            "Epoch: 9/30, Batch: 28/91, Loss: 4.456887245178223\n",
            "Epoch: 9/30, Batch: 29/91, Loss: 3.976114273071289\n",
            "Epoch: 9/30, Batch: 30/91, Loss: 4.0798020362854\n",
            "Epoch: 9/30, Batch: 31/91, Loss: 4.278803825378418\n",
            "Epoch: 9/30, Batch: 32/91, Loss: 3.5510056018829346\n",
            "Epoch: 9/30, Batch: 33/91, Loss: 3.92900013923645\n",
            "Epoch: 9/30, Batch: 34/91, Loss: 5.153408527374268\n",
            "Epoch: 9/30, Batch: 35/91, Loss: 4.254714488983154\n",
            "Epoch: 9/30, Batch: 36/91, Loss: 4.080125331878662\n",
            "Epoch: 9/30, Batch: 37/91, Loss: 2.8321726322174072\n",
            "Epoch: 9/30, Batch: 38/91, Loss: 4.181975841522217\n",
            "Epoch: 9/30, Batch: 39/91, Loss: 4.581905364990234\n",
            "Epoch: 9/30, Batch: 40/91, Loss: 4.508052825927734\n",
            "Epoch: 9/30, Batch: 41/91, Loss: 4.6199870109558105\n",
            "Epoch: 9/30, Batch: 42/91, Loss: 4.930615425109863\n",
            "Epoch: 9/30, Batch: 43/91, Loss: 5.813045024871826\n",
            "Epoch: 9/30, Batch: 44/91, Loss: 2.657743453979492\n",
            "Epoch: 9/30, Batch: 45/91, Loss: 3.692582607269287\n",
            "Epoch: 9/30, Batch: 46/91, Loss: 4.682397365570068\n",
            "Epoch: 9/30, Batch: 47/91, Loss: 3.9390954971313477\n",
            "Epoch: 9/30, Batch: 48/91, Loss: 3.812971591949463\n",
            "Epoch: 9/30, Batch: 49/91, Loss: 3.01924204826355\n",
            "Epoch: 9/30, Batch: 50/91, Loss: 4.579109191894531\n",
            "Epoch: 9/30, Batch: 51/91, Loss: 4.441019535064697\n",
            "Epoch: 9/30, Batch: 52/91, Loss: 3.7919704914093018\n",
            "Epoch: 9/30, Batch: 53/91, Loss: 3.991010904312134\n",
            "Epoch: 9/30, Batch: 54/91, Loss: 3.5010626316070557\n",
            "Epoch: 9/30, Batch: 55/91, Loss: 5.150423049926758\n",
            "Epoch: 9/30, Batch: 56/91, Loss: 4.433167934417725\n",
            "Epoch: 9/30, Batch: 57/91, Loss: 4.443988800048828\n",
            "Epoch: 9/30, Batch: 58/91, Loss: 3.846031665802002\n",
            "Epoch: 9/30, Batch: 59/91, Loss: 4.484779357910156\n",
            "Epoch: 9/30, Batch: 60/91, Loss: 4.523850917816162\n",
            "Epoch: 9/30, Batch: 61/91, Loss: 4.904369354248047\n",
            "Epoch: 9/30, Batch: 62/91, Loss: 4.040791034698486\n",
            "Epoch: 9/30, Batch: 63/91, Loss: 4.277892112731934\n",
            "Epoch: 9/30, Batch: 64/91, Loss: 4.297179222106934\n",
            "Epoch: 9/30, Batch: 65/91, Loss: 3.738403797149658\n",
            "Epoch: 9/30, Batch: 66/91, Loss: 4.1628828048706055\n",
            "Epoch: 9/30, Batch: 67/91, Loss: 4.1305718421936035\n",
            "Epoch: 9/30, Batch: 68/91, Loss: 4.209613800048828\n",
            "Epoch: 9/30, Batch: 69/91, Loss: 4.653761863708496\n",
            "Epoch: 9/30, Batch: 70/91, Loss: 4.383920669555664\n",
            "Epoch: 9/30, Batch: 71/91, Loss: 3.8316216468811035\n",
            "Epoch: 9/30, Batch: 72/91, Loss: 4.825098991394043\n",
            "Epoch: 9/30, Batch: 73/91, Loss: 3.827608346939087\n",
            "Epoch: 9/30, Batch: 74/91, Loss: 4.494470596313477\n",
            "Epoch: 9/30, Batch: 75/91, Loss: 4.1416916847229\n",
            "Epoch: 9/30, Batch: 76/91, Loss: 4.888823509216309\n",
            "Epoch: 9/30, Batch: 77/91, Loss: 4.2548065185546875\n",
            "Epoch: 9/30, Batch: 78/91, Loss: 3.8141746520996094\n",
            "Epoch: 9/30, Batch: 79/91, Loss: 4.509183883666992\n",
            "Epoch: 9/30, Batch: 80/91, Loss: 4.6820878982543945\n",
            "Epoch: 9/30, Batch: 81/91, Loss: 4.742411136627197\n",
            "Epoch: 9/30, Batch: 82/91, Loss: 4.734914779663086\n",
            "Epoch: 9/30, Batch: 83/91, Loss: 3.7747364044189453\n",
            "Epoch: 9/30, Batch: 84/91, Loss: 4.43978214263916\n",
            "Epoch: 9/30, Batch: 85/91, Loss: 3.928645133972168\n",
            "Epoch: 9/30, Batch: 86/91, Loss: 3.217352867126465\n",
            "Epoch: 9/30, Batch: 87/91, Loss: 4.6031880378723145\n",
            "Epoch: 9/30, Batch: 88/91, Loss: 4.007293701171875\n",
            "Epoch: 9/30, Batch: 89/91, Loss: 4.752719402313232\n",
            "Epoch: 9/30, Batch: 90/91, Loss: 5.001183032989502\n",
            "Epoch: 10/30, Batch: 0/91, Loss: 4.815169334411621\n",
            "Epoch: 10/30, Batch: 1/91, Loss: 4.884636878967285\n",
            "Epoch: 10/30, Batch: 2/91, Loss: 3.9999866485595703\n",
            "Epoch: 10/30, Batch: 3/91, Loss: 3.883697271347046\n",
            "Epoch: 10/30, Batch: 4/91, Loss: 4.8811421394348145\n",
            "Epoch: 10/30, Batch: 5/91, Loss: 3.9483344554901123\n",
            "Epoch: 10/30, Batch: 6/91, Loss: 4.2015380859375\n",
            "Epoch: 10/30, Batch: 7/91, Loss: 4.472907066345215\n",
            "Epoch: 10/30, Batch: 8/91, Loss: 3.9824161529541016\n",
            "Epoch: 10/30, Batch: 9/91, Loss: 4.127601146697998\n",
            "Epoch: 10/30, Batch: 10/91, Loss: 4.066063404083252\n",
            "Epoch: 10/30, Batch: 11/91, Loss: 4.104580402374268\n",
            "Epoch: 10/30, Batch: 12/91, Loss: 4.640604019165039\n",
            "Epoch: 10/30, Batch: 13/91, Loss: 3.8640763759613037\n",
            "Epoch: 10/30, Batch: 14/91, Loss: 3.733976125717163\n",
            "Epoch: 10/30, Batch: 15/91, Loss: 4.38123893737793\n",
            "Epoch: 10/30, Batch: 16/91, Loss: 4.190677642822266\n",
            "Epoch: 10/30, Batch: 17/91, Loss: 4.218017101287842\n",
            "Epoch: 10/30, Batch: 18/91, Loss: 3.9585676193237305\n",
            "Epoch: 10/30, Batch: 19/91, Loss: 2.983269691467285\n",
            "Epoch: 10/30, Batch: 20/91, Loss: 3.9249236583709717\n",
            "Epoch: 10/30, Batch: 21/91, Loss: 4.351902484893799\n",
            "Epoch: 10/30, Batch: 22/91, Loss: 3.790959358215332\n",
            "Epoch: 10/30, Batch: 23/91, Loss: 3.006415367126465\n",
            "Epoch: 10/30, Batch: 24/91, Loss: 4.540124416351318\n",
            "Epoch: 10/30, Batch: 25/91, Loss: 4.5088114738464355\n",
            "Epoch: 10/30, Batch: 26/91, Loss: 5.091207027435303\n",
            "Epoch: 10/30, Batch: 27/91, Loss: 3.1454875469207764\n",
            "Epoch: 10/30, Batch: 28/91, Loss: 4.720559597015381\n",
            "Epoch: 10/30, Batch: 29/91, Loss: 3.942467212677002\n",
            "Epoch: 10/30, Batch: 30/91, Loss: 4.535248279571533\n",
            "Epoch: 10/30, Batch: 31/91, Loss: 4.622109889984131\n",
            "Epoch: 10/30, Batch: 32/91, Loss: 4.666171550750732\n",
            "Epoch: 10/30, Batch: 33/91, Loss: 4.4539313316345215\n",
            "Epoch: 10/30, Batch: 34/91, Loss: 3.5374913215637207\n",
            "Epoch: 10/30, Batch: 35/91, Loss: 4.396888256072998\n",
            "Epoch: 10/30, Batch: 36/91, Loss: 3.881028890609741\n",
            "Epoch: 10/30, Batch: 37/91, Loss: 3.8989109992980957\n",
            "Epoch: 10/30, Batch: 38/91, Loss: 4.1586012840271\n",
            "Epoch: 10/30, Batch: 39/91, Loss: 3.7756235599517822\n",
            "Epoch: 10/30, Batch: 40/91, Loss: 4.502640247344971\n",
            "Epoch: 10/30, Batch: 41/91, Loss: 3.4974069595336914\n",
            "Epoch: 10/30, Batch: 42/91, Loss: 3.528562545776367\n",
            "Epoch: 10/30, Batch: 43/91, Loss: 3.36487078666687\n",
            "Epoch: 10/30, Batch: 44/91, Loss: 4.07866907119751\n",
            "Epoch: 10/30, Batch: 45/91, Loss: 4.400690078735352\n",
            "Epoch: 10/30, Batch: 46/91, Loss: 4.5437517166137695\n",
            "Epoch: 10/30, Batch: 47/91, Loss: 2.8355345726013184\n",
            "Epoch: 10/30, Batch: 48/91, Loss: 4.42733097076416\n",
            "Epoch: 10/30, Batch: 49/91, Loss: 3.753826856613159\n",
            "Epoch: 10/30, Batch: 50/91, Loss: 4.713518142700195\n",
            "Epoch: 10/30, Batch: 51/91, Loss: 4.342484951019287\n",
            "Epoch: 10/30, Batch: 52/91, Loss: 3.896144151687622\n",
            "Epoch: 10/30, Batch: 53/91, Loss: 4.4602155685424805\n",
            "Epoch: 10/30, Batch: 54/91, Loss: 3.8197827339172363\n",
            "Epoch: 10/30, Batch: 55/91, Loss: 3.6355342864990234\n",
            "Epoch: 10/30, Batch: 56/91, Loss: 4.685882091522217\n",
            "Epoch: 10/30, Batch: 57/91, Loss: 4.2768473625183105\n",
            "Epoch: 10/30, Batch: 58/91, Loss: 4.343993186950684\n",
            "Epoch: 10/30, Batch: 59/91, Loss: 4.462337493896484\n",
            "Epoch: 10/30, Batch: 60/91, Loss: 4.654911041259766\n",
            "Epoch: 10/30, Batch: 61/91, Loss: 4.5975799560546875\n",
            "Epoch: 10/30, Batch: 62/91, Loss: 4.237015724182129\n",
            "Epoch: 10/30, Batch: 63/91, Loss: 4.086304664611816\n",
            "Epoch: 10/30, Batch: 64/91, Loss: 4.756274223327637\n",
            "Epoch: 10/30, Batch: 65/91, Loss: 2.7240726947784424\n",
            "Epoch: 10/30, Batch: 66/91, Loss: 4.421433925628662\n",
            "Epoch: 10/30, Batch: 67/91, Loss: 4.766289234161377\n",
            "Epoch: 10/30, Batch: 68/91, Loss: 3.431644916534424\n",
            "Epoch: 10/30, Batch: 69/91, Loss: 3.369760513305664\n",
            "Epoch: 10/30, Batch: 70/91, Loss: 4.5477070808410645\n",
            "Epoch: 10/30, Batch: 71/91, Loss: 4.490678310394287\n",
            "Epoch: 10/30, Batch: 72/91, Loss: 2.4145150184631348\n",
            "Epoch: 10/30, Batch: 73/91, Loss: 4.138227462768555\n",
            "Epoch: 10/30, Batch: 74/91, Loss: 3.771183967590332\n",
            "Epoch: 10/30, Batch: 75/91, Loss: 3.5002601146698\n",
            "Epoch: 10/30, Batch: 76/91, Loss: 5.1714558601379395\n",
            "Epoch: 10/30, Batch: 77/91, Loss: 5.001800537109375\n",
            "Epoch: 10/30, Batch: 78/91, Loss: 4.107586860656738\n",
            "Epoch: 10/30, Batch: 79/91, Loss: 3.8561911582946777\n",
            "Epoch: 10/30, Batch: 80/91, Loss: 4.499088287353516\n",
            "Epoch: 10/30, Batch: 81/91, Loss: 4.944047451019287\n",
            "Epoch: 10/30, Batch: 82/91, Loss: 4.6470513343811035\n",
            "Epoch: 10/30, Batch: 83/91, Loss: 4.476784706115723\n",
            "Epoch: 10/30, Batch: 84/91, Loss: 4.717472553253174\n",
            "Epoch: 10/30, Batch: 85/91, Loss: 2.648171901702881\n",
            "Epoch: 10/30, Batch: 86/91, Loss: 3.5324394702911377\n",
            "Epoch: 10/30, Batch: 87/91, Loss: 4.3003249168396\n",
            "Epoch: 10/30, Batch: 88/91, Loss: 4.4777607917785645\n",
            "Epoch: 10/30, Batch: 89/91, Loss: 4.381056785583496\n",
            "Epoch: 10/30, Batch: 90/91, Loss: 4.422526836395264\n",
            "Epoch: 11/30, Batch: 0/91, Loss: 4.211888790130615\n",
            "Epoch: 11/30, Batch: 1/91, Loss: 3.7774314880371094\n",
            "Epoch: 11/30, Batch: 2/91, Loss: 4.923564434051514\n",
            "Epoch: 11/30, Batch: 3/91, Loss: 3.8926889896392822\n",
            "Epoch: 11/30, Batch: 4/91, Loss: 4.510426998138428\n",
            "Epoch: 11/30, Batch: 5/91, Loss: 4.3927178382873535\n",
            "Epoch: 11/30, Batch: 6/91, Loss: 4.731155872344971\n",
            "Epoch: 11/30, Batch: 7/91, Loss: 4.611809253692627\n",
            "Epoch: 11/30, Batch: 8/91, Loss: 4.392205238342285\n",
            "Epoch: 11/30, Batch: 9/91, Loss: 4.692507743835449\n",
            "Epoch: 11/30, Batch: 10/91, Loss: 3.624927043914795\n",
            "Epoch: 11/30, Batch: 11/91, Loss: 4.599432468414307\n",
            "Epoch: 11/30, Batch: 12/91, Loss: 3.509230852127075\n",
            "Epoch: 11/30, Batch: 13/91, Loss: 4.3390583992004395\n",
            "Epoch: 11/30, Batch: 14/91, Loss: 3.8110246658325195\n",
            "Epoch: 11/30, Batch: 15/91, Loss: 4.139708995819092\n",
            "Epoch: 11/30, Batch: 16/91, Loss: 4.393612861633301\n",
            "Epoch: 11/30, Batch: 17/91, Loss: 4.188177585601807\n",
            "Epoch: 11/30, Batch: 18/91, Loss: 4.255822658538818\n",
            "Epoch: 11/30, Batch: 19/91, Loss: 4.465649604797363\n",
            "Epoch: 11/30, Batch: 20/91, Loss: 4.937017917633057\n",
            "Epoch: 11/30, Batch: 21/91, Loss: 4.40291166305542\n",
            "Epoch: 11/30, Batch: 22/91, Loss: 3.7826805114746094\n",
            "Epoch: 11/30, Batch: 23/91, Loss: 4.466494560241699\n",
            "Epoch: 11/30, Batch: 24/91, Loss: 4.228881359100342\n",
            "Epoch: 11/30, Batch: 25/91, Loss: 4.257518291473389\n",
            "Epoch: 11/30, Batch: 26/91, Loss: 4.2696146965026855\n",
            "Epoch: 11/30, Batch: 27/91, Loss: 4.99445915222168\n",
            "Epoch: 11/30, Batch: 28/91, Loss: 4.9830121994018555\n",
            "Epoch: 11/30, Batch: 29/91, Loss: 4.0476226806640625\n",
            "Epoch: 11/30, Batch: 30/91, Loss: 4.210106372833252\n",
            "Epoch: 11/30, Batch: 31/91, Loss: 4.407135963439941\n",
            "Epoch: 11/30, Batch: 32/91, Loss: 3.592196464538574\n",
            "Epoch: 11/30, Batch: 33/91, Loss: 4.4056315422058105\n",
            "Epoch: 11/30, Batch: 34/91, Loss: 3.828655481338501\n",
            "Epoch: 11/30, Batch: 35/91, Loss: 3.933032751083374\n",
            "Epoch: 11/30, Batch: 36/91, Loss: 3.939026355743408\n",
            "Epoch: 11/30, Batch: 37/91, Loss: 3.3641655445098877\n",
            "Epoch: 11/30, Batch: 38/91, Loss: 3.878573179244995\n",
            "Epoch: 11/30, Batch: 39/91, Loss: 4.528450965881348\n",
            "Epoch: 11/30, Batch: 40/91, Loss: 3.9412169456481934\n",
            "Epoch: 11/30, Batch: 41/91, Loss: 3.999663829803467\n",
            "Epoch: 11/30, Batch: 42/91, Loss: 4.2294158935546875\n",
            "Epoch: 11/30, Batch: 43/91, Loss: 3.417132616043091\n",
            "Epoch: 11/30, Batch: 44/91, Loss: 4.448803901672363\n",
            "Epoch: 11/30, Batch: 45/91, Loss: 3.7886226177215576\n",
            "Epoch: 11/30, Batch: 46/91, Loss: 4.245561599731445\n",
            "Epoch: 11/30, Batch: 47/91, Loss: 4.303654193878174\n",
            "Epoch: 11/30, Batch: 48/91, Loss: 4.659757614135742\n",
            "Epoch: 11/30, Batch: 49/91, Loss: 4.15264892578125\n",
            "Epoch: 11/30, Batch: 50/91, Loss: 4.242913246154785\n",
            "Epoch: 11/30, Batch: 51/91, Loss: 3.593432664871216\n",
            "Epoch: 11/30, Batch: 52/91, Loss: 2.7617344856262207\n",
            "Epoch: 11/30, Batch: 53/91, Loss: 4.308159828186035\n",
            "Epoch: 11/30, Batch: 54/91, Loss: 3.6594290733337402\n",
            "Epoch: 11/30, Batch: 55/91, Loss: 4.255651473999023\n",
            "Epoch: 11/30, Batch: 56/91, Loss: 3.9787490367889404\n",
            "Epoch: 11/30, Batch: 57/91, Loss: 4.853262901306152\n",
            "Epoch: 11/30, Batch: 58/91, Loss: 4.268733501434326\n",
            "Epoch: 11/30, Batch: 59/91, Loss: 3.870198965072632\n",
            "Epoch: 11/30, Batch: 60/91, Loss: 4.9155707359313965\n",
            "Epoch: 11/30, Batch: 61/91, Loss: 3.804447650909424\n",
            "Epoch: 11/30, Batch: 62/91, Loss: 4.902958869934082\n",
            "Epoch: 11/30, Batch: 63/91, Loss: 3.9651565551757812\n",
            "Epoch: 11/30, Batch: 64/91, Loss: 4.96347188949585\n",
            "Epoch: 11/30, Batch: 65/91, Loss: 4.777779579162598\n",
            "Epoch: 11/30, Batch: 66/91, Loss: 2.95920991897583\n",
            "Epoch: 11/30, Batch: 67/91, Loss: 4.431098461151123\n",
            "Epoch: 11/30, Batch: 68/91, Loss: 4.456315040588379\n",
            "Epoch: 11/30, Batch: 69/91, Loss: 4.4301252365112305\n",
            "Epoch: 11/30, Batch: 70/91, Loss: 4.69114351272583\n",
            "Epoch: 11/30, Batch: 71/91, Loss: 4.134964942932129\n",
            "Epoch: 11/30, Batch: 72/91, Loss: 4.4835710525512695\n",
            "Epoch: 11/30, Batch: 73/91, Loss: 4.32342004776001\n",
            "Epoch: 11/30, Batch: 74/91, Loss: 3.605970621109009\n",
            "Epoch: 11/30, Batch: 75/91, Loss: 2.918673038482666\n",
            "Epoch: 11/30, Batch: 76/91, Loss: 4.190725803375244\n",
            "Epoch: 11/30, Batch: 77/91, Loss: 4.260167121887207\n",
            "Epoch: 11/30, Batch: 78/91, Loss: 3.8692736625671387\n",
            "Epoch: 11/30, Batch: 79/91, Loss: 4.729519367218018\n",
            "Epoch: 11/30, Batch: 80/91, Loss: 2.91922664642334\n",
            "Epoch: 11/30, Batch: 81/91, Loss: 4.293759822845459\n",
            "Epoch: 11/30, Batch: 82/91, Loss: 4.446698188781738\n",
            "Epoch: 11/30, Batch: 83/91, Loss: 4.86399507522583\n",
            "Epoch: 11/30, Batch: 84/91, Loss: 3.0023481845855713\n",
            "Epoch: 11/30, Batch: 85/91, Loss: 4.492120265960693\n",
            "Epoch: 11/30, Batch: 86/91, Loss: 3.865581750869751\n",
            "Epoch: 11/30, Batch: 87/91, Loss: 4.319903373718262\n",
            "Epoch: 11/30, Batch: 88/91, Loss: 4.336129665374756\n",
            "Epoch: 11/30, Batch: 89/91, Loss: 4.947677135467529\n",
            "Epoch: 11/30, Batch: 90/91, Loss: 4.279214382171631\n",
            "Epoch: 12/30, Batch: 0/91, Loss: 4.413517951965332\n",
            "Epoch: 12/30, Batch: 1/91, Loss: 4.110250949859619\n",
            "Epoch: 12/30, Batch: 2/91, Loss: 3.971673011779785\n",
            "Epoch: 12/30, Batch: 3/91, Loss: 4.428504943847656\n",
            "Epoch: 12/30, Batch: 4/91, Loss: 3.579019069671631\n",
            "Epoch: 12/30, Batch: 5/91, Loss: 4.354418754577637\n",
            "Epoch: 12/30, Batch: 6/91, Loss: 4.631633758544922\n",
            "Epoch: 12/30, Batch: 7/91, Loss: 4.0208234786987305\n",
            "Epoch: 12/30, Batch: 8/91, Loss: 4.306962490081787\n",
            "Epoch: 12/30, Batch: 9/91, Loss: 4.495708465576172\n",
            "Epoch: 12/30, Batch: 10/91, Loss: 3.9190011024475098\n",
            "Epoch: 12/30, Batch: 11/91, Loss: 4.141367435455322\n",
            "Epoch: 12/30, Batch: 12/91, Loss: 4.167128562927246\n",
            "Epoch: 12/30, Batch: 13/91, Loss: 4.624317169189453\n",
            "Epoch: 12/30, Batch: 14/91, Loss: 4.280343055725098\n",
            "Epoch: 12/30, Batch: 15/91, Loss: 3.8002490997314453\n",
            "Epoch: 12/30, Batch: 16/91, Loss: 4.807459831237793\n",
            "Epoch: 12/30, Batch: 17/91, Loss: 4.342635154724121\n",
            "Epoch: 12/30, Batch: 18/91, Loss: 4.89680290222168\n",
            "Epoch: 12/30, Batch: 19/91, Loss: 4.727972507476807\n",
            "Epoch: 12/30, Batch: 20/91, Loss: 4.0254435539245605\n",
            "Epoch: 12/30, Batch: 21/91, Loss: 4.418290138244629\n",
            "Epoch: 12/30, Batch: 22/91, Loss: 3.6059517860412598\n",
            "Epoch: 12/30, Batch: 23/91, Loss: 3.0599923133850098\n",
            "Epoch: 12/30, Batch: 24/91, Loss: 4.847106456756592\n",
            "Epoch: 12/30, Batch: 25/91, Loss: 3.6551010608673096\n",
            "Epoch: 12/30, Batch: 26/91, Loss: 2.719456434249878\n",
            "Epoch: 12/30, Batch: 27/91, Loss: 3.806938886642456\n",
            "Epoch: 12/30, Batch: 28/91, Loss: 3.654794216156006\n",
            "Epoch: 12/30, Batch: 29/91, Loss: 3.9423303604125977\n",
            "Epoch: 12/30, Batch: 30/91, Loss: 4.381203651428223\n",
            "Epoch: 12/30, Batch: 31/91, Loss: 2.6287264823913574\n",
            "Epoch: 12/30, Batch: 32/91, Loss: 4.621845245361328\n",
            "Epoch: 12/30, Batch: 33/91, Loss: 4.318058490753174\n",
            "Epoch: 12/30, Batch: 34/91, Loss: 5.366472244262695\n",
            "Epoch: 12/30, Batch: 35/91, Loss: 3.0983262062072754\n",
            "Epoch: 12/30, Batch: 36/91, Loss: 4.224793434143066\n",
            "Epoch: 12/30, Batch: 37/91, Loss: 3.853318214416504\n",
            "Epoch: 12/30, Batch: 38/91, Loss: 3.5417959690093994\n",
            "Epoch: 12/30, Batch: 39/91, Loss: 4.615637302398682\n",
            "Epoch: 12/30, Batch: 40/91, Loss: 2.850782871246338\n",
            "Epoch: 12/30, Batch: 41/91, Loss: 4.674233913421631\n",
            "Epoch: 12/30, Batch: 42/91, Loss: 4.659494400024414\n",
            "Epoch: 12/30, Batch: 43/91, Loss: 5.091245651245117\n",
            "Epoch: 12/30, Batch: 44/91, Loss: 3.090076446533203\n",
            "Epoch: 12/30, Batch: 45/91, Loss: 3.759584426879883\n",
            "Epoch: 12/30, Batch: 46/91, Loss: 4.147951126098633\n",
            "Epoch: 12/30, Batch: 47/91, Loss: 4.61964225769043\n",
            "Epoch: 12/30, Batch: 48/91, Loss: 3.954916477203369\n",
            "Epoch: 12/30, Batch: 49/91, Loss: 3.745791435241699\n",
            "Epoch: 12/30, Batch: 50/91, Loss: 4.539544105529785\n",
            "Epoch: 12/30, Batch: 51/91, Loss: 4.591166019439697\n",
            "Epoch: 12/30, Batch: 52/91, Loss: 3.7517964839935303\n",
            "Epoch: 12/30, Batch: 53/91, Loss: 3.7437875270843506\n",
            "Epoch: 12/30, Batch: 54/91, Loss: 3.775294303894043\n",
            "Epoch: 12/30, Batch: 55/91, Loss: 4.06940221786499\n",
            "Epoch: 12/30, Batch: 56/91, Loss: 4.419683933258057\n",
            "Epoch: 12/30, Batch: 57/91, Loss: 4.380425930023193\n",
            "Epoch: 12/30, Batch: 58/91, Loss: 4.78704833984375\n",
            "Epoch: 12/30, Batch: 59/91, Loss: 4.957047462463379\n",
            "Epoch: 12/30, Batch: 60/91, Loss: 4.464481353759766\n",
            "Epoch: 12/30, Batch: 61/91, Loss: 4.634016990661621\n",
            "Epoch: 12/30, Batch: 62/91, Loss: 3.9026992321014404\n",
            "Epoch: 12/30, Batch: 63/91, Loss: 4.960423946380615\n",
            "Epoch: 12/30, Batch: 64/91, Loss: 4.344509124755859\n",
            "Epoch: 12/30, Batch: 65/91, Loss: 3.2744157314300537\n",
            "Epoch: 12/30, Batch: 66/91, Loss: 4.199721336364746\n",
            "Epoch: 12/30, Batch: 67/91, Loss: 4.431842803955078\n",
            "Epoch: 12/30, Batch: 68/91, Loss: 3.4081099033355713\n",
            "Epoch: 12/30, Batch: 69/91, Loss: 5.056837558746338\n",
            "Epoch: 12/30, Batch: 70/91, Loss: 4.512078762054443\n",
            "Epoch: 12/30, Batch: 71/91, Loss: 4.798608779907227\n",
            "Epoch: 12/30, Batch: 72/91, Loss: 4.410268306732178\n",
            "Epoch: 12/30, Batch: 73/91, Loss: 4.023095607757568\n",
            "Epoch: 12/30, Batch: 74/91, Loss: 4.944668769836426\n",
            "Epoch: 12/30, Batch: 75/91, Loss: 4.000461578369141\n",
            "Epoch: 12/30, Batch: 76/91, Loss: 4.885232925415039\n",
            "Epoch: 12/30, Batch: 77/91, Loss: 3.749476194381714\n",
            "Epoch: 12/30, Batch: 78/91, Loss: 4.17080545425415\n",
            "Epoch: 12/30, Batch: 79/91, Loss: 3.6510722637176514\n",
            "Epoch: 12/30, Batch: 80/91, Loss: 4.865744590759277\n",
            "Epoch: 12/30, Batch: 81/91, Loss: 4.924144744873047\n",
            "Epoch: 12/30, Batch: 82/91, Loss: 4.4206366539001465\n",
            "Epoch: 12/30, Batch: 83/91, Loss: 3.891374111175537\n",
            "Epoch: 12/30, Batch: 84/91, Loss: 3.3883066177368164\n",
            "Epoch: 12/30, Batch: 85/91, Loss: 4.2625908851623535\n",
            "Epoch: 12/30, Batch: 86/91, Loss: 4.38457727432251\n",
            "Epoch: 12/30, Batch: 87/91, Loss: 3.807577133178711\n",
            "Epoch: 12/30, Batch: 88/91, Loss: 3.933483600616455\n",
            "Epoch: 12/30, Batch: 89/91, Loss: 4.4180450439453125\n",
            "Epoch: 12/30, Batch: 90/91, Loss: 4.396455764770508\n",
            "Epoch: 13/30, Batch: 0/91, Loss: 3.600468397140503\n",
            "Epoch: 13/30, Batch: 1/91, Loss: 4.3097758293151855\n",
            "Epoch: 13/30, Batch: 2/91, Loss: 4.992120265960693\n",
            "Epoch: 13/30, Batch: 3/91, Loss: 3.7099947929382324\n",
            "Epoch: 13/30, Batch: 4/91, Loss: 3.8268330097198486\n",
            "Epoch: 13/30, Batch: 5/91, Loss: 2.776273488998413\n",
            "Epoch: 13/30, Batch: 6/91, Loss: 4.080909252166748\n",
            "Epoch: 13/30, Batch: 7/91, Loss: 4.629119873046875\n",
            "Epoch: 13/30, Batch: 8/91, Loss: 4.680929183959961\n",
            "Epoch: 13/30, Batch: 9/91, Loss: 4.385373592376709\n",
            "Epoch: 13/30, Batch: 10/91, Loss: 5.266779899597168\n",
            "Epoch: 13/30, Batch: 11/91, Loss: 4.230225563049316\n",
            "Epoch: 13/30, Batch: 12/91, Loss: 4.877536296844482\n",
            "Epoch: 13/30, Batch: 13/91, Loss: 4.485373020172119\n",
            "Epoch: 13/30, Batch: 14/91, Loss: 4.337028503417969\n",
            "Epoch: 13/30, Batch: 15/91, Loss: 3.087448835372925\n",
            "Epoch: 13/30, Batch: 16/91, Loss: 4.140166282653809\n",
            "Epoch: 13/30, Batch: 17/91, Loss: 3.872324228286743\n",
            "Epoch: 13/30, Batch: 18/91, Loss: 4.014666557312012\n",
            "Epoch: 13/30, Batch: 19/91, Loss: 3.8464558124542236\n",
            "Epoch: 13/30, Batch: 20/91, Loss: 4.068900108337402\n",
            "Epoch: 13/30, Batch: 21/91, Loss: 4.298932075500488\n",
            "Epoch: 13/30, Batch: 22/91, Loss: 2.9045729637145996\n",
            "Epoch: 13/30, Batch: 23/91, Loss: 4.4129509925842285\n",
            "Epoch: 13/30, Batch: 24/91, Loss: 3.0300047397613525\n",
            "Epoch: 13/30, Batch: 25/91, Loss: 4.373002529144287\n",
            "Epoch: 13/30, Batch: 26/91, Loss: 3.742497205734253\n",
            "Epoch: 13/30, Batch: 27/91, Loss: 3.9990386962890625\n",
            "Epoch: 13/30, Batch: 28/91, Loss: 4.2138237953186035\n",
            "Epoch: 13/30, Batch: 29/91, Loss: 3.862950325012207\n",
            "Epoch: 13/30, Batch: 30/91, Loss: 3.8749444484710693\n",
            "Epoch: 13/30, Batch: 31/91, Loss: 4.530837059020996\n",
            "Epoch: 13/30, Batch: 32/91, Loss: 4.244101047515869\n",
            "Epoch: 13/30, Batch: 33/91, Loss: 4.257611274719238\n",
            "Epoch: 13/30, Batch: 34/91, Loss: 4.696770191192627\n",
            "Epoch: 13/30, Batch: 35/91, Loss: 4.1937713623046875\n",
            "Epoch: 13/30, Batch: 36/91, Loss: 3.6339166164398193\n",
            "Epoch: 13/30, Batch: 37/91, Loss: 4.010312080383301\n",
            "Epoch: 13/30, Batch: 38/91, Loss: 3.7731146812438965\n",
            "Epoch: 13/30, Batch: 39/91, Loss: 4.528004169464111\n",
            "Epoch: 13/30, Batch: 40/91, Loss: 3.92095685005188\n",
            "Epoch: 13/30, Batch: 41/91, Loss: 3.5453684329986572\n",
            "Epoch: 13/30, Batch: 42/91, Loss: 4.187397003173828\n",
            "Epoch: 13/30, Batch: 43/91, Loss: 4.368173599243164\n",
            "Epoch: 13/30, Batch: 44/91, Loss: 4.206132411956787\n",
            "Epoch: 13/30, Batch: 45/91, Loss: 4.8425822257995605\n",
            "Epoch: 13/30, Batch: 46/91, Loss: 4.812404155731201\n",
            "Epoch: 13/30, Batch: 47/91, Loss: 4.269207954406738\n",
            "Epoch: 13/30, Batch: 48/91, Loss: 4.803934097290039\n",
            "Epoch: 13/30, Batch: 49/91, Loss: 4.902370929718018\n",
            "Epoch: 13/30, Batch: 50/91, Loss: 3.947521686553955\n",
            "Epoch: 13/30, Batch: 51/91, Loss: 4.752850532531738\n",
            "Epoch: 13/30, Batch: 52/91, Loss: 4.587477207183838\n",
            "Epoch: 13/30, Batch: 53/91, Loss: 2.7216625213623047\n",
            "Epoch: 13/30, Batch: 54/91, Loss: 3.9613826274871826\n",
            "Epoch: 13/30, Batch: 55/91, Loss: 5.040332794189453\n",
            "Epoch: 13/30, Batch: 56/91, Loss: 4.3137102127075195\n",
            "Epoch: 13/30, Batch: 57/91, Loss: 4.271146297454834\n",
            "Epoch: 13/30, Batch: 58/91, Loss: 3.2892730236053467\n",
            "Epoch: 13/30, Batch: 59/91, Loss: 4.657393455505371\n",
            "Epoch: 13/30, Batch: 60/91, Loss: 3.0820367336273193\n",
            "Epoch: 13/30, Batch: 61/91, Loss: 4.451401710510254\n",
            "Epoch: 13/30, Batch: 62/91, Loss: 4.559969425201416\n",
            "Epoch: 13/30, Batch: 63/91, Loss: 4.181288719177246\n",
            "Epoch: 13/30, Batch: 64/91, Loss: 3.073908805847168\n",
            "Epoch: 13/30, Batch: 65/91, Loss: 4.2724480628967285\n",
            "Epoch: 13/30, Batch: 66/91, Loss: 3.595830202102661\n",
            "Epoch: 13/30, Batch: 67/91, Loss: 3.957612991333008\n",
            "Epoch: 13/30, Batch: 68/91, Loss: 4.331367015838623\n",
            "Epoch: 13/30, Batch: 69/91, Loss: 4.577712535858154\n",
            "Epoch: 13/30, Batch: 70/91, Loss: 4.723428726196289\n",
            "Epoch: 13/30, Batch: 71/91, Loss: 4.233474254608154\n",
            "Epoch: 13/30, Batch: 72/91, Loss: 4.24367618560791\n",
            "Epoch: 13/30, Batch: 73/91, Loss: 4.626250743865967\n",
            "Epoch: 13/30, Batch: 74/91, Loss: 4.581239223480225\n",
            "Epoch: 13/30, Batch: 75/91, Loss: 3.436974048614502\n",
            "Epoch: 13/30, Batch: 76/91, Loss: 3.921860933303833\n",
            "Epoch: 13/30, Batch: 77/91, Loss: 4.41361141204834\n",
            "Epoch: 13/30, Batch: 78/91, Loss: 4.013484954833984\n",
            "Epoch: 13/30, Batch: 79/91, Loss: 4.885900497436523\n",
            "Epoch: 13/30, Batch: 80/91, Loss: 4.462087631225586\n",
            "Epoch: 13/30, Batch: 81/91, Loss: 4.369189262390137\n",
            "Epoch: 13/30, Batch: 82/91, Loss: 3.8628413677215576\n",
            "Epoch: 13/30, Batch: 83/91, Loss: 3.520508289337158\n",
            "Epoch: 13/30, Batch: 84/91, Loss: 4.5396857261657715\n",
            "Epoch: 13/30, Batch: 85/91, Loss: 3.765786647796631\n",
            "Epoch: 13/30, Batch: 86/91, Loss: 4.474643707275391\n",
            "Epoch: 13/30, Batch: 87/91, Loss: 4.223923683166504\n",
            "Epoch: 13/30, Batch: 88/91, Loss: 2.8158295154571533\n",
            "Epoch: 13/30, Batch: 89/91, Loss: 3.789661407470703\n",
            "Epoch: 13/30, Batch: 90/91, Loss: 4.092021942138672\n",
            "Epoch: 14/30, Batch: 0/91, Loss: 4.085899829864502\n",
            "Epoch: 14/30, Batch: 1/91, Loss: 2.280017137527466\n",
            "Epoch: 14/30, Batch: 2/91, Loss: 4.092783451080322\n",
            "Epoch: 14/30, Batch: 3/91, Loss: 4.43761682510376\n",
            "Epoch: 14/30, Batch: 4/91, Loss: 4.546110153198242\n",
            "Epoch: 14/30, Batch: 5/91, Loss: 3.9465436935424805\n",
            "Epoch: 14/30, Batch: 6/91, Loss: 4.236887454986572\n",
            "Epoch: 14/30, Batch: 7/91, Loss: 4.244055271148682\n",
            "Epoch: 14/30, Batch: 8/91, Loss: 4.094602584838867\n",
            "Epoch: 14/30, Batch: 9/91, Loss: 4.446789741516113\n",
            "Epoch: 14/30, Batch: 10/91, Loss: 3.6353681087493896\n",
            "Epoch: 14/30, Batch: 11/91, Loss: 3.861496686935425\n",
            "Epoch: 14/30, Batch: 12/91, Loss: 4.163081169128418\n",
            "Epoch: 14/30, Batch: 13/91, Loss: 3.8253514766693115\n",
            "Epoch: 14/30, Batch: 14/91, Loss: 4.590933322906494\n",
            "Epoch: 14/30, Batch: 15/91, Loss: 4.461248874664307\n",
            "Epoch: 14/30, Batch: 16/91, Loss: 2.609851837158203\n",
            "Epoch: 14/30, Batch: 17/91, Loss: 3.8714261054992676\n",
            "Epoch: 14/30, Batch: 18/91, Loss: 2.9921047687530518\n",
            "Epoch: 14/30, Batch: 19/91, Loss: 4.576248645782471\n",
            "Epoch: 14/30, Batch: 20/91, Loss: 2.9424591064453125\n",
            "Epoch: 14/30, Batch: 21/91, Loss: 4.941848278045654\n",
            "Epoch: 14/30, Batch: 22/91, Loss: 4.251570224761963\n",
            "Epoch: 14/30, Batch: 23/91, Loss: 4.09080696105957\n",
            "Epoch: 14/30, Batch: 24/91, Loss: 4.28115177154541\n",
            "Epoch: 14/30, Batch: 25/91, Loss: 3.7450127601623535\n",
            "Epoch: 14/30, Batch: 26/91, Loss: 4.825161933898926\n",
            "Epoch: 14/30, Batch: 27/91, Loss: 4.327789783477783\n",
            "Epoch: 14/30, Batch: 28/91, Loss: 3.4176790714263916\n",
            "Epoch: 14/30, Batch: 29/91, Loss: 3.3741281032562256\n",
            "Epoch: 14/30, Batch: 30/91, Loss: 4.353504180908203\n",
            "Epoch: 14/30, Batch: 31/91, Loss: 4.976910591125488\n",
            "Epoch: 14/30, Batch: 32/91, Loss: 4.230477809906006\n",
            "Epoch: 14/30, Batch: 33/91, Loss: 4.550165176391602\n",
            "Epoch: 14/30, Batch: 34/91, Loss: 4.270125865936279\n",
            "Epoch: 14/30, Batch: 35/91, Loss: 4.859074115753174\n",
            "Epoch: 14/30, Batch: 36/91, Loss: 4.778079509735107\n",
            "Epoch: 14/30, Batch: 37/91, Loss: 3.464080333709717\n",
            "Epoch: 14/30, Batch: 38/91, Loss: 4.7296295166015625\n",
            "Epoch: 14/30, Batch: 39/91, Loss: 3.6382334232330322\n",
            "Epoch: 14/30, Batch: 40/91, Loss: 4.059970378875732\n",
            "Epoch: 14/30, Batch: 41/91, Loss: 4.6947503089904785\n",
            "Epoch: 14/30, Batch: 42/91, Loss: 4.9388251304626465\n",
            "Epoch: 14/30, Batch: 43/91, Loss: 4.97446870803833\n",
            "Epoch: 14/30, Batch: 44/91, Loss: 3.6707866191864014\n",
            "Epoch: 14/30, Batch: 45/91, Loss: 3.822862386703491\n",
            "Epoch: 14/30, Batch: 46/91, Loss: 4.4097819328308105\n",
            "Epoch: 14/30, Batch: 47/91, Loss: 3.756880521774292\n",
            "Epoch: 14/30, Batch: 48/91, Loss: 3.8093998432159424\n",
            "Epoch: 14/30, Batch: 49/91, Loss: 3.7319650650024414\n",
            "Epoch: 14/30, Batch: 50/91, Loss: 4.5885210037231445\n",
            "Epoch: 14/30, Batch: 51/91, Loss: 4.612235069274902\n",
            "Epoch: 14/30, Batch: 52/91, Loss: 4.238892078399658\n",
            "Epoch: 14/30, Batch: 53/91, Loss: 4.888938903808594\n",
            "Epoch: 14/30, Batch: 54/91, Loss: 4.422497749328613\n",
            "Epoch: 14/30, Batch: 55/91, Loss: 4.299124717712402\n",
            "Epoch: 14/30, Batch: 56/91, Loss: 4.540415287017822\n",
            "Epoch: 14/30, Batch: 57/91, Loss: 3.9225270748138428\n",
            "Epoch: 14/30, Batch: 58/91, Loss: 4.1300787925720215\n",
            "Epoch: 14/30, Batch: 59/91, Loss: 5.015860557556152\n",
            "Epoch: 14/30, Batch: 60/91, Loss: 4.404014587402344\n",
            "Epoch: 14/30, Batch: 61/91, Loss: 4.326485633850098\n",
            "Epoch: 14/30, Batch: 62/91, Loss: 4.324188709259033\n",
            "Epoch: 14/30, Batch: 63/91, Loss: 4.602488040924072\n",
            "Epoch: 14/30, Batch: 64/91, Loss: 4.333836078643799\n",
            "Epoch: 14/30, Batch: 65/91, Loss: 4.975644588470459\n",
            "Epoch: 14/30, Batch: 66/91, Loss: 4.574068546295166\n",
            "Epoch: 14/30, Batch: 67/91, Loss: 3.967593193054199\n",
            "Epoch: 14/30, Batch: 68/91, Loss: 2.712131977081299\n",
            "Epoch: 14/30, Batch: 69/91, Loss: 3.469477891921997\n",
            "Epoch: 14/30, Batch: 70/91, Loss: 4.0257134437561035\n",
            "Epoch: 14/30, Batch: 71/91, Loss: 4.027510166168213\n",
            "Epoch: 14/30, Batch: 72/91, Loss: 3.8237955570220947\n",
            "Epoch: 14/30, Batch: 73/91, Loss: 4.401214599609375\n",
            "Epoch: 14/30, Batch: 74/91, Loss: 4.034642696380615\n",
            "Epoch: 14/30, Batch: 75/91, Loss: 3.620537519454956\n",
            "Epoch: 14/30, Batch: 76/91, Loss: 4.128541469573975\n",
            "Epoch: 14/30, Batch: 77/91, Loss: 3.961803674697876\n",
            "Epoch: 14/30, Batch: 78/91, Loss: 3.9949467182159424\n",
            "Epoch: 14/30, Batch: 79/91, Loss: 3.7985479831695557\n",
            "Epoch: 14/30, Batch: 80/91, Loss: 3.9113729000091553\n",
            "Epoch: 14/30, Batch: 81/91, Loss: 3.1058969497680664\n",
            "Epoch: 14/30, Batch: 82/91, Loss: 3.8470382690429688\n",
            "Epoch: 14/30, Batch: 83/91, Loss: 4.579229831695557\n",
            "Epoch: 14/30, Batch: 84/91, Loss: 3.469494581222534\n",
            "Epoch: 14/30, Batch: 85/91, Loss: 3.8401072025299072\n",
            "Epoch: 14/30, Batch: 86/91, Loss: 4.675514221191406\n",
            "Epoch: 14/30, Batch: 87/91, Loss: 4.466396808624268\n",
            "Epoch: 14/30, Batch: 88/91, Loss: 4.318578243255615\n",
            "Epoch: 14/30, Batch: 89/91, Loss: 3.694074869155884\n",
            "Epoch: 14/30, Batch: 90/91, Loss: 4.760136127471924\n",
            "Epoch: 15/30, Batch: 0/91, Loss: 4.92124080657959\n",
            "Epoch: 15/30, Batch: 1/91, Loss: 4.061749458312988\n",
            "Epoch: 15/30, Batch: 2/91, Loss: 4.207831382751465\n",
            "Epoch: 15/30, Batch: 3/91, Loss: 3.3171608448028564\n",
            "Epoch: 15/30, Batch: 4/91, Loss: 4.102060317993164\n",
            "Epoch: 15/30, Batch: 5/91, Loss: 3.6685097217559814\n",
            "Epoch: 15/30, Batch: 6/91, Loss: 3.7130391597747803\n",
            "Epoch: 15/30, Batch: 7/91, Loss: 3.503981351852417\n",
            "Epoch: 15/30, Batch: 8/91, Loss: 4.214232444763184\n",
            "Epoch: 15/30, Batch: 9/91, Loss: 3.857883930206299\n",
            "Epoch: 15/30, Batch: 10/91, Loss: 4.089803218841553\n",
            "Epoch: 15/30, Batch: 11/91, Loss: 4.236642837524414\n",
            "Epoch: 15/30, Batch: 12/91, Loss: 4.284975051879883\n",
            "Epoch: 15/30, Batch: 13/91, Loss: 4.775310039520264\n",
            "Epoch: 15/30, Batch: 14/91, Loss: 4.9045634269714355\n",
            "Epoch: 15/30, Batch: 15/91, Loss: 4.608448505401611\n",
            "Epoch: 15/30, Batch: 16/91, Loss: 4.031878471374512\n",
            "Epoch: 15/30, Batch: 17/91, Loss: 4.309951305389404\n",
            "Epoch: 15/30, Batch: 18/91, Loss: 4.4344801902771\n",
            "Epoch: 15/30, Batch: 19/91, Loss: 4.238070964813232\n",
            "Epoch: 15/30, Batch: 20/91, Loss: 4.000328540802002\n",
            "Epoch: 15/30, Batch: 21/91, Loss: 3.9676220417022705\n",
            "Epoch: 15/30, Batch: 22/91, Loss: 3.8113908767700195\n",
            "Epoch: 15/30, Batch: 23/91, Loss: 3.5607006549835205\n",
            "Epoch: 15/30, Batch: 24/91, Loss: 4.223508834838867\n",
            "Epoch: 15/30, Batch: 25/91, Loss: 4.7003936767578125\n",
            "Epoch: 15/30, Batch: 26/91, Loss: 4.255785942077637\n",
            "Epoch: 15/30, Batch: 27/91, Loss: 4.5153398513793945\n",
            "Epoch: 15/30, Batch: 28/91, Loss: 4.50922966003418\n",
            "Epoch: 15/30, Batch: 29/91, Loss: 3.9983434677124023\n",
            "Epoch: 15/30, Batch: 30/91, Loss: 5.075742721557617\n",
            "Epoch: 15/30, Batch: 31/91, Loss: 3.581878185272217\n",
            "Epoch: 15/30, Batch: 32/91, Loss: 3.6979920864105225\n",
            "Epoch: 15/30, Batch: 33/91, Loss: 4.6243977546691895\n",
            "Epoch: 15/30, Batch: 34/91, Loss: 3.5417306423187256\n",
            "Epoch: 15/30, Batch: 35/91, Loss: 3.7142128944396973\n",
            "Epoch: 15/30, Batch: 36/91, Loss: 4.036864280700684\n",
            "Epoch: 15/30, Batch: 37/91, Loss: 3.5149290561676025\n",
            "Epoch: 15/30, Batch: 38/91, Loss: 4.008193492889404\n",
            "Epoch: 15/30, Batch: 39/91, Loss: 4.235166072845459\n",
            "Epoch: 15/30, Batch: 40/91, Loss: 4.471628665924072\n",
            "Epoch: 15/30, Batch: 41/91, Loss: 4.3547282218933105\n",
            "Epoch: 15/30, Batch: 42/91, Loss: 3.7418763637542725\n",
            "Epoch: 15/30, Batch: 43/91, Loss: 5.046538829803467\n",
            "Epoch: 15/30, Batch: 44/91, Loss: 3.680013418197632\n",
            "Epoch: 15/30, Batch: 45/91, Loss: 4.116613388061523\n",
            "Epoch: 15/30, Batch: 46/91, Loss: 4.37111759185791\n",
            "Epoch: 15/30, Batch: 47/91, Loss: 5.030636310577393\n",
            "Epoch: 15/30, Batch: 48/91, Loss: 4.478902816772461\n",
            "Epoch: 15/30, Batch: 49/91, Loss: 4.418049335479736\n",
            "Epoch: 15/30, Batch: 50/91, Loss: 4.211349010467529\n",
            "Epoch: 15/30, Batch: 51/91, Loss: 4.272348880767822\n",
            "Epoch: 15/30, Batch: 52/91, Loss: 4.313248157501221\n",
            "Epoch: 15/30, Batch: 53/91, Loss: 3.8622288703918457\n",
            "Epoch: 15/30, Batch: 54/91, Loss: 4.583637714385986\n",
            "Epoch: 15/30, Batch: 55/91, Loss: 4.09715461730957\n",
            "Epoch: 15/30, Batch: 56/91, Loss: 4.912528038024902\n",
            "Epoch: 15/30, Batch: 57/91, Loss: 4.641720771789551\n",
            "Epoch: 15/30, Batch: 58/91, Loss: 4.601973533630371\n",
            "Epoch: 15/30, Batch: 59/91, Loss: 3.818028450012207\n",
            "Epoch: 15/30, Batch: 60/91, Loss: 3.575282335281372\n",
            "Epoch: 15/30, Batch: 61/91, Loss: 2.8114912509918213\n",
            "Epoch: 15/30, Batch: 62/91, Loss: 3.915663003921509\n",
            "Epoch: 15/30, Batch: 63/91, Loss: 4.585245609283447\n",
            "Epoch: 15/30, Batch: 64/91, Loss: 3.837390184402466\n",
            "Epoch: 15/30, Batch: 65/91, Loss: 4.198661804199219\n",
            "Epoch: 15/30, Batch: 66/91, Loss: 4.540502071380615\n",
            "Epoch: 15/30, Batch: 67/91, Loss: 3.481318950653076\n",
            "Epoch: 15/30, Batch: 68/91, Loss: 3.8065176010131836\n",
            "Epoch: 15/30, Batch: 69/91, Loss: 4.780754089355469\n",
            "Epoch: 15/30, Batch: 70/91, Loss: 2.6018927097320557\n",
            "Epoch: 15/30, Batch: 71/91, Loss: 3.759467840194702\n",
            "Epoch: 15/30, Batch: 72/91, Loss: 3.7042553424835205\n",
            "Epoch: 15/30, Batch: 73/91, Loss: 4.3898420333862305\n",
            "Epoch: 15/30, Batch: 74/91, Loss: 5.073448657989502\n",
            "Epoch: 15/30, Batch: 75/91, Loss: 2.9299521446228027\n",
            "Epoch: 15/30, Batch: 76/91, Loss: 5.127437591552734\n",
            "Epoch: 15/30, Batch: 77/91, Loss: 4.364255905151367\n",
            "Epoch: 15/30, Batch: 78/91, Loss: 4.3334808349609375\n",
            "Epoch: 15/30, Batch: 79/91, Loss: 4.4325270652771\n",
            "Epoch: 15/30, Batch: 80/91, Loss: 3.797116279602051\n",
            "Epoch: 15/30, Batch: 81/91, Loss: 4.501945972442627\n",
            "Epoch: 15/30, Batch: 82/91, Loss: 4.639093399047852\n",
            "Epoch: 15/30, Batch: 83/91, Loss: 4.598788738250732\n",
            "Epoch: 15/30, Batch: 84/91, Loss: 4.435388088226318\n",
            "Epoch: 15/30, Batch: 85/91, Loss: 4.070627212524414\n",
            "Epoch: 15/30, Batch: 86/91, Loss: 4.629709243774414\n",
            "Epoch: 15/30, Batch: 87/91, Loss: 4.121072769165039\n",
            "Epoch: 15/30, Batch: 88/91, Loss: 4.312276363372803\n",
            "Epoch: 15/30, Batch: 89/91, Loss: 4.834990501403809\n",
            "Epoch: 15/30, Batch: 90/91, Loss: 4.00964879989624\n",
            "Epoch: 16/30, Batch: 0/91, Loss: 4.248551845550537\n",
            "Epoch: 16/30, Batch: 1/91, Loss: 4.450883865356445\n",
            "Epoch: 16/30, Batch: 2/91, Loss: 4.373482704162598\n",
            "Epoch: 16/30, Batch: 3/91, Loss: 4.4878621101379395\n",
            "Epoch: 16/30, Batch: 4/91, Loss: 4.61122989654541\n",
            "Epoch: 16/30, Batch: 5/91, Loss: 3.718252182006836\n",
            "Epoch: 16/30, Batch: 6/91, Loss: 3.9216697216033936\n",
            "Epoch: 16/30, Batch: 7/91, Loss: 3.803802967071533\n",
            "Epoch: 16/30, Batch: 8/91, Loss: 4.269526958465576\n",
            "Epoch: 16/30, Batch: 9/91, Loss: 4.128829479217529\n",
            "Epoch: 16/30, Batch: 10/91, Loss: 4.145570278167725\n",
            "Epoch: 16/30, Batch: 11/91, Loss: 3.2405166625976562\n",
            "Epoch: 16/30, Batch: 12/91, Loss: 4.360134124755859\n",
            "Epoch: 16/30, Batch: 13/91, Loss: 4.757061004638672\n",
            "Epoch: 16/30, Batch: 14/91, Loss: 3.707958698272705\n",
            "Epoch: 16/30, Batch: 15/91, Loss: 2.2288026809692383\n",
            "Epoch: 16/30, Batch: 16/91, Loss: 4.31261682510376\n",
            "Epoch: 16/30, Batch: 17/91, Loss: 4.051259994506836\n",
            "Epoch: 16/30, Batch: 18/91, Loss: 4.479319095611572\n",
            "Epoch: 16/30, Batch: 19/91, Loss: 3.652517080307007\n",
            "Epoch: 16/30, Batch: 20/91, Loss: 4.151636123657227\n",
            "Epoch: 16/30, Batch: 21/91, Loss: 4.3310675621032715\n",
            "Epoch: 16/30, Batch: 22/91, Loss: 2.9874579906463623\n",
            "Epoch: 16/30, Batch: 23/91, Loss: 4.921468734741211\n",
            "Epoch: 16/30, Batch: 24/91, Loss: 4.06593132019043\n",
            "Epoch: 16/30, Batch: 25/91, Loss: 4.1922783851623535\n",
            "Epoch: 16/30, Batch: 26/91, Loss: 4.4171319007873535\n",
            "Epoch: 16/30, Batch: 27/91, Loss: 3.825500726699829\n",
            "Epoch: 16/30, Batch: 28/91, Loss: 3.978489398956299\n",
            "Epoch: 16/30, Batch: 29/91, Loss: 2.8822481632232666\n",
            "Epoch: 16/30, Batch: 30/91, Loss: 4.237846851348877\n",
            "Epoch: 16/30, Batch: 31/91, Loss: 5.0637712478637695\n",
            "Epoch: 16/30, Batch: 32/91, Loss: 5.032538414001465\n",
            "Epoch: 16/30, Batch: 33/91, Loss: 4.807395935058594\n",
            "Epoch: 16/30, Batch: 34/91, Loss: 4.49190616607666\n",
            "Epoch: 16/30, Batch: 35/91, Loss: 3.0623738765716553\n",
            "Epoch: 16/30, Batch: 36/91, Loss: 3.4665746688842773\n",
            "Epoch: 16/30, Batch: 37/91, Loss: 4.627345085144043\n",
            "Epoch: 16/30, Batch: 38/91, Loss: 4.262214183807373\n",
            "Epoch: 16/30, Batch: 39/91, Loss: 3.378239631652832\n",
            "Epoch: 16/30, Batch: 40/91, Loss: 3.8547539710998535\n",
            "Epoch: 16/30, Batch: 41/91, Loss: 4.4608683586120605\n",
            "Epoch: 16/30, Batch: 42/91, Loss: 4.049843788146973\n",
            "Epoch: 16/30, Batch: 43/91, Loss: 3.6684165000915527\n",
            "Epoch: 16/30, Batch: 44/91, Loss: 3.4021267890930176\n",
            "Epoch: 16/30, Batch: 45/91, Loss: 4.1814165115356445\n",
            "Epoch: 16/30, Batch: 46/91, Loss: 4.534523010253906\n",
            "Epoch: 16/30, Batch: 47/91, Loss: 4.283954620361328\n",
            "Epoch: 16/30, Batch: 48/91, Loss: 4.40467643737793\n",
            "Epoch: 16/30, Batch: 49/91, Loss: 4.6177592277526855\n",
            "Epoch: 16/30, Batch: 50/91, Loss: 3.878425359725952\n",
            "Epoch: 16/30, Batch: 51/91, Loss: 3.6991374492645264\n",
            "Epoch: 16/30, Batch: 52/91, Loss: 2.6598706245422363\n",
            "Epoch: 16/30, Batch: 53/91, Loss: 4.891089916229248\n",
            "Epoch: 16/30, Batch: 54/91, Loss: 4.015685081481934\n",
            "Epoch: 16/30, Batch: 55/91, Loss: 4.627018451690674\n",
            "Epoch: 16/30, Batch: 56/91, Loss: 4.2589545249938965\n",
            "Epoch: 16/30, Batch: 57/91, Loss: 4.551281929016113\n",
            "Epoch: 16/30, Batch: 58/91, Loss: 4.557013034820557\n",
            "Epoch: 16/30, Batch: 59/91, Loss: 2.873758554458618\n",
            "Epoch: 16/30, Batch: 60/91, Loss: 3.495511770248413\n",
            "Epoch: 16/30, Batch: 61/91, Loss: 4.165523052215576\n",
            "Epoch: 16/30, Batch: 62/91, Loss: 3.6578147411346436\n",
            "Epoch: 16/30, Batch: 63/91, Loss: 4.497426986694336\n",
            "Epoch: 16/30, Batch: 64/91, Loss: 4.185144424438477\n",
            "Epoch: 16/30, Batch: 65/91, Loss: 3.686258554458618\n",
            "Epoch: 16/30, Batch: 66/91, Loss: 4.91871452331543\n",
            "Epoch: 16/30, Batch: 67/91, Loss: 5.165319919586182\n",
            "Epoch: 16/30, Batch: 68/91, Loss: 4.77944278717041\n",
            "Epoch: 16/30, Batch: 69/91, Loss: 4.1065168380737305\n",
            "Epoch: 16/30, Batch: 70/91, Loss: 3.584780216217041\n",
            "Epoch: 16/30, Batch: 71/91, Loss: 3.8286538124084473\n",
            "Epoch: 16/30, Batch: 72/91, Loss: 4.0077433586120605\n",
            "Epoch: 16/30, Batch: 73/91, Loss: 4.282450199127197\n",
            "Epoch: 16/30, Batch: 74/91, Loss: 3.7949695587158203\n",
            "Epoch: 16/30, Batch: 75/91, Loss: 3.796457290649414\n",
            "Epoch: 16/30, Batch: 76/91, Loss: 4.336103916168213\n",
            "Epoch: 16/30, Batch: 77/91, Loss: 4.3752546310424805\n",
            "Epoch: 16/30, Batch: 78/91, Loss: 3.412731409072876\n",
            "Epoch: 16/30, Batch: 79/91, Loss: 3.7125635147094727\n",
            "Epoch: 16/30, Batch: 80/91, Loss: 4.617044448852539\n",
            "Epoch: 16/30, Batch: 81/91, Loss: 4.497039318084717\n",
            "Epoch: 16/30, Batch: 82/91, Loss: 4.063952922821045\n",
            "Epoch: 16/30, Batch: 83/91, Loss: 4.682337284088135\n",
            "Epoch: 16/30, Batch: 84/91, Loss: 3.807950496673584\n",
            "Epoch: 16/30, Batch: 85/91, Loss: 5.17065954208374\n",
            "Epoch: 16/30, Batch: 86/91, Loss: 4.638034343719482\n",
            "Epoch: 16/30, Batch: 87/91, Loss: 4.777190685272217\n",
            "Epoch: 16/30, Batch: 88/91, Loss: 3.9714584350585938\n",
            "Epoch: 16/30, Batch: 89/91, Loss: 4.641266822814941\n",
            "Epoch: 16/30, Batch: 90/91, Loss: 4.6966094970703125\n",
            "Epoch: 17/30, Batch: 0/91, Loss: 4.281737327575684\n",
            "Epoch: 17/30, Batch: 1/91, Loss: 4.494156360626221\n",
            "Epoch: 17/30, Batch: 2/91, Loss: 4.6702165603637695\n",
            "Epoch: 17/30, Batch: 3/91, Loss: 4.937775135040283\n",
            "Epoch: 17/30, Batch: 4/91, Loss: 4.5241546630859375\n",
            "Epoch: 17/30, Batch: 5/91, Loss: 4.184665679931641\n",
            "Epoch: 17/30, Batch: 6/91, Loss: 3.6050865650177\n",
            "Epoch: 17/30, Batch: 7/91, Loss: 4.855442047119141\n",
            "Epoch: 17/30, Batch: 8/91, Loss: 3.4369490146636963\n",
            "Epoch: 17/30, Batch: 9/91, Loss: 4.742279052734375\n",
            "Epoch: 17/30, Batch: 10/91, Loss: 4.639085292816162\n",
            "Epoch: 17/30, Batch: 11/91, Loss: 4.876096248626709\n",
            "Epoch: 17/30, Batch: 12/91, Loss: 3.7404227256774902\n",
            "Epoch: 17/30, Batch: 13/91, Loss: 3.1757001876831055\n",
            "Epoch: 17/30, Batch: 14/91, Loss: 3.343177318572998\n",
            "Epoch: 17/30, Batch: 15/91, Loss: 4.749746322631836\n",
            "Epoch: 17/30, Batch: 16/91, Loss: 3.6782851219177246\n",
            "Epoch: 17/30, Batch: 17/91, Loss: 4.206821918487549\n",
            "Epoch: 17/30, Batch: 18/91, Loss: 2.9581549167633057\n",
            "Epoch: 17/30, Batch: 19/91, Loss: 4.376750946044922\n",
            "Epoch: 17/30, Batch: 20/91, Loss: 4.782260417938232\n",
            "Epoch: 17/30, Batch: 21/91, Loss: 3.572418212890625\n",
            "Epoch: 17/30, Batch: 22/91, Loss: 4.032536029815674\n",
            "Epoch: 17/30, Batch: 23/91, Loss: 4.109915256500244\n",
            "Epoch: 17/30, Batch: 24/91, Loss: 4.8896708488464355\n",
            "Epoch: 17/30, Batch: 25/91, Loss: 4.3066558837890625\n",
            "Epoch: 17/30, Batch: 26/91, Loss: 4.579308032989502\n",
            "Epoch: 17/30, Batch: 27/91, Loss: 3.719789743423462\n",
            "Epoch: 17/30, Batch: 28/91, Loss: 4.201433181762695\n",
            "Epoch: 17/30, Batch: 29/91, Loss: 4.659740447998047\n",
            "Epoch: 17/30, Batch: 30/91, Loss: 4.004370212554932\n",
            "Epoch: 17/30, Batch: 31/91, Loss: 4.420470237731934\n",
            "Epoch: 17/30, Batch: 32/91, Loss: 4.118063926696777\n",
            "Epoch: 17/30, Batch: 33/91, Loss: 4.587090969085693\n",
            "Epoch: 17/30, Batch: 34/91, Loss: 4.745124816894531\n",
            "Epoch: 17/30, Batch: 35/91, Loss: 4.422572612762451\n",
            "Epoch: 17/30, Batch: 36/91, Loss: 4.2530083656311035\n",
            "Epoch: 17/30, Batch: 37/91, Loss: 4.2104291915893555\n",
            "Epoch: 17/30, Batch: 38/91, Loss: 3.440950393676758\n",
            "Epoch: 17/30, Batch: 39/91, Loss: 3.7795543670654297\n",
            "Epoch: 17/30, Batch: 40/91, Loss: 4.587761878967285\n",
            "Epoch: 17/30, Batch: 41/91, Loss: 3.5914223194122314\n",
            "Epoch: 17/30, Batch: 42/91, Loss: 4.340335369110107\n",
            "Epoch: 17/30, Batch: 43/91, Loss: 5.025108337402344\n",
            "Epoch: 17/30, Batch: 44/91, Loss: 4.691183090209961\n",
            "Epoch: 17/30, Batch: 45/91, Loss: 4.528820037841797\n",
            "Epoch: 17/30, Batch: 46/91, Loss: 4.302602291107178\n",
            "Epoch: 17/30, Batch: 47/91, Loss: 4.559985637664795\n",
            "Epoch: 17/30, Batch: 48/91, Loss: 2.9229118824005127\n",
            "Epoch: 17/30, Batch: 49/91, Loss: 3.7010905742645264\n",
            "Epoch: 17/30, Batch: 50/91, Loss: 3.4674153327941895\n",
            "Epoch: 17/30, Batch: 51/91, Loss: 2.7902114391326904\n",
            "Epoch: 17/30, Batch: 52/91, Loss: 4.133432865142822\n",
            "Epoch: 17/30, Batch: 53/91, Loss: 3.3773014545440674\n",
            "Epoch: 17/30, Batch: 54/91, Loss: 4.4686126708984375\n",
            "Epoch: 17/30, Batch: 55/91, Loss: 4.390988826751709\n",
            "Epoch: 17/30, Batch: 56/91, Loss: 2.5940146446228027\n",
            "Epoch: 17/30, Batch: 57/91, Loss: 4.1503729820251465\n",
            "Epoch: 17/30, Batch: 58/91, Loss: 4.551872730255127\n",
            "Epoch: 17/30, Batch: 59/91, Loss: 3.8756823539733887\n",
            "Epoch: 17/30, Batch: 60/91, Loss: 4.589166164398193\n",
            "Epoch: 17/30, Batch: 61/91, Loss: 3.7238457202911377\n",
            "Epoch: 17/30, Batch: 62/91, Loss: 4.10004997253418\n",
            "Epoch: 17/30, Batch: 63/91, Loss: 3.616201639175415\n",
            "Epoch: 17/30, Batch: 64/91, Loss: 5.063082218170166\n",
            "Epoch: 17/30, Batch: 65/91, Loss: 3.8721117973327637\n",
            "Epoch: 17/30, Batch: 66/91, Loss: 4.57451868057251\n",
            "Epoch: 17/30, Batch: 67/91, Loss: 4.067948341369629\n",
            "Epoch: 17/30, Batch: 68/91, Loss: 3.9048023223876953\n",
            "Epoch: 17/30, Batch: 69/91, Loss: 3.510869264602661\n",
            "Epoch: 17/30, Batch: 70/91, Loss: 4.311092853546143\n",
            "Epoch: 17/30, Batch: 71/91, Loss: 3.610008716583252\n",
            "Epoch: 17/30, Batch: 72/91, Loss: 4.781557559967041\n",
            "Epoch: 17/30, Batch: 73/91, Loss: 4.014317989349365\n",
            "Epoch: 17/30, Batch: 74/91, Loss: 4.374273300170898\n",
            "Epoch: 17/30, Batch: 75/91, Loss: 4.912860870361328\n",
            "Epoch: 17/30, Batch: 76/91, Loss: 3.98122501373291\n",
            "Epoch: 17/30, Batch: 77/91, Loss: 3.7683842182159424\n",
            "Epoch: 17/30, Batch: 78/91, Loss: 4.8801727294921875\n",
            "Epoch: 17/30, Batch: 79/91, Loss: 4.261317253112793\n",
            "Epoch: 17/30, Batch: 80/91, Loss: 4.469344139099121\n",
            "Epoch: 17/30, Batch: 81/91, Loss: 3.7036795616149902\n",
            "Epoch: 17/30, Batch: 82/91, Loss: 5.036988735198975\n",
            "Epoch: 17/30, Batch: 83/91, Loss: 4.982177734375\n",
            "Epoch: 17/30, Batch: 84/91, Loss: 4.447507381439209\n",
            "Epoch: 17/30, Batch: 85/91, Loss: 4.446367263793945\n",
            "Epoch: 17/30, Batch: 86/91, Loss: 3.683906316757202\n",
            "Epoch: 17/30, Batch: 87/91, Loss: 3.9379172325134277\n",
            "Epoch: 17/30, Batch: 88/91, Loss: 3.9710724353790283\n",
            "Epoch: 17/30, Batch: 89/91, Loss: 4.623199462890625\n",
            "Epoch: 17/30, Batch: 90/91, Loss: 4.36224889755249\n",
            "Epoch: 18/30, Batch: 0/91, Loss: 3.840341091156006\n",
            "Epoch: 18/30, Batch: 1/91, Loss: 3.177553176879883\n",
            "Epoch: 18/30, Batch: 2/91, Loss: 4.317824840545654\n",
            "Epoch: 18/30, Batch: 3/91, Loss: 4.31072998046875\n",
            "Epoch: 18/30, Batch: 4/91, Loss: 3.8843929767608643\n",
            "Epoch: 18/30, Batch: 5/91, Loss: 4.466851234436035\n",
            "Epoch: 18/30, Batch: 6/91, Loss: 3.373321294784546\n",
            "Epoch: 18/30, Batch: 7/91, Loss: 4.444931983947754\n",
            "Epoch: 18/30, Batch: 8/91, Loss: 4.544459819793701\n",
            "Epoch: 18/30, Batch: 9/91, Loss: 4.123254299163818\n",
            "Epoch: 18/30, Batch: 10/91, Loss: 4.362428665161133\n",
            "Epoch: 18/30, Batch: 11/91, Loss: 4.231505870819092\n",
            "Epoch: 18/30, Batch: 12/91, Loss: 4.961660385131836\n",
            "Epoch: 18/30, Batch: 13/91, Loss: 4.386806011199951\n",
            "Epoch: 18/30, Batch: 14/91, Loss: 4.3389482498168945\n",
            "Epoch: 18/30, Batch: 15/91, Loss: 4.505738258361816\n",
            "Epoch: 18/30, Batch: 16/91, Loss: 3.6803953647613525\n",
            "Epoch: 18/30, Batch: 17/91, Loss: 4.5448431968688965\n",
            "Epoch: 18/30, Batch: 18/91, Loss: 4.332322120666504\n",
            "Epoch: 18/30, Batch: 19/91, Loss: 4.848481178283691\n",
            "Epoch: 18/30, Batch: 20/91, Loss: 4.818864345550537\n",
            "Epoch: 18/30, Batch: 21/91, Loss: 4.075596809387207\n",
            "Epoch: 18/30, Batch: 22/91, Loss: 4.264819622039795\n",
            "Epoch: 18/30, Batch: 23/91, Loss: 3.860316038131714\n",
            "Epoch: 18/30, Batch: 24/91, Loss: 3.6883599758148193\n",
            "Epoch: 18/30, Batch: 25/91, Loss: 5.202664375305176\n",
            "Epoch: 18/30, Batch: 26/91, Loss: 3.742743492126465\n",
            "Epoch: 18/30, Batch: 27/91, Loss: 3.4303102493286133\n",
            "Epoch: 18/30, Batch: 28/91, Loss: 4.541779041290283\n",
            "Epoch: 18/30, Batch: 29/91, Loss: 3.6123206615448\n",
            "Epoch: 18/30, Batch: 30/91, Loss: 4.120141506195068\n",
            "Epoch: 18/30, Batch: 31/91, Loss: 4.663271903991699\n",
            "Epoch: 18/30, Batch: 32/91, Loss: 4.477385520935059\n",
            "Epoch: 18/30, Batch: 33/91, Loss: 3.663800001144409\n",
            "Epoch: 18/30, Batch: 34/91, Loss: 3.922232151031494\n",
            "Epoch: 18/30, Batch: 35/91, Loss: 3.753261089324951\n",
            "Epoch: 18/30, Batch: 36/91, Loss: 4.446697235107422\n",
            "Epoch: 18/30, Batch: 37/91, Loss: 3.359221935272217\n",
            "Epoch: 18/30, Batch: 38/91, Loss: 3.7891767024993896\n",
            "Epoch: 18/30, Batch: 39/91, Loss: 4.137439727783203\n",
            "Epoch: 18/30, Batch: 40/91, Loss: 4.243114471435547\n",
            "Epoch: 18/30, Batch: 41/91, Loss: 3.9801247119903564\n",
            "Epoch: 18/30, Batch: 42/91, Loss: 4.635645389556885\n",
            "Epoch: 18/30, Batch: 43/91, Loss: 3.935743808746338\n",
            "Epoch: 18/30, Batch: 44/91, Loss: 4.446092128753662\n",
            "Epoch: 18/30, Batch: 45/91, Loss: 3.8948049545288086\n",
            "Epoch: 18/30, Batch: 46/91, Loss: 3.7752649784088135\n",
            "Epoch: 18/30, Batch: 47/91, Loss: 4.766568660736084\n",
            "Epoch: 18/30, Batch: 48/91, Loss: 4.15146541595459\n",
            "Epoch: 18/30, Batch: 49/91, Loss: 4.747064113616943\n",
            "Epoch: 18/30, Batch: 50/91, Loss: 4.9560089111328125\n",
            "Epoch: 18/30, Batch: 51/91, Loss: 4.556362152099609\n",
            "Epoch: 18/30, Batch: 52/91, Loss: 3.3708982467651367\n",
            "Epoch: 18/30, Batch: 53/91, Loss: 3.715498447418213\n",
            "Epoch: 18/30, Batch: 54/91, Loss: 3.946472644805908\n",
            "Epoch: 18/30, Batch: 55/91, Loss: 3.9790923595428467\n",
            "Epoch: 18/30, Batch: 56/91, Loss: 4.241395950317383\n",
            "Epoch: 18/30, Batch: 57/91, Loss: 4.096555709838867\n",
            "Epoch: 18/30, Batch: 58/91, Loss: 4.1295061111450195\n",
            "Epoch: 18/30, Batch: 59/91, Loss: 4.301685333251953\n",
            "Epoch: 18/30, Batch: 60/91, Loss: 4.189099311828613\n",
            "Epoch: 18/30, Batch: 61/91, Loss: 4.344760417938232\n",
            "Epoch: 18/30, Batch: 62/91, Loss: 3.9300479888916016\n",
            "Epoch: 18/30, Batch: 63/91, Loss: 4.3155927658081055\n",
            "Epoch: 18/30, Batch: 64/91, Loss: 4.883912563323975\n",
            "Epoch: 18/30, Batch: 65/91, Loss: 4.421267509460449\n",
            "Epoch: 18/30, Batch: 66/91, Loss: 4.037320613861084\n",
            "Epoch: 18/30, Batch: 67/91, Loss: 2.9046387672424316\n",
            "Epoch: 18/30, Batch: 68/91, Loss: 3.8487296104431152\n",
            "Epoch: 18/30, Batch: 69/91, Loss: 4.291247844696045\n",
            "Epoch: 18/30, Batch: 70/91, Loss: 2.588681936264038\n",
            "Epoch: 18/30, Batch: 71/91, Loss: 3.6188390254974365\n",
            "Epoch: 18/30, Batch: 72/91, Loss: 4.3836989402771\n",
            "Epoch: 18/30, Batch: 73/91, Loss: 4.777173042297363\n",
            "Epoch: 18/30, Batch: 74/91, Loss: 4.262404441833496\n",
            "Epoch: 18/30, Batch: 75/91, Loss: 4.526161193847656\n",
            "Epoch: 18/30, Batch: 76/91, Loss: 4.184595584869385\n",
            "Epoch: 18/30, Batch: 77/91, Loss: 4.604407787322998\n",
            "Epoch: 18/30, Batch: 78/91, Loss: 2.7808897495269775\n",
            "Epoch: 18/30, Batch: 79/91, Loss: 4.906431674957275\n",
            "Epoch: 18/30, Batch: 80/91, Loss: 3.5320417881011963\n",
            "Epoch: 18/30, Batch: 81/91, Loss: 3.980245590209961\n",
            "Epoch: 18/30, Batch: 82/91, Loss: 5.00491189956665\n",
            "Epoch: 18/30, Batch: 83/91, Loss: 4.697444438934326\n",
            "Epoch: 18/30, Batch: 84/91, Loss: 4.375575542449951\n",
            "Epoch: 18/30, Batch: 85/91, Loss: 4.787572383880615\n",
            "Epoch: 18/30, Batch: 86/91, Loss: 3.210467576980591\n",
            "Epoch: 18/30, Batch: 87/91, Loss: 4.296596050262451\n",
            "Epoch: 18/30, Batch: 88/91, Loss: 3.6376349925994873\n",
            "Epoch: 18/30, Batch: 89/91, Loss: 2.2721850872039795\n",
            "Epoch: 18/30, Batch: 90/91, Loss: 2.9510796070098877\n",
            "Epoch: 19/30, Batch: 0/91, Loss: 4.384283542633057\n",
            "Epoch: 19/30, Batch: 1/91, Loss: 3.8972623348236084\n",
            "Epoch: 19/30, Batch: 2/91, Loss: 4.167635440826416\n",
            "Epoch: 19/30, Batch: 3/91, Loss: 3.5466065406799316\n",
            "Epoch: 19/30, Batch: 4/91, Loss: 4.361366271972656\n",
            "Epoch: 19/30, Batch: 5/91, Loss: 3.5882341861724854\n",
            "Epoch: 19/30, Batch: 6/91, Loss: 3.7377071380615234\n",
            "Epoch: 19/30, Batch: 7/91, Loss: 2.6818716526031494\n",
            "Epoch: 19/30, Batch: 8/91, Loss: 3.7863521575927734\n",
            "Epoch: 19/30, Batch: 9/91, Loss: 3.7495954036712646\n",
            "Epoch: 19/30, Batch: 10/91, Loss: 4.621786594390869\n",
            "Epoch: 19/30, Batch: 11/91, Loss: 3.7910521030426025\n",
            "Epoch: 19/30, Batch: 12/91, Loss: 4.259028434753418\n",
            "Epoch: 19/30, Batch: 13/91, Loss: 3.445321559906006\n",
            "Epoch: 19/30, Batch: 14/91, Loss: 5.2068610191345215\n",
            "Epoch: 19/30, Batch: 15/91, Loss: 3.9984209537506104\n",
            "Epoch: 19/30, Batch: 16/91, Loss: 4.5260796546936035\n",
            "Epoch: 19/30, Batch: 17/91, Loss: 4.410043716430664\n",
            "Epoch: 19/30, Batch: 18/91, Loss: 4.551007270812988\n",
            "Epoch: 19/30, Batch: 19/91, Loss: 4.199449062347412\n",
            "Epoch: 19/30, Batch: 20/91, Loss: 4.07164192199707\n",
            "Epoch: 19/30, Batch: 21/91, Loss: 3.460939407348633\n",
            "Epoch: 19/30, Batch: 22/91, Loss: 4.337225437164307\n",
            "Epoch: 19/30, Batch: 23/91, Loss: 4.467496871948242\n",
            "Epoch: 19/30, Batch: 24/91, Loss: 4.263889312744141\n",
            "Epoch: 19/30, Batch: 25/91, Loss: 4.256504058837891\n",
            "Epoch: 19/30, Batch: 26/91, Loss: 4.102081298828125\n",
            "Epoch: 19/30, Batch: 27/91, Loss: 4.172605514526367\n",
            "Epoch: 19/30, Batch: 28/91, Loss: 2.8570120334625244\n",
            "Epoch: 19/30, Batch: 29/91, Loss: 4.8747406005859375\n",
            "Epoch: 19/30, Batch: 30/91, Loss: 4.774691104888916\n",
            "Epoch: 19/30, Batch: 31/91, Loss: 4.386209011077881\n",
            "Epoch: 19/30, Batch: 32/91, Loss: 3.019740581512451\n",
            "Epoch: 19/30, Batch: 33/91, Loss: 3.754978895187378\n",
            "Epoch: 19/30, Batch: 34/91, Loss: 4.246959686279297\n",
            "Epoch: 19/30, Batch: 35/91, Loss: 3.951047897338867\n",
            "Epoch: 19/30, Batch: 36/91, Loss: 4.658608913421631\n",
            "Epoch: 19/30, Batch: 37/91, Loss: 3.8329155445098877\n",
            "Epoch: 19/30, Batch: 38/91, Loss: 4.23511266708374\n",
            "Epoch: 19/30, Batch: 39/91, Loss: 4.494419574737549\n",
            "Epoch: 19/30, Batch: 40/91, Loss: 3.89970064163208\n",
            "Epoch: 19/30, Batch: 41/91, Loss: 3.129004716873169\n",
            "Epoch: 19/30, Batch: 42/91, Loss: 4.8634467124938965\n",
            "Epoch: 19/30, Batch: 43/91, Loss: 4.879547595977783\n",
            "Epoch: 19/30, Batch: 44/91, Loss: 2.7949464321136475\n",
            "Epoch: 19/30, Batch: 45/91, Loss: 3.9730584621429443\n",
            "Epoch: 19/30, Batch: 46/91, Loss: 4.466676235198975\n",
            "Epoch: 19/30, Batch: 47/91, Loss: 3.8671276569366455\n",
            "Epoch: 19/30, Batch: 48/91, Loss: 3.9009432792663574\n",
            "Epoch: 19/30, Batch: 49/91, Loss: 4.7309088706970215\n",
            "Epoch: 19/30, Batch: 50/91, Loss: 2.555474042892456\n",
            "Epoch: 19/30, Batch: 51/91, Loss: 3.882699728012085\n",
            "Epoch: 19/30, Batch: 52/91, Loss: 2.8894617557525635\n",
            "Epoch: 19/30, Batch: 53/91, Loss: 2.981279134750366\n",
            "Epoch: 19/30, Batch: 54/91, Loss: 4.355157852172852\n",
            "Epoch: 19/30, Batch: 55/91, Loss: 4.758357048034668\n",
            "Epoch: 19/30, Batch: 56/91, Loss: 4.200623512268066\n",
            "Epoch: 19/30, Batch: 57/91, Loss: 4.078121662139893\n",
            "Epoch: 19/30, Batch: 58/91, Loss: 4.0581793785095215\n",
            "Epoch: 19/30, Batch: 59/91, Loss: 4.242391586303711\n",
            "Epoch: 19/30, Batch: 60/91, Loss: 3.4436872005462646\n",
            "Epoch: 19/30, Batch: 61/91, Loss: 3.5109543800354004\n",
            "Epoch: 19/30, Batch: 62/91, Loss: 3.585846424102783\n",
            "Epoch: 19/30, Batch: 63/91, Loss: 4.087351322174072\n",
            "Epoch: 19/30, Batch: 64/91, Loss: 4.2105302810668945\n",
            "Epoch: 19/30, Batch: 65/91, Loss: 4.184370517730713\n",
            "Epoch: 19/30, Batch: 66/91, Loss: 4.422896385192871\n",
            "Epoch: 19/30, Batch: 67/91, Loss: 4.284253120422363\n",
            "Epoch: 19/30, Batch: 68/91, Loss: 4.935613632202148\n",
            "Epoch: 19/30, Batch: 69/91, Loss: 3.9508142471313477\n",
            "Epoch: 19/30, Batch: 70/91, Loss: 3.913756847381592\n",
            "Epoch: 19/30, Batch: 71/91, Loss: 4.151252746582031\n",
            "Epoch: 19/30, Batch: 72/91, Loss: 3.3724238872528076\n",
            "Epoch: 19/30, Batch: 73/91, Loss: 4.676994323730469\n",
            "Epoch: 19/30, Batch: 74/91, Loss: 4.660888195037842\n",
            "Epoch: 19/30, Batch: 75/91, Loss: 2.0924153327941895\n",
            "Epoch: 19/30, Batch: 76/91, Loss: 4.296319961547852\n",
            "Epoch: 19/30, Batch: 77/91, Loss: 4.27853536605835\n",
            "Epoch: 19/30, Batch: 78/91, Loss: 4.649904727935791\n",
            "Epoch: 19/30, Batch: 79/91, Loss: 4.460503101348877\n",
            "Epoch: 19/30, Batch: 80/91, Loss: 4.835092067718506\n",
            "Epoch: 19/30, Batch: 81/91, Loss: 4.3041534423828125\n",
            "Epoch: 19/30, Batch: 82/91, Loss: 4.308931827545166\n",
            "Epoch: 19/30, Batch: 83/91, Loss: 3.7799882888793945\n",
            "Epoch: 19/30, Batch: 84/91, Loss: 4.6667399406433105\n",
            "Epoch: 19/30, Batch: 85/91, Loss: 4.531907081604004\n",
            "Epoch: 19/30, Batch: 86/91, Loss: 4.669585227966309\n",
            "Epoch: 19/30, Batch: 87/91, Loss: 4.522303104400635\n",
            "Epoch: 19/30, Batch: 88/91, Loss: 4.341238975524902\n",
            "Epoch: 19/30, Batch: 89/91, Loss: 4.037297248840332\n",
            "Epoch: 19/30, Batch: 90/91, Loss: 4.315255641937256\n",
            "Epoch: 20/30, Batch: 0/91, Loss: 4.397109508514404\n",
            "Epoch: 20/30, Batch: 1/91, Loss: 4.479773998260498\n",
            "Epoch: 20/30, Batch: 2/91, Loss: 4.812307357788086\n",
            "Epoch: 20/30, Batch: 3/91, Loss: 4.332066059112549\n",
            "Epoch: 20/30, Batch: 4/91, Loss: 4.267870903015137\n",
            "Epoch: 20/30, Batch: 5/91, Loss: 3.564929962158203\n",
            "Epoch: 20/30, Batch: 6/91, Loss: 3.924945116043091\n",
            "Epoch: 20/30, Batch: 7/91, Loss: 3.8660528659820557\n",
            "Epoch: 20/30, Batch: 8/91, Loss: 4.7864556312561035\n",
            "Epoch: 20/30, Batch: 9/91, Loss: 3.670473337173462\n",
            "Epoch: 20/30, Batch: 10/91, Loss: 4.521060466766357\n",
            "Epoch: 20/30, Batch: 11/91, Loss: 4.751883506774902\n",
            "Epoch: 20/30, Batch: 12/91, Loss: 3.256352186203003\n",
            "Epoch: 20/30, Batch: 13/91, Loss: 3.7530219554901123\n",
            "Epoch: 20/30, Batch: 14/91, Loss: 4.310032844543457\n",
            "Epoch: 20/30, Batch: 15/91, Loss: 4.9902825355529785\n",
            "Epoch: 20/30, Batch: 16/91, Loss: 4.395025730133057\n",
            "Epoch: 20/30, Batch: 17/91, Loss: 4.307845115661621\n",
            "Epoch: 20/30, Batch: 18/91, Loss: 4.828108310699463\n",
            "Epoch: 20/30, Batch: 19/91, Loss: 4.322091102600098\n",
            "Epoch: 20/30, Batch: 20/91, Loss: 4.736603260040283\n",
            "Epoch: 20/30, Batch: 21/91, Loss: 2.6569128036499023\n",
            "Epoch: 20/30, Batch: 22/91, Loss: 4.5281524658203125\n",
            "Epoch: 20/30, Batch: 23/91, Loss: 4.376880645751953\n",
            "Epoch: 20/30, Batch: 24/91, Loss: 4.458371639251709\n",
            "Epoch: 20/30, Batch: 25/91, Loss: 4.173386573791504\n",
            "Epoch: 20/30, Batch: 26/91, Loss: 4.848936080932617\n",
            "Epoch: 20/30, Batch: 27/91, Loss: 4.328188896179199\n",
            "Epoch: 20/30, Batch: 28/91, Loss: 3.6770377159118652\n",
            "Epoch: 20/30, Batch: 29/91, Loss: 5.037732124328613\n",
            "Epoch: 20/30, Batch: 30/91, Loss: 4.025140285491943\n",
            "Epoch: 20/30, Batch: 31/91, Loss: 4.507514953613281\n",
            "Epoch: 20/30, Batch: 32/91, Loss: 4.646138668060303\n",
            "Epoch: 20/30, Batch: 33/91, Loss: 4.554054260253906\n",
            "Epoch: 20/30, Batch: 34/91, Loss: 4.607779026031494\n",
            "Epoch: 20/30, Batch: 35/91, Loss: 3.6283106803894043\n",
            "Epoch: 20/30, Batch: 36/91, Loss: 4.215031147003174\n",
            "Epoch: 20/30, Batch: 37/91, Loss: 3.8811228275299072\n",
            "Epoch: 20/30, Batch: 38/91, Loss: 3.8349626064300537\n",
            "Epoch: 20/30, Batch: 39/91, Loss: 3.4224648475646973\n",
            "Epoch: 20/30, Batch: 40/91, Loss: 3.8638899326324463\n",
            "Epoch: 20/30, Batch: 41/91, Loss: 3.513277292251587\n",
            "Epoch: 20/30, Batch: 42/91, Loss: 3.6786129474639893\n",
            "Epoch: 20/30, Batch: 43/91, Loss: 4.5798659324646\n",
            "Epoch: 20/30, Batch: 44/91, Loss: 4.212337493896484\n",
            "Epoch: 20/30, Batch: 45/91, Loss: 3.755890130996704\n",
            "Epoch: 20/30, Batch: 46/91, Loss: 4.416467189788818\n",
            "Epoch: 20/30, Batch: 47/91, Loss: 4.0292558670043945\n",
            "Epoch: 20/30, Batch: 48/91, Loss: 3.6991126537323\n",
            "Epoch: 20/30, Batch: 49/91, Loss: 4.222594261169434\n",
            "Epoch: 20/30, Batch: 50/91, Loss: 4.563265800476074\n",
            "Epoch: 20/30, Batch: 51/91, Loss: 3.909269332885742\n",
            "Epoch: 20/30, Batch: 52/91, Loss: 3.7277140617370605\n",
            "Epoch: 20/30, Batch: 53/91, Loss: 4.50812292098999\n",
            "Epoch: 20/30, Batch: 54/91, Loss: 4.039444446563721\n",
            "Epoch: 20/30, Batch: 55/91, Loss: 4.945882320404053\n",
            "Epoch: 20/30, Batch: 56/91, Loss: 4.293036460876465\n",
            "Epoch: 20/30, Batch: 57/91, Loss: 4.345902442932129\n",
            "Epoch: 20/30, Batch: 58/91, Loss: 4.621800422668457\n",
            "Epoch: 20/30, Batch: 59/91, Loss: 3.3616654872894287\n",
            "Epoch: 20/30, Batch: 60/91, Loss: 4.429927825927734\n",
            "Epoch: 20/30, Batch: 61/91, Loss: 3.9608006477355957\n",
            "Epoch: 20/30, Batch: 62/91, Loss: 3.6144959926605225\n",
            "Epoch: 20/30, Batch: 63/91, Loss: 3.316941022872925\n",
            "Epoch: 20/30, Batch: 64/91, Loss: 4.582206726074219\n",
            "Epoch: 20/30, Batch: 65/91, Loss: 4.198537826538086\n",
            "Epoch: 20/30, Batch: 66/91, Loss: 3.847987413406372\n",
            "Epoch: 20/30, Batch: 67/91, Loss: 4.149027347564697\n",
            "Epoch: 20/30, Batch: 68/91, Loss: 4.075831890106201\n",
            "Epoch: 20/30, Batch: 69/91, Loss: 4.761885166168213\n",
            "Epoch: 20/30, Batch: 70/91, Loss: 4.470051288604736\n",
            "Epoch: 20/30, Batch: 71/91, Loss: 3.9492268562316895\n",
            "Epoch: 20/30, Batch: 72/91, Loss: 4.717094898223877\n",
            "Epoch: 20/30, Batch: 73/91, Loss: 4.548678874969482\n",
            "Epoch: 20/30, Batch: 74/91, Loss: 4.744559288024902\n",
            "Epoch: 20/30, Batch: 75/91, Loss: 4.145618438720703\n",
            "Epoch: 20/30, Batch: 76/91, Loss: 2.837651252746582\n",
            "Epoch: 20/30, Batch: 77/91, Loss: 2.917572259902954\n",
            "Epoch: 20/30, Batch: 78/91, Loss: 3.096909999847412\n",
            "Epoch: 20/30, Batch: 79/91, Loss: 4.734629154205322\n",
            "Epoch: 20/30, Batch: 80/91, Loss: 4.583633899688721\n",
            "Epoch: 20/30, Batch: 81/91, Loss: 3.792046546936035\n",
            "Epoch: 20/30, Batch: 82/91, Loss: 3.935276746749878\n",
            "Epoch: 20/30, Batch: 83/91, Loss: 3.994748830795288\n",
            "Epoch: 20/30, Batch: 84/91, Loss: 4.241470813751221\n",
            "Epoch: 20/30, Batch: 85/91, Loss: 4.2091264724731445\n",
            "Epoch: 20/30, Batch: 86/91, Loss: 4.213184833526611\n",
            "Epoch: 20/30, Batch: 88/91, Loss: 3.9708406925201416\n",
            "Epoch: 20/30, Batch: 89/91, Loss: 3.057941436767578\n",
            "Epoch: 20/30, Batch: 90/91, Loss: 4.4155683517456055\n",
            "Epoch: 21/30, Batch: 0/91, Loss: 4.607885837554932\n",
            "Epoch: 21/30, Batch: 1/91, Loss: 4.493509769439697\n",
            "Epoch: 21/30, Batch: 2/91, Loss: 4.156066417694092\n",
            "Epoch: 21/30, Batch: 3/91, Loss: 3.3940796852111816\n",
            "Epoch: 21/30, Batch: 4/91, Loss: 2.910888910293579\n",
            "Epoch: 21/30, Batch: 5/91, Loss: 4.878809928894043\n",
            "Epoch: 21/30, Batch: 6/91, Loss: 3.9262123107910156\n",
            "Epoch: 21/30, Batch: 7/91, Loss: 3.681413173675537\n",
            "Epoch: 21/30, Batch: 8/91, Loss: 3.730391263961792\n",
            "Epoch: 21/30, Batch: 9/91, Loss: 4.807145118713379\n",
            "Epoch: 21/30, Batch: 10/91, Loss: 3.0808756351470947\n",
            "Epoch: 21/30, Batch: 11/91, Loss: 4.503781795501709\n",
            "Epoch: 21/30, Batch: 12/91, Loss: 4.8075127601623535\n",
            "Epoch: 21/30, Batch: 13/91, Loss: 4.123352527618408\n",
            "Epoch: 21/30, Batch: 14/91, Loss: 3.7011497020721436\n",
            "Epoch: 21/30, Batch: 15/91, Loss: 4.38880729675293\n",
            "Epoch: 21/30, Batch: 16/91, Loss: 4.4871439933776855\n",
            "Epoch: 21/30, Batch: 17/91, Loss: 3.4144530296325684\n",
            "Epoch: 21/30, Batch: 18/91, Loss: 4.307760715484619\n",
            "Epoch: 21/30, Batch: 19/91, Loss: 4.273462772369385\n",
            "Epoch: 21/30, Batch: 20/91, Loss: 4.29463529586792\n",
            "Epoch: 21/30, Batch: 21/91, Loss: 4.206435680389404\n",
            "Epoch: 21/30, Batch: 22/91, Loss: 3.997828722000122\n",
            "Epoch: 21/30, Batch: 23/91, Loss: 3.9098715782165527\n",
            "Epoch: 21/30, Batch: 24/91, Loss: 4.5576653480529785\n",
            "Epoch: 21/30, Batch: 25/91, Loss: 4.647303104400635\n",
            "Epoch: 21/30, Batch: 26/91, Loss: 3.229595422744751\n",
            "Epoch: 21/30, Batch: 27/91, Loss: 4.946200847625732\n",
            "Epoch: 21/30, Batch: 28/91, Loss: 3.748420238494873\n",
            "Epoch: 21/30, Batch: 29/91, Loss: 5.229702472686768\n",
            "Epoch: 21/30, Batch: 30/91, Loss: 4.769565105438232\n",
            "Epoch: 21/30, Batch: 31/91, Loss: 4.487730503082275\n",
            "Epoch: 21/30, Batch: 32/91, Loss: 4.245854377746582\n",
            "Epoch: 21/30, Batch: 33/91, Loss: 3.647493362426758\n",
            "Epoch: 21/30, Batch: 34/91, Loss: 4.016866207122803\n",
            "Epoch: 21/30, Batch: 35/91, Loss: 3.872199296951294\n",
            "Epoch: 21/30, Batch: 36/91, Loss: 4.364967346191406\n",
            "Epoch: 21/30, Batch: 37/91, Loss: 2.9147047996520996\n",
            "Epoch: 21/30, Batch: 38/91, Loss: 4.196131229400635\n",
            "Epoch: 21/30, Batch: 39/91, Loss: 4.2995100021362305\n",
            "Epoch: 21/30, Batch: 40/91, Loss: 4.546057224273682\n",
            "Epoch: 21/30, Batch: 41/91, Loss: 4.300774097442627\n",
            "Epoch: 21/30, Batch: 42/91, Loss: 3.3183844089508057\n",
            "Epoch: 21/30, Batch: 43/91, Loss: 4.626378536224365\n",
            "Epoch: 21/30, Batch: 44/91, Loss: 4.241245746612549\n",
            "Epoch: 21/30, Batch: 45/91, Loss: 3.809785842895508\n",
            "Epoch: 21/30, Batch: 46/91, Loss: 4.279820919036865\n",
            "Epoch: 21/30, Batch: 47/91, Loss: 4.443833351135254\n",
            "Epoch: 21/30, Batch: 48/91, Loss: 4.042962074279785\n",
            "Epoch: 21/30, Batch: 49/91, Loss: 4.338118553161621\n",
            "Epoch: 21/30, Batch: 50/91, Loss: 4.635600566864014\n",
            "Epoch: 21/30, Batch: 51/91, Loss: 4.248500347137451\n",
            "Epoch: 21/30, Batch: 52/91, Loss: 4.972926139831543\n",
            "Epoch: 21/30, Batch: 53/91, Loss: 3.832242727279663\n",
            "Epoch: 21/30, Batch: 54/91, Loss: 1.9838454723358154\n",
            "Epoch: 21/30, Batch: 55/91, Loss: 4.297306060791016\n",
            "Epoch: 21/30, Batch: 56/91, Loss: 4.398955821990967\n",
            "Epoch: 21/30, Batch: 57/91, Loss: 3.4495279788970947\n",
            "Epoch: 21/30, Batch: 58/91, Loss: 4.754028797149658\n",
            "Epoch: 21/30, Batch: 59/91, Loss: 3.3194522857666016\n",
            "Epoch: 21/30, Batch: 60/91, Loss: 3.63944673538208\n",
            "Epoch: 21/30, Batch: 61/91, Loss: 4.856485843658447\n",
            "Epoch: 21/30, Batch: 62/91, Loss: 3.942478895187378\n",
            "Epoch: 21/30, Batch: 63/91, Loss: 4.414669990539551\n",
            "Epoch: 21/30, Batch: 64/91, Loss: 3.564842939376831\n",
            "Epoch: 21/30, Batch: 65/91, Loss: 2.9745993614196777\n",
            "Epoch: 21/30, Batch: 66/91, Loss: 4.515443801879883\n",
            "Epoch: 21/30, Batch: 67/91, Loss: 4.372223854064941\n",
            "Epoch: 21/30, Batch: 68/91, Loss: 4.779314041137695\n",
            "Epoch: 21/30, Batch: 69/91, Loss: 3.2168028354644775\n",
            "Epoch: 21/30, Batch: 70/91, Loss: 4.9800238609313965\n",
            "Epoch: 21/30, Batch: 71/91, Loss: 4.627137184143066\n",
            "Epoch: 21/30, Batch: 72/91, Loss: 4.543572425842285\n",
            "Epoch: 21/30, Batch: 73/91, Loss: 3.500457525253296\n",
            "Epoch: 21/30, Batch: 74/91, Loss: 4.581320285797119\n",
            "Epoch: 21/30, Batch: 75/91, Loss: 3.4139914512634277\n",
            "Epoch: 21/30, Batch: 76/91, Loss: 3.8699514865875244\n",
            "Epoch: 21/30, Batch: 77/91, Loss: 2.5699503421783447\n",
            "Epoch: 21/30, Batch: 78/91, Loss: 3.4366137981414795\n",
            "Epoch: 21/30, Batch: 79/91, Loss: 4.530396461486816\n",
            "Epoch: 21/30, Batch: 80/91, Loss: 4.693931579589844\n",
            "Epoch: 21/30, Batch: 81/91, Loss: 4.571525573730469\n",
            "Epoch: 21/30, Batch: 82/91, Loss: 3.9677610397338867\n",
            "Epoch: 21/30, Batch: 83/91, Loss: 4.995386123657227\n",
            "Epoch: 21/30, Batch: 84/91, Loss: 4.069572448730469\n",
            "Epoch: 21/30, Batch: 85/91, Loss: 4.397845268249512\n",
            "Epoch: 21/30, Batch: 86/91, Loss: 3.4359030723571777\n",
            "Epoch: 21/30, Batch: 87/91, Loss: 4.756531715393066\n",
            "Epoch: 21/30, Batch: 88/91, Loss: 4.16485071182251\n",
            "Epoch: 21/30, Batch: 89/91, Loss: 4.207762718200684\n",
            "Epoch: 21/30, Batch: 90/91, Loss: 4.296729564666748\n",
            "Epoch: 22/30, Batch: 0/91, Loss: 3.620218515396118\n",
            "Epoch: 22/30, Batch: 1/91, Loss: 4.205844879150391\n",
            "Epoch: 22/30, Batch: 2/91, Loss: 4.392947196960449\n",
            "Epoch: 22/30, Batch: 3/91, Loss: 4.066977024078369\n",
            "Epoch: 22/30, Batch: 4/91, Loss: 4.453558444976807\n",
            "Epoch: 22/30, Batch: 5/91, Loss: 4.053686618804932\n",
            "Epoch: 22/30, Batch: 6/91, Loss: 4.321221828460693\n",
            "Epoch: 22/30, Batch: 7/91, Loss: 3.541754722595215\n",
            "Epoch: 22/30, Batch: 8/91, Loss: 3.9699394702911377\n",
            "Epoch: 22/30, Batch: 9/91, Loss: 4.679521083831787\n",
            "Epoch: 22/30, Batch: 10/91, Loss: 3.843672513961792\n",
            "Epoch: 22/30, Batch: 11/91, Loss: 4.004629135131836\n",
            "Epoch: 22/30, Batch: 12/91, Loss: 4.198775768280029\n",
            "Epoch: 22/30, Batch: 13/91, Loss: 3.586458921432495\n",
            "Epoch: 22/30, Batch: 14/91, Loss: 4.088357925415039\n",
            "Epoch: 22/30, Batch: 15/91, Loss: 4.775728225708008\n",
            "Epoch: 22/30, Batch: 16/91, Loss: 4.197144985198975\n",
            "Epoch: 22/30, Batch: 17/91, Loss: 3.7932207584381104\n",
            "Epoch: 22/30, Batch: 18/91, Loss: 3.966848850250244\n",
            "Epoch: 22/30, Batch: 19/91, Loss: 4.401707172393799\n",
            "Epoch: 22/30, Batch: 20/91, Loss: 4.620260715484619\n",
            "Epoch: 22/30, Batch: 21/91, Loss: 3.9788336753845215\n",
            "Epoch: 22/30, Batch: 22/91, Loss: 4.453851222991943\n",
            "Epoch: 22/30, Batch: 23/91, Loss: 3.9240171909332275\n",
            "Epoch: 22/30, Batch: 24/91, Loss: 4.094601154327393\n",
            "Epoch: 22/30, Batch: 25/91, Loss: 3.8829233646392822\n",
            "Epoch: 22/30, Batch: 26/91, Loss: 4.285095691680908\n",
            "Epoch: 22/30, Batch: 27/91, Loss: 3.6464550495147705\n",
            "Epoch: 22/30, Batch: 28/91, Loss: 4.2031779289245605\n",
            "Epoch: 22/30, Batch: 29/91, Loss: 2.763831615447998\n",
            "Epoch: 22/30, Batch: 30/91, Loss: 3.3881099224090576\n",
            "Epoch: 22/30, Batch: 31/91, Loss: 4.009157657623291\n",
            "Epoch: 22/30, Batch: 32/91, Loss: 4.497538089752197\n",
            "Epoch: 22/30, Batch: 33/91, Loss: 3.8568954467773438\n",
            "Epoch: 22/30, Batch: 34/91, Loss: 4.18629789352417\n",
            "Epoch: 22/30, Batch: 35/91, Loss: 4.8478474617004395\n",
            "Epoch: 22/30, Batch: 36/91, Loss: 3.513556480407715\n",
            "Epoch: 22/30, Batch: 37/91, Loss: 3.9845082759857178\n",
            "Epoch: 22/30, Batch: 38/91, Loss: 2.810720682144165\n",
            "Epoch: 22/30, Batch: 39/91, Loss: 4.912423133850098\n",
            "Epoch: 22/30, Batch: 40/91, Loss: 3.741276741027832\n",
            "Epoch: 22/30, Batch: 41/91, Loss: 3.6503114700317383\n",
            "Epoch: 22/30, Batch: 42/91, Loss: 4.255046844482422\n",
            "Epoch: 22/30, Batch: 43/91, Loss: 4.091226100921631\n",
            "Epoch: 22/30, Batch: 44/91, Loss: 4.742476940155029\n",
            "Epoch: 22/30, Batch: 45/91, Loss: 3.751373291015625\n",
            "Epoch: 22/30, Batch: 46/91, Loss: 4.519248008728027\n",
            "Epoch: 22/30, Batch: 47/91, Loss: 3.9735634326934814\n",
            "Epoch: 22/30, Batch: 48/91, Loss: 3.550670623779297\n",
            "Epoch: 22/30, Batch: 49/91, Loss: 4.047545433044434\n",
            "Epoch: 22/30, Batch: 50/91, Loss: 3.8366386890411377\n",
            "Epoch: 22/30, Batch: 51/91, Loss: 4.838892459869385\n",
            "Epoch: 22/30, Batch: 52/91, Loss: 4.201448440551758\n",
            "Epoch: 22/30, Batch: 53/91, Loss: 4.59246301651001\n",
            "Epoch: 22/30, Batch: 54/91, Loss: 4.402399063110352\n",
            "Epoch: 22/30, Batch: 55/91, Loss: 2.6727190017700195\n",
            "Epoch: 22/30, Batch: 56/91, Loss: 4.990795135498047\n",
            "Epoch: 22/30, Batch: 57/91, Loss: 4.0102858543396\n",
            "Epoch: 22/30, Batch: 58/91, Loss: 4.282726287841797\n",
            "Epoch: 22/30, Batch: 59/91, Loss: 4.42981481552124\n",
            "Epoch: 22/30, Batch: 60/91, Loss: 3.790381669998169\n",
            "Epoch: 22/30, Batch: 61/91, Loss: 3.959746837615967\n",
            "Epoch: 22/30, Batch: 62/91, Loss: 4.852579116821289\n",
            "Epoch: 22/30, Batch: 63/91, Loss: 3.6739962100982666\n",
            "Epoch: 22/30, Batch: 64/91, Loss: 3.047353744506836\n",
            "Epoch: 22/30, Batch: 65/91, Loss: 4.201760292053223\n",
            "Epoch: 22/30, Batch: 66/91, Loss: 4.775566101074219\n",
            "Epoch: 22/30, Batch: 67/91, Loss: 4.402124881744385\n",
            "Epoch: 22/30, Batch: 68/91, Loss: 4.669925689697266\n",
            "Epoch: 22/30, Batch: 69/91, Loss: 4.492391586303711\n",
            "Epoch: 22/30, Batch: 70/91, Loss: 3.560398578643799\n",
            "Epoch: 22/30, Batch: 71/91, Loss: 3.565518617630005\n",
            "Epoch: 22/30, Batch: 72/91, Loss: 3.6015567779541016\n",
            "Epoch: 22/30, Batch: 73/91, Loss: 4.46181583404541\n",
            "Epoch: 22/30, Batch: 74/91, Loss: 4.481681823730469\n",
            "Epoch: 22/30, Batch: 75/91, Loss: 4.109658241271973\n",
            "Epoch: 22/30, Batch: 76/91, Loss: 4.917560577392578\n",
            "Epoch: 22/30, Batch: 77/91, Loss: 3.713200330734253\n",
            "Epoch: 22/30, Batch: 78/91, Loss: 4.1100077629089355\n",
            "Epoch: 22/30, Batch: 79/91, Loss: 4.585351467132568\n",
            "Epoch: 22/30, Batch: 80/91, Loss: 4.738062858581543\n",
            "Epoch: 22/30, Batch: 81/91, Loss: 3.867767333984375\n",
            "Epoch: 22/30, Batch: 82/91, Loss: 4.102713584899902\n",
            "Epoch: 22/30, Batch: 83/91, Loss: 4.536839485168457\n",
            "Epoch: 22/30, Batch: 84/91, Loss: 4.613726615905762\n",
            "Epoch: 22/30, Batch: 85/91, Loss: 3.6686103343963623\n",
            "Epoch: 22/30, Batch: 86/91, Loss: 4.513782978057861\n",
            "Epoch: 22/30, Batch: 87/91, Loss: 4.911651134490967\n",
            "Epoch: 22/30, Batch: 88/91, Loss: 4.668484687805176\n",
            "Epoch: 22/30, Batch: 89/91, Loss: 4.611465930938721\n",
            "Epoch: 22/30, Batch: 90/91, Loss: 4.310102939605713\n",
            "Epoch: 23/30, Batch: 0/91, Loss: 5.049777984619141\n",
            "Epoch: 23/30, Batch: 1/91, Loss: 4.542036056518555\n",
            "Epoch: 23/30, Batch: 2/91, Loss: 4.325281143188477\n",
            "Epoch: 23/30, Batch: 3/91, Loss: 4.865288257598877\n",
            "Epoch: 23/30, Batch: 4/91, Loss: 3.9349355697631836\n",
            "Epoch: 23/30, Batch: 5/91, Loss: 4.476865768432617\n",
            "Epoch: 23/30, Batch: 6/91, Loss: 3.36553692817688\n",
            "Epoch: 23/30, Batch: 7/91, Loss: 4.703608512878418\n",
            "Epoch: 23/30, Batch: 8/91, Loss: 4.347256183624268\n",
            "Epoch: 23/30, Batch: 9/91, Loss: 4.003538131713867\n",
            "Epoch: 23/30, Batch: 10/91, Loss: 4.4809675216674805\n",
            "Epoch: 23/30, Batch: 11/91, Loss: 4.068875789642334\n",
            "Epoch: 23/30, Batch: 12/91, Loss: 2.8296520709991455\n",
            "Epoch: 23/30, Batch: 13/91, Loss: 4.4517951011657715\n",
            "Epoch: 23/30, Batch: 14/91, Loss: 4.214935302734375\n",
            "Epoch: 23/30, Batch: 15/91, Loss: 4.883756637573242\n",
            "Epoch: 23/30, Batch: 16/91, Loss: 4.290942192077637\n",
            "Epoch: 23/30, Batch: 17/91, Loss: 4.374093055725098\n",
            "Epoch: 23/30, Batch: 18/91, Loss: 3.9026758670806885\n",
            "Epoch: 23/30, Batch: 19/91, Loss: 3.9323718547821045\n",
            "Epoch: 23/30, Batch: 20/91, Loss: 4.126011371612549\n",
            "Epoch: 23/30, Batch: 21/91, Loss: 4.250044345855713\n",
            "Epoch: 23/30, Batch: 22/91, Loss: 4.330979347229004\n",
            "Epoch: 23/30, Batch: 23/91, Loss: 3.4858875274658203\n",
            "Epoch: 23/30, Batch: 24/91, Loss: 2.7824137210845947\n",
            "Epoch: 23/30, Batch: 25/91, Loss: 4.535624980926514\n",
            "Epoch: 23/30, Batch: 26/91, Loss: 4.568634986877441\n",
            "Epoch: 23/30, Batch: 27/91, Loss: 3.6375555992126465\n",
            "Epoch: 23/30, Batch: 28/91, Loss: 4.181272029876709\n",
            "Epoch: 23/30, Batch: 29/91, Loss: 4.676447868347168\n",
            "Epoch: 23/30, Batch: 30/91, Loss: 3.676933526992798\n",
            "Epoch: 23/30, Batch: 31/91, Loss: 3.913346290588379\n",
            "Epoch: 23/30, Batch: 32/91, Loss: 3.6206068992614746\n",
            "Epoch: 23/30, Batch: 33/91, Loss: 4.052128791809082\n",
            "Epoch: 23/30, Batch: 34/91, Loss: 4.516563892364502\n",
            "Epoch: 23/30, Batch: 35/91, Loss: 4.404422283172607\n",
            "Epoch: 23/30, Batch: 36/91, Loss: 5.177011489868164\n",
            "Epoch: 23/30, Batch: 37/91, Loss: 3.797091484069824\n",
            "Epoch: 23/30, Batch: 38/91, Loss: 4.04603910446167\n",
            "Epoch: 23/30, Batch: 39/91, Loss: 3.9193084239959717\n",
            "Epoch: 23/30, Batch: 40/91, Loss: 4.215719223022461\n",
            "Epoch: 23/30, Batch: 41/91, Loss: 4.041933059692383\n",
            "Epoch: 23/30, Batch: 42/91, Loss: 3.320671558380127\n",
            "Epoch: 23/30, Batch: 43/91, Loss: 4.8137712478637695\n",
            "Epoch: 23/30, Batch: 44/91, Loss: 3.9431352615356445\n",
            "Epoch: 23/30, Batch: 45/91, Loss: 3.540822744369507\n",
            "Epoch: 23/30, Batch: 46/91, Loss: 5.021496772766113\n",
            "Epoch: 23/30, Batch: 47/91, Loss: 3.418405055999756\n",
            "Epoch: 23/30, Batch: 48/91, Loss: 4.210700988769531\n",
            "Epoch: 23/30, Batch: 49/91, Loss: 3.7716257572174072\n",
            "Epoch: 23/30, Batch: 50/91, Loss: 3.768488645553589\n",
            "Epoch: 23/30, Batch: 51/91, Loss: 4.917336463928223\n",
            "Epoch: 23/30, Batch: 52/91, Loss: 3.804588556289673\n",
            "Epoch: 23/30, Batch: 53/91, Loss: 4.622208118438721\n",
            "Epoch: 23/30, Batch: 54/91, Loss: 4.2833027839660645\n",
            "Epoch: 23/30, Batch: 55/91, Loss: 3.696979522705078\n",
            "Epoch: 23/30, Batch: 56/91, Loss: 4.120146751403809\n",
            "Epoch: 23/30, Batch: 57/91, Loss: 4.434694290161133\n",
            "Epoch: 23/30, Batch: 58/91, Loss: 4.085849761962891\n",
            "Epoch: 23/30, Batch: 59/91, Loss: 4.334601879119873\n",
            "Epoch: 23/30, Batch: 60/91, Loss: 4.733589172363281\n",
            "Epoch: 23/30, Batch: 61/91, Loss: 4.019906997680664\n",
            "Epoch: 23/30, Batch: 62/91, Loss: 4.357260704040527\n",
            "Epoch: 23/30, Batch: 63/91, Loss: 3.7868731021881104\n",
            "Epoch: 23/30, Batch: 64/91, Loss: 4.22452449798584\n",
            "Epoch: 23/30, Batch: 65/91, Loss: 4.250374794006348\n",
            "Epoch: 23/30, Batch: 66/91, Loss: 4.085059642791748\n",
            "Epoch: 23/30, Batch: 67/91, Loss: 4.262126922607422\n",
            "Epoch: 23/30, Batch: 68/91, Loss: 4.710710525512695\n",
            "Epoch: 23/30, Batch: 69/91, Loss: 3.334052801132202\n",
            "Epoch: 23/30, Batch: 70/91, Loss: 3.7283637523651123\n",
            "Epoch: 23/30, Batch: 71/91, Loss: 2.982404947280884\n",
            "Epoch: 23/30, Batch: 72/91, Loss: 4.014001846313477\n",
            "Epoch: 23/30, Batch: 73/91, Loss: 3.9658398628234863\n",
            "Epoch: 23/30, Batch: 74/91, Loss: 3.715951442718506\n",
            "Epoch: 23/30, Batch: 75/91, Loss: 4.888416290283203\n",
            "Epoch: 23/30, Batch: 76/91, Loss: 4.20263147354126\n",
            "Epoch: 23/30, Batch: 77/91, Loss: 4.227504730224609\n",
            "Epoch: 23/30, Batch: 78/91, Loss: 4.499340534210205\n",
            "Epoch: 23/30, Batch: 79/91, Loss: 3.8938510417938232\n",
            "Epoch: 23/30, Batch: 80/91, Loss: 3.7508087158203125\n",
            "Epoch: 23/30, Batch: 81/91, Loss: 2.7885942459106445\n",
            "Epoch: 23/30, Batch: 82/91, Loss: 4.400264739990234\n",
            "Epoch: 23/30, Batch: 83/91, Loss: 4.012972831726074\n",
            "Epoch: 23/30, Batch: 84/91, Loss: 4.030616760253906\n",
            "Epoch: 23/30, Batch: 85/91, Loss: 4.087685585021973\n",
            "Epoch: 23/30, Batch: 86/91, Loss: 4.280747413635254\n",
            "Epoch: 23/30, Batch: 87/91, Loss: 3.770867347717285\n",
            "Epoch: 23/30, Batch: 88/91, Loss: 2.023817539215088\n",
            "Epoch: 23/30, Batch: 89/91, Loss: 3.299867630004883\n",
            "Epoch: 23/30, Batch: 90/91, Loss: 4.972503662109375\n",
            "Epoch: 24/30, Batch: 0/91, Loss: 3.473297595977783\n",
            "Epoch: 24/30, Batch: 1/91, Loss: 4.908400535583496\n",
            "Epoch: 24/30, Batch: 2/91, Loss: 4.627363681793213\n",
            "Epoch: 24/30, Batch: 3/91, Loss: 3.929946184158325\n",
            "Epoch: 24/30, Batch: 4/91, Loss: 4.333287239074707\n",
            "Epoch: 24/30, Batch: 5/91, Loss: 4.394720077514648\n",
            "Epoch: 24/30, Batch: 6/91, Loss: 4.32243013381958\n",
            "Epoch: 24/30, Batch: 7/91, Loss: 4.296505451202393\n",
            "Epoch: 24/30, Batch: 8/91, Loss: 4.390780448913574\n",
            "Epoch: 24/30, Batch: 9/91, Loss: 3.9668707847595215\n",
            "Epoch: 24/30, Batch: 10/91, Loss: 4.101470470428467\n",
            "Epoch: 24/30, Batch: 11/91, Loss: 4.423506736755371\n",
            "Epoch: 24/30, Batch: 12/91, Loss: 4.1080732345581055\n",
            "Epoch: 24/30, Batch: 13/91, Loss: 3.6071791648864746\n",
            "Epoch: 24/30, Batch: 14/91, Loss: 3.560235023498535\n",
            "Epoch: 24/30, Batch: 15/91, Loss: 3.661668539047241\n",
            "Epoch: 24/30, Batch: 16/91, Loss: 4.7436747550964355\n",
            "Epoch: 24/30, Batch: 17/91, Loss: 4.599829196929932\n",
            "Epoch: 24/30, Batch: 18/91, Loss: 4.166258335113525\n",
            "Epoch: 24/30, Batch: 19/91, Loss: 4.258786678314209\n",
            "Epoch: 24/30, Batch: 20/91, Loss: 4.327290058135986\n",
            "Epoch: 24/30, Batch: 21/91, Loss: 4.645001411437988\n",
            "Epoch: 24/30, Batch: 22/91, Loss: 4.338974952697754\n",
            "Epoch: 24/30, Batch: 23/91, Loss: 4.784093856811523\n",
            "Epoch: 24/30, Batch: 24/91, Loss: 4.053491115570068\n",
            "Epoch: 24/30, Batch: 25/91, Loss: 3.13466477394104\n",
            "Epoch: 24/30, Batch: 26/91, Loss: 2.8948774337768555\n",
            "Epoch: 24/30, Batch: 27/91, Loss: 2.053206443786621\n",
            "Epoch: 24/30, Batch: 28/91, Loss: 4.603329181671143\n",
            "Epoch: 24/30, Batch: 29/91, Loss: 4.014866828918457\n",
            "Epoch: 24/30, Batch: 30/91, Loss: 3.332402467727661\n",
            "Epoch: 24/30, Batch: 31/91, Loss: 3.353022575378418\n",
            "Epoch: 24/30, Batch: 32/91, Loss: 4.277657508850098\n",
            "Epoch: 24/30, Batch: 33/91, Loss: 4.549496650695801\n",
            "Epoch: 24/30, Batch: 34/91, Loss: 4.676275730133057\n",
            "Epoch: 24/30, Batch: 35/91, Loss: 3.7477712631225586\n",
            "Epoch: 24/30, Batch: 36/91, Loss: 4.729822158813477\n",
            "Epoch: 24/30, Batch: 37/91, Loss: 4.035360813140869\n",
            "Epoch: 24/30, Batch: 38/91, Loss: 4.2656354904174805\n",
            "Epoch: 24/30, Batch: 39/91, Loss: 4.933650016784668\n",
            "Epoch: 24/30, Batch: 40/91, Loss: 3.8503224849700928\n",
            "Epoch: 24/30, Batch: 41/91, Loss: 5.2798357009887695\n",
            "Epoch: 24/30, Batch: 42/91, Loss: 3.6011884212493896\n",
            "Epoch: 24/30, Batch: 43/91, Loss: 4.59434175491333\n",
            "Epoch: 24/30, Batch: 44/91, Loss: 3.9646332263946533\n",
            "Epoch: 24/30, Batch: 45/91, Loss: 4.780879020690918\n",
            "Epoch: 24/30, Batch: 46/91, Loss: 3.29069185256958\n",
            "Epoch: 24/30, Batch: 47/91, Loss: 4.554144859313965\n",
            "Epoch: 24/30, Batch: 48/91, Loss: 4.330392837524414\n",
            "Epoch: 24/30, Batch: 49/91, Loss: 4.322793483734131\n",
            "Epoch: 24/30, Batch: 50/91, Loss: 4.8109259605407715\n",
            "Epoch: 24/30, Batch: 51/91, Loss: 4.36672830581665\n",
            "Epoch: 24/30, Batch: 52/91, Loss: 3.9288549423217773\n",
            "Epoch: 24/30, Batch: 53/91, Loss: 4.3249735832214355\n",
            "Epoch: 24/30, Batch: 54/91, Loss: 3.920875310897827\n",
            "Epoch: 24/30, Batch: 55/91, Loss: 3.675325393676758\n",
            "Epoch: 24/30, Batch: 56/91, Loss: 4.44063663482666\n",
            "Epoch: 24/30, Batch: 57/91, Loss: 3.8504416942596436\n",
            "Epoch: 24/30, Batch: 58/91, Loss: 4.711062431335449\n",
            "Epoch: 24/30, Batch: 59/91, Loss: 3.9044439792633057\n",
            "Epoch: 24/30, Batch: 60/91, Loss: 4.203531265258789\n",
            "Epoch: 24/30, Batch: 61/91, Loss: 3.9204230308532715\n",
            "Epoch: 24/30, Batch: 62/91, Loss: 3.5635387897491455\n",
            "Epoch: 24/30, Batch: 63/91, Loss: 3.824660539627075\n",
            "Epoch: 24/30, Batch: 64/91, Loss: 3.7986483573913574\n",
            "Epoch: 24/30, Batch: 65/91, Loss: 3.91552472114563\n",
            "Epoch: 24/30, Batch: 66/91, Loss: 4.160802364349365\n",
            "Epoch: 24/30, Batch: 67/91, Loss: 4.540488243103027\n",
            "Epoch: 24/30, Batch: 68/91, Loss: 4.564192295074463\n",
            "Epoch: 24/30, Batch: 69/91, Loss: 3.456935405731201\n",
            "Epoch: 24/30, Batch: 70/91, Loss: 4.097568988800049\n",
            "Epoch: 24/30, Batch: 71/91, Loss: 4.40838098526001\n",
            "Epoch: 24/30, Batch: 72/91, Loss: 3.812997579574585\n",
            "Epoch: 24/30, Batch: 73/91, Loss: 4.4307541847229\n",
            "Epoch: 24/30, Batch: 74/91, Loss: 4.239951133728027\n",
            "Epoch: 24/30, Batch: 75/91, Loss: 4.135822772979736\n",
            "Epoch: 24/30, Batch: 76/91, Loss: 3.313054323196411\n",
            "Epoch: 24/30, Batch: 77/91, Loss: 4.764071941375732\n",
            "Epoch: 24/30, Batch: 78/91, Loss: 4.454368591308594\n",
            "Epoch: 24/30, Batch: 79/91, Loss: 3.3899590969085693\n",
            "Epoch: 24/30, Batch: 80/91, Loss: 4.256960391998291\n",
            "Epoch: 24/30, Batch: 81/91, Loss: 3.645655870437622\n",
            "Epoch: 24/30, Batch: 82/91, Loss: 4.753898620605469\n",
            "Epoch: 24/30, Batch: 83/91, Loss: 4.2476806640625\n",
            "Epoch: 24/30, Batch: 84/91, Loss: 3.712474822998047\n",
            "Epoch: 24/30, Batch: 85/91, Loss: 4.027710914611816\n",
            "Epoch: 24/30, Batch: 86/91, Loss: 3.132164716720581\n",
            "Epoch: 24/30, Batch: 87/91, Loss: 4.3131103515625\n",
            "Epoch: 24/30, Batch: 88/91, Loss: 4.093209266662598\n",
            "Epoch: 24/30, Batch: 89/91, Loss: 4.083742618560791\n",
            "Epoch: 24/30, Batch: 90/91, Loss: 5.036308288574219\n",
            "Epoch: 25/30, Batch: 0/91, Loss: 3.951108694076538\n",
            "Epoch: 25/30, Batch: 1/91, Loss: 4.278926849365234\n",
            "Epoch: 25/30, Batch: 2/91, Loss: 4.551175594329834\n",
            "Epoch: 25/30, Batch: 3/91, Loss: 4.879664421081543\n",
            "Epoch: 25/30, Batch: 4/91, Loss: 3.4267985820770264\n",
            "Epoch: 25/30, Batch: 5/91, Loss: 2.823575019836426\n",
            "Epoch: 25/30, Batch: 6/91, Loss: 4.70303201675415\n",
            "Epoch: 25/30, Batch: 7/91, Loss: 4.503733158111572\n",
            "Epoch: 25/30, Batch: 8/91, Loss: 4.433515548706055\n",
            "Epoch: 25/30, Batch: 9/91, Loss: 4.329683303833008\n",
            "Epoch: 25/30, Batch: 10/91, Loss: 4.061099529266357\n",
            "Epoch: 25/30, Batch: 11/91, Loss: 4.275242328643799\n",
            "Epoch: 25/30, Batch: 12/91, Loss: 3.5178439617156982\n",
            "Epoch: 25/30, Batch: 13/91, Loss: 4.06185245513916\n",
            "Epoch: 25/30, Batch: 14/91, Loss: 4.099816799163818\n",
            "Epoch: 25/30, Batch: 15/91, Loss: 3.489849328994751\n",
            "Epoch: 25/30, Batch: 16/91, Loss: 3.5150580406188965\n",
            "Epoch: 25/30, Batch: 17/91, Loss: 4.566934585571289\n",
            "Epoch: 25/30, Batch: 18/91, Loss: 4.470846652984619\n",
            "Epoch: 25/30, Batch: 19/91, Loss: 3.6256327629089355\n",
            "Epoch: 25/30, Batch: 20/91, Loss: 4.760860443115234\n",
            "Epoch: 25/30, Batch: 21/91, Loss: 4.3695526123046875\n",
            "Epoch: 25/30, Batch: 22/91, Loss: 5.0609517097473145\n",
            "Epoch: 25/30, Batch: 23/91, Loss: 4.69849157333374\n",
            "Epoch: 25/30, Batch: 24/91, Loss: 4.026615619659424\n",
            "Epoch: 25/30, Batch: 25/91, Loss: 3.8913843631744385\n",
            "Epoch: 25/30, Batch: 26/91, Loss: 4.355163097381592\n",
            "Epoch: 25/30, Batch: 27/91, Loss: 2.6982421875\n",
            "Epoch: 25/30, Batch: 28/91, Loss: 2.0471303462982178\n",
            "Epoch: 25/30, Batch: 29/91, Loss: 4.252727508544922\n",
            "Epoch: 25/30, Batch: 30/91, Loss: 4.71333122253418\n",
            "Epoch: 25/30, Batch: 31/91, Loss: 4.633218765258789\n",
            "Epoch: 25/30, Batch: 32/91, Loss: 4.05035400390625\n",
            "Epoch: 25/30, Batch: 33/91, Loss: 3.966600179672241\n",
            "Epoch: 25/30, Batch: 34/91, Loss: 4.651978969573975\n",
            "Epoch: 25/30, Batch: 35/91, Loss: 4.086000919342041\n",
            "Epoch: 25/30, Batch: 36/91, Loss: 4.722134590148926\n",
            "Epoch: 25/30, Batch: 37/91, Loss: 3.332119941711426\n",
            "Epoch: 25/30, Batch: 38/91, Loss: 4.169166564941406\n",
            "Epoch: 25/30, Batch: 39/91, Loss: 3.6497910022735596\n",
            "Epoch: 25/30, Batch: 40/91, Loss: 4.449576377868652\n",
            "Epoch: 25/30, Batch: 41/91, Loss: 4.304662227630615\n",
            "Epoch: 25/30, Batch: 42/91, Loss: 3.278632879257202\n",
            "Epoch: 25/30, Batch: 43/91, Loss: 4.782456874847412\n",
            "Epoch: 25/30, Batch: 44/91, Loss: 4.319215297698975\n",
            "Epoch: 25/30, Batch: 45/91, Loss: 3.8946597576141357\n",
            "Epoch: 25/30, Batch: 46/91, Loss: 3.5108156204223633\n",
            "Epoch: 25/30, Batch: 47/91, Loss: 4.746312141418457\n",
            "Epoch: 25/30, Batch: 48/91, Loss: 3.7181270122528076\n",
            "Epoch: 25/30, Batch: 49/91, Loss: 4.322511196136475\n",
            "Epoch: 25/30, Batch: 50/91, Loss: 4.490446090698242\n",
            "Epoch: 25/30, Batch: 51/91, Loss: 3.5284321308135986\n",
            "Epoch: 25/30, Batch: 52/91, Loss: 3.7182388305664062\n",
            "Epoch: 25/30, Batch: 53/91, Loss: 4.247442722320557\n",
            "Epoch: 25/30, Batch: 54/91, Loss: 4.836725234985352\n",
            "Epoch: 25/30, Batch: 55/91, Loss: 3.9909772872924805\n",
            "Epoch: 25/30, Batch: 56/91, Loss: 4.781542778015137\n",
            "Epoch: 25/30, Batch: 57/91, Loss: 3.538646697998047\n",
            "Epoch: 25/30, Batch: 58/91, Loss: 4.330788612365723\n",
            "Epoch: 25/30, Batch: 59/91, Loss: 4.0714192390441895\n",
            "Epoch: 25/30, Batch: 60/91, Loss: 4.659348964691162\n",
            "Epoch: 25/30, Batch: 61/91, Loss: 3.767627000808716\n",
            "Epoch: 25/30, Batch: 62/91, Loss: 4.124520301818848\n",
            "Epoch: 25/30, Batch: 63/91, Loss: 4.253844261169434\n",
            "Epoch: 25/30, Batch: 64/91, Loss: 3.649963140487671\n",
            "Epoch: 25/30, Batch: 65/91, Loss: 3.8783910274505615\n",
            "Epoch: 25/30, Batch: 66/91, Loss: 3.8485217094421387\n",
            "Epoch: 25/30, Batch: 67/91, Loss: 4.497170925140381\n",
            "Epoch: 25/30, Batch: 68/91, Loss: 3.846637010574341\n",
            "Epoch: 25/30, Batch: 69/91, Loss: 3.8931119441986084\n",
            "Epoch: 25/30, Batch: 70/91, Loss: 4.598487854003906\n",
            "Epoch: 25/30, Batch: 71/91, Loss: 4.017171859741211\n",
            "Epoch: 25/30, Batch: 72/91, Loss: 3.462559700012207\n",
            "Epoch: 25/30, Batch: 73/91, Loss: 3.8969650268554688\n",
            "Epoch: 25/30, Batch: 74/91, Loss: 4.43794584274292\n",
            "Epoch: 25/30, Batch: 75/91, Loss: 3.7351648807525635\n",
            "Epoch: 25/30, Batch: 76/91, Loss: 3.1360435485839844\n",
            "Epoch: 25/30, Batch: 77/91, Loss: 3.7530159950256348\n",
            "Epoch: 25/30, Batch: 78/91, Loss: 3.7157509326934814\n",
            "Epoch: 25/30, Batch: 79/91, Loss: 4.31320333480835\n",
            "Epoch: 25/30, Batch: 80/91, Loss: 4.607719898223877\n",
            "Epoch: 25/30, Batch: 81/91, Loss: 3.785534381866455\n",
            "Epoch: 25/30, Batch: 82/91, Loss: 4.598654747009277\n",
            "Epoch: 25/30, Batch: 83/91, Loss: 4.881099700927734\n",
            "Epoch: 25/30, Batch: 84/91, Loss: 3.9734902381896973\n",
            "Epoch: 25/30, Batch: 85/91, Loss: 4.239267349243164\n",
            "Epoch: 25/30, Batch: 86/91, Loss: 3.9343087673187256\n",
            "Epoch: 25/30, Batch: 87/91, Loss: 4.383613586425781\n",
            "Epoch: 25/30, Batch: 88/91, Loss: 3.2367258071899414\n",
            "Epoch: 25/30, Batch: 89/91, Loss: 3.5444421768188477\n",
            "Epoch: 25/30, Batch: 90/91, Loss: 4.619683265686035\n",
            "Epoch: 26/30, Batch: 0/91, Loss: 2.832881212234497\n",
            "Epoch: 26/30, Batch: 1/91, Loss: 3.686002254486084\n",
            "Epoch: 26/30, Batch: 2/91, Loss: 2.027984142303467\n",
            "Epoch: 26/30, Batch: 3/91, Loss: 4.522364139556885\n",
            "Epoch: 26/30, Batch: 4/91, Loss: 4.477723121643066\n",
            "Epoch: 26/30, Batch: 5/91, Loss: 3.340261936187744\n",
            "Epoch: 26/30, Batch: 6/91, Loss: 4.641010761260986\n",
            "Epoch: 26/30, Batch: 7/91, Loss: 4.185427188873291\n",
            "Epoch: 26/30, Batch: 8/91, Loss: 4.660467624664307\n",
            "Epoch: 26/30, Batch: 9/91, Loss: 3.6032562255859375\n",
            "Epoch: 26/30, Batch: 10/91, Loss: 4.5610551834106445\n",
            "Epoch: 26/30, Batch: 11/91, Loss: 3.7190725803375244\n",
            "Epoch: 26/30, Batch: 12/91, Loss: 4.730921745300293\n",
            "Epoch: 26/30, Batch: 13/91, Loss: 3.8158977031707764\n",
            "Epoch: 26/30, Batch: 14/91, Loss: 3.9326846599578857\n",
            "Epoch: 26/30, Batch: 15/91, Loss: 4.810612201690674\n",
            "Epoch: 26/30, Batch: 16/91, Loss: 3.6977951526641846\n",
            "Epoch: 26/30, Batch: 17/91, Loss: 4.425871849060059\n",
            "Epoch: 26/30, Batch: 18/91, Loss: 3.58170223236084\n",
            "Epoch: 26/30, Batch: 19/91, Loss: 4.86325740814209\n",
            "Epoch: 26/30, Batch: 20/91, Loss: 3.8922951221466064\n",
            "Epoch: 26/30, Batch: 21/91, Loss: 4.4397783279418945\n",
            "Epoch: 26/30, Batch: 22/91, Loss: 4.162609100341797\n",
            "Epoch: 26/30, Batch: 23/91, Loss: 3.998302936553955\n",
            "Epoch: 26/30, Batch: 24/91, Loss: 4.439364433288574\n",
            "Epoch: 26/30, Batch: 25/91, Loss: 3.9590208530426025\n",
            "Epoch: 26/30, Batch: 26/91, Loss: 3.5593576431274414\n",
            "Epoch: 26/30, Batch: 27/91, Loss: 3.861243486404419\n",
            "Epoch: 26/30, Batch: 28/91, Loss: 4.000482082366943\n",
            "Epoch: 26/30, Batch: 29/91, Loss: 3.874199390411377\n",
            "Epoch: 26/30, Batch: 30/91, Loss: 4.429581165313721\n",
            "Epoch: 26/30, Batch: 31/91, Loss: 4.391056537628174\n",
            "Epoch: 26/30, Batch: 32/91, Loss: 4.288194179534912\n",
            "Epoch: 26/30, Batch: 33/91, Loss: 4.058506965637207\n",
            "Epoch: 26/30, Batch: 34/91, Loss: 3.639653444290161\n",
            "Epoch: 26/30, Batch: 35/91, Loss: 4.341624736785889\n",
            "Epoch: 26/30, Batch: 36/91, Loss: 4.06536340713501\n",
            "Epoch: 26/30, Batch: 37/91, Loss: 4.938265800476074\n",
            "Epoch: 26/30, Batch: 38/91, Loss: 4.75349760055542\n",
            "Epoch: 26/30, Batch: 39/91, Loss: 2.847386598587036\n",
            "Epoch: 26/30, Batch: 40/91, Loss: 4.446458339691162\n",
            "Epoch: 26/30, Batch: 41/91, Loss: 4.654914379119873\n",
            "Epoch: 26/30, Batch: 42/91, Loss: 4.320548057556152\n",
            "Epoch: 26/30, Batch: 43/91, Loss: 3.307246685028076\n",
            "Epoch: 26/30, Batch: 44/91, Loss: 4.167407512664795\n",
            "Epoch: 26/30, Batch: 45/91, Loss: 4.227797031402588\n",
            "Epoch: 26/30, Batch: 46/91, Loss: 4.242401123046875\n",
            "Epoch: 26/30, Batch: 47/91, Loss: 4.5570173263549805\n",
            "Epoch: 26/30, Batch: 48/91, Loss: 4.258488178253174\n",
            "Epoch: 26/30, Batch: 49/91, Loss: 2.8524672985076904\n",
            "Epoch: 26/30, Batch: 50/91, Loss: 4.591080188751221\n",
            "Epoch: 26/30, Batch: 51/91, Loss: 4.326372146606445\n",
            "Epoch: 26/30, Batch: 52/91, Loss: 2.5896081924438477\n",
            "Epoch: 26/30, Batch: 53/91, Loss: 3.6857776641845703\n",
            "Epoch: 26/30, Batch: 54/91, Loss: 4.894181251525879\n",
            "Epoch: 26/30, Batch: 55/91, Loss: 4.284782886505127\n",
            "Epoch: 26/30, Batch: 56/91, Loss: 3.4818413257598877\n",
            "Epoch: 26/30, Batch: 57/91, Loss: 4.296685218811035\n",
            "Epoch: 26/30, Batch: 58/91, Loss: 4.3581929206848145\n",
            "Epoch: 26/30, Batch: 59/91, Loss: 4.05493688583374\n",
            "Epoch: 26/30, Batch: 60/91, Loss: 4.556951999664307\n",
            "Epoch: 26/30, Batch: 61/91, Loss: 4.401185512542725\n",
            "Epoch: 26/30, Batch: 62/91, Loss: 3.815276861190796\n",
            "Epoch: 26/30, Batch: 63/91, Loss: 3.9918627738952637\n",
            "Epoch: 26/30, Batch: 64/91, Loss: 3.988408327102661\n",
            "Epoch: 26/30, Batch: 65/91, Loss: 3.720857858657837\n",
            "Epoch: 26/30, Batch: 66/91, Loss: 4.935127258300781\n",
            "Epoch: 26/30, Batch: 67/91, Loss: 4.209490776062012\n",
            "Epoch: 26/30, Batch: 68/91, Loss: 4.706076145172119\n",
            "Epoch: 26/30, Batch: 69/91, Loss: 3.782402992248535\n",
            "Epoch: 26/30, Batch: 70/91, Loss: 4.125069618225098\n",
            "Epoch: 26/30, Batch: 71/91, Loss: 4.62265157699585\n",
            "Epoch: 26/30, Batch: 72/91, Loss: 4.404396057128906\n",
            "Epoch: 26/30, Batch: 73/91, Loss: 4.388278484344482\n",
            "Epoch: 26/30, Batch: 74/91, Loss: 4.478056907653809\n",
            "Epoch: 26/30, Batch: 75/91, Loss: 2.6605279445648193\n",
            "Epoch: 26/30, Batch: 76/91, Loss: 3.7513034343719482\n",
            "Epoch: 26/30, Batch: 77/91, Loss: 4.202115058898926\n",
            "Epoch: 26/30, Batch: 78/91, Loss: 4.334191799163818\n",
            "Epoch: 26/30, Batch: 79/91, Loss: 3.4077744483947754\n",
            "Epoch: 26/30, Batch: 80/91, Loss: 4.597097396850586\n",
            "Epoch: 26/30, Batch: 81/91, Loss: 3.667553663253784\n",
            "Epoch: 26/30, Batch: 82/91, Loss: 3.8076558113098145\n",
            "Epoch: 26/30, Batch: 83/91, Loss: 4.390956878662109\n",
            "Epoch: 26/30, Batch: 84/91, Loss: 4.193225383758545\n",
            "Epoch: 26/30, Batch: 85/91, Loss: 3.938598155975342\n",
            "Epoch: 26/30, Batch: 86/91, Loss: 3.7716915607452393\n",
            "Epoch: 26/30, Batch: 87/91, Loss: 4.244093894958496\n",
            "Epoch: 26/30, Batch: 88/91, Loss: 2.7582967281341553\n",
            "Epoch: 26/30, Batch: 89/91, Loss: 4.598434925079346\n",
            "Epoch: 26/30, Batch: 90/91, Loss: 3.9502627849578857\n",
            "Epoch: 27/30, Batch: 0/91, Loss: 4.36144495010376\n",
            "Epoch: 27/30, Batch: 1/91, Loss: 4.291212558746338\n",
            "Epoch: 27/30, Batch: 2/91, Loss: 4.060337543487549\n",
            "Epoch: 27/30, Batch: 3/91, Loss: 4.225907802581787\n",
            "Epoch: 27/30, Batch: 4/91, Loss: 4.23389196395874\n",
            "Epoch: 27/30, Batch: 5/91, Loss: 4.267971992492676\n",
            "Epoch: 27/30, Batch: 6/91, Loss: 3.910890817642212\n",
            "Epoch: 27/30, Batch: 7/91, Loss: 4.274306297302246\n",
            "Epoch: 27/30, Batch: 8/91, Loss: 4.887562274932861\n",
            "Epoch: 27/30, Batch: 9/91, Loss: 4.467457294464111\n",
            "Epoch: 27/30, Batch: 10/91, Loss: 4.002305030822754\n",
            "Epoch: 27/30, Batch: 11/91, Loss: 4.429601669311523\n",
            "Epoch: 27/30, Batch: 12/91, Loss: 3.7683231830596924\n",
            "Epoch: 27/30, Batch: 13/91, Loss: 4.538100719451904\n",
            "Epoch: 27/30, Batch: 14/91, Loss: 3.3877530097961426\n",
            "Epoch: 27/30, Batch: 15/91, Loss: 3.1916329860687256\n",
            "Epoch: 27/30, Batch: 16/91, Loss: 3.515131950378418\n",
            "Epoch: 27/30, Batch: 17/91, Loss: 3.7230961322784424\n",
            "Epoch: 27/30, Batch: 18/91, Loss: 3.891458034515381\n",
            "Epoch: 27/30, Batch: 19/91, Loss: 4.178893566131592\n",
            "Epoch: 27/30, Batch: 20/91, Loss: 4.668076038360596\n",
            "Epoch: 27/30, Batch: 21/91, Loss: 4.34335470199585\n",
            "Epoch: 27/30, Batch: 22/91, Loss: 3.9063198566436768\n",
            "Epoch: 27/30, Batch: 23/91, Loss: 3.6091489791870117\n",
            "Epoch: 27/30, Batch: 24/91, Loss: 4.7728400230407715\n",
            "Epoch: 27/30, Batch: 25/91, Loss: 4.493897438049316\n",
            "Epoch: 27/30, Batch: 26/91, Loss: 3.5197203159332275\n",
            "Epoch: 27/30, Batch: 27/91, Loss: 3.936875104904175\n",
            "Epoch: 27/30, Batch: 28/91, Loss: 3.436344623565674\n",
            "Epoch: 27/30, Batch: 29/91, Loss: 4.520397186279297\n",
            "Epoch: 27/30, Batch: 30/91, Loss: 4.559538841247559\n",
            "Epoch: 27/30, Batch: 31/91, Loss: 3.582508087158203\n",
            "Epoch: 27/30, Batch: 32/91, Loss: 4.143240451812744\n",
            "Epoch: 27/30, Batch: 33/91, Loss: 2.967290163040161\n",
            "Epoch: 27/30, Batch: 34/91, Loss: 4.636725902557373\n",
            "Epoch: 27/30, Batch: 35/91, Loss: 4.3091936111450195\n",
            "Epoch: 27/30, Batch: 36/91, Loss: 4.118731498718262\n",
            "Epoch: 27/30, Batch: 37/91, Loss: 4.870670318603516\n",
            "Epoch: 27/30, Batch: 38/91, Loss: 4.470279693603516\n",
            "Epoch: 27/30, Batch: 39/91, Loss: 4.2306671142578125\n",
            "Epoch: 27/30, Batch: 40/91, Loss: 3.48241925239563\n",
            "Epoch: 27/30, Batch: 41/91, Loss: 4.40162992477417\n",
            "Epoch: 27/30, Batch: 42/91, Loss: 3.5300521850585938\n",
            "Epoch: 27/30, Batch: 43/91, Loss: 4.798813343048096\n",
            "Epoch: 27/30, Batch: 44/91, Loss: 4.185857772827148\n",
            "Epoch: 27/30, Batch: 45/91, Loss: 4.180711269378662\n",
            "Epoch: 27/30, Batch: 46/91, Loss: 4.385349750518799\n",
            "Epoch: 27/30, Batch: 47/91, Loss: 3.9098434448242188\n",
            "Epoch: 27/30, Batch: 48/91, Loss: 4.1639404296875\n",
            "Epoch: 27/30, Batch: 49/91, Loss: 4.4085283279418945\n",
            "Epoch: 27/30, Batch: 50/91, Loss: 4.184346675872803\n",
            "Epoch: 27/30, Batch: 51/91, Loss: 3.228659152984619\n",
            "Epoch: 27/30, Batch: 52/91, Loss: 3.9311606884002686\n",
            "Epoch: 27/30, Batch: 53/91, Loss: 3.495673418045044\n",
            "Epoch: 27/30, Batch: 54/91, Loss: 4.695955753326416\n",
            "Epoch: 27/30, Batch: 55/91, Loss: 4.727852821350098\n",
            "Epoch: 27/30, Batch: 56/91, Loss: 4.326623439788818\n",
            "Epoch: 27/30, Batch: 57/91, Loss: 3.9985876083374023\n",
            "Epoch: 27/30, Batch: 58/91, Loss: 4.661787509918213\n",
            "Epoch: 27/30, Batch: 59/91, Loss: 3.602818727493286\n",
            "Epoch: 27/30, Batch: 60/91, Loss: 3.81059193611145\n",
            "Epoch: 27/30, Batch: 61/91, Loss: 3.531022548675537\n",
            "Epoch: 27/30, Batch: 62/91, Loss: 3.8961715698242188\n",
            "Epoch: 27/30, Batch: 63/91, Loss: 4.699097633361816\n",
            "Epoch: 27/30, Batch: 64/91, Loss: 3.967196464538574\n",
            "Epoch: 27/30, Batch: 65/91, Loss: 4.299831867218018\n",
            "Epoch: 27/30, Batch: 66/91, Loss: 3.7983243465423584\n",
            "Epoch: 27/30, Batch: 67/91, Loss: 3.145115852355957\n",
            "Epoch: 27/30, Batch: 68/91, Loss: 3.8810691833496094\n",
            "Epoch: 27/30, Batch: 69/91, Loss: 4.538141250610352\n",
            "Epoch: 27/30, Batch: 70/91, Loss: 4.009852886199951\n",
            "Epoch: 27/30, Batch: 71/91, Loss: 4.210505962371826\n",
            "Epoch: 27/30, Batch: 72/91, Loss: 4.7568488121032715\n",
            "Epoch: 27/30, Batch: 73/91, Loss: 4.2489399909973145\n",
            "Epoch: 27/30, Batch: 74/91, Loss: 3.816283702850342\n",
            "Epoch: 27/30, Batch: 75/91, Loss: 4.817864894866943\n",
            "Epoch: 27/30, Batch: 76/91, Loss: 4.48261022567749\n",
            "Epoch: 27/30, Batch: 77/91, Loss: 4.028116226196289\n",
            "Epoch: 27/30, Batch: 78/91, Loss: 3.7543089389801025\n",
            "Epoch: 27/30, Batch: 79/91, Loss: 4.345378398895264\n",
            "Epoch: 27/30, Batch: 80/91, Loss: 3.803795099258423\n",
            "Epoch: 27/30, Batch: 81/91, Loss: 4.132063865661621\n",
            "Epoch: 27/30, Batch: 82/91, Loss: 4.414180278778076\n",
            "Epoch: 27/30, Batch: 83/91, Loss: 4.348884582519531\n",
            "Epoch: 27/30, Batch: 84/91, Loss: 4.085775852203369\n",
            "Epoch: 27/30, Batch: 85/91, Loss: 4.105544567108154\n",
            "Epoch: 27/30, Batch: 86/91, Loss: 2.834298610687256\n",
            "Epoch: 27/30, Batch: 87/91, Loss: 4.334524631500244\n",
            "Epoch: 27/30, Batch: 88/91, Loss: 3.7589001655578613\n",
            "Epoch: 27/30, Batch: 89/91, Loss: 2.985121250152588\n",
            "Epoch: 27/30, Batch: 90/91, Loss: 4.802850246429443\n",
            "Epoch: 28/30, Batch: 0/91, Loss: 4.928508758544922\n",
            "Epoch: 28/30, Batch: 1/91, Loss: 4.358534812927246\n",
            "Epoch: 28/30, Batch: 2/91, Loss: 4.364032745361328\n",
            "Epoch: 28/30, Batch: 3/91, Loss: 4.583491325378418\n",
            "Epoch: 28/30, Batch: 4/91, Loss: 3.5025246143341064\n",
            "Epoch: 28/30, Batch: 5/91, Loss: 3.583980083465576\n",
            "Epoch: 28/30, Batch: 6/91, Loss: 5.043327808380127\n",
            "Epoch: 28/30, Batch: 7/91, Loss: 3.6494758129119873\n",
            "Epoch: 28/30, Batch: 8/91, Loss: 3.762042760848999\n",
            "Epoch: 28/30, Batch: 9/91, Loss: 2.4214329719543457\n",
            "Epoch: 28/30, Batch: 10/91, Loss: 4.5858025550842285\n",
            "Epoch: 28/30, Batch: 11/91, Loss: 4.15382194519043\n",
            "Epoch: 28/30, Batch: 12/91, Loss: 2.6436822414398193\n",
            "Epoch: 28/30, Batch: 13/91, Loss: 3.765753746032715\n",
            "Epoch: 28/30, Batch: 14/91, Loss: 3.864591121673584\n",
            "Epoch: 28/30, Batch: 15/91, Loss: 4.545309543609619\n",
            "Epoch: 28/30, Batch: 16/91, Loss: 3.4108333587646484\n",
            "Epoch: 28/30, Batch: 17/91, Loss: 3.8700387477874756\n",
            "Epoch: 28/30, Batch: 18/91, Loss: 4.77116584777832\n",
            "Epoch: 28/30, Batch: 19/91, Loss: 4.258759021759033\n",
            "Epoch: 28/30, Batch: 20/91, Loss: 4.2325873374938965\n",
            "Epoch: 28/30, Batch: 21/91, Loss: 4.807226181030273\n",
            "Epoch: 28/30, Batch: 22/91, Loss: 3.3704147338867188\n",
            "Epoch: 28/30, Batch: 23/91, Loss: 2.7831666469573975\n",
            "Epoch: 28/30, Batch: 24/91, Loss: 2.721512794494629\n",
            "Epoch: 28/30, Batch: 25/91, Loss: 4.711103916168213\n",
            "Epoch: 28/30, Batch: 26/91, Loss: 4.272931098937988\n",
            "Epoch: 28/30, Batch: 27/91, Loss: 4.285552024841309\n",
            "Epoch: 28/30, Batch: 28/91, Loss: 4.012406826019287\n",
            "Epoch: 28/30, Batch: 29/91, Loss: 4.15716028213501\n",
            "Epoch: 28/30, Batch: 30/91, Loss: 4.003236293792725\n",
            "Epoch: 28/30, Batch: 31/91, Loss: 3.7486183643341064\n",
            "Epoch: 28/30, Batch: 32/91, Loss: 4.79849910736084\n",
            "Epoch: 28/30, Batch: 33/91, Loss: 4.604867458343506\n",
            "Epoch: 28/30, Batch: 34/91, Loss: 3.635643720626831\n",
            "Epoch: 28/30, Batch: 35/91, Loss: 3.6663084030151367\n",
            "Epoch: 28/30, Batch: 36/91, Loss: 4.005363941192627\n",
            "Epoch: 28/30, Batch: 37/91, Loss: 4.503198146820068\n",
            "Epoch: 28/30, Batch: 38/91, Loss: 4.067541122436523\n",
            "Epoch: 28/30, Batch: 39/91, Loss: 3.8289847373962402\n",
            "Epoch: 28/30, Batch: 40/91, Loss: 4.470974922180176\n",
            "Epoch: 28/30, Batch: 41/91, Loss: 4.23927116394043\n",
            "Epoch: 28/30, Batch: 42/91, Loss: 3.3101346492767334\n",
            "Epoch: 28/30, Batch: 43/91, Loss: 4.521332740783691\n",
            "Epoch: 28/30, Batch: 44/91, Loss: 4.0376667976379395\n",
            "Epoch: 28/30, Batch: 45/91, Loss: 2.564894914627075\n",
            "Epoch: 28/30, Batch: 46/91, Loss: 3.4243175983428955\n",
            "Epoch: 28/30, Batch: 47/91, Loss: 3.460601568222046\n",
            "Epoch: 28/30, Batch: 48/91, Loss: 3.8717503547668457\n",
            "Epoch: 28/30, Batch: 49/91, Loss: 3.0808167457580566\n",
            "Epoch: 28/30, Batch: 50/91, Loss: 3.8828861713409424\n",
            "Epoch: 28/30, Batch: 51/91, Loss: 4.467015743255615\n",
            "Epoch: 28/30, Batch: 52/91, Loss: 4.907731056213379\n",
            "Epoch: 28/30, Batch: 53/91, Loss: 3.52129864692688\n",
            "Epoch: 28/30, Batch: 54/91, Loss: 3.8505847454071045\n",
            "Epoch: 28/30, Batch: 55/91, Loss: 3.319741725921631\n",
            "Epoch: 28/30, Batch: 56/91, Loss: 4.045291423797607\n",
            "Epoch: 28/30, Batch: 57/91, Loss: 3.963535785675049\n",
            "Epoch: 28/30, Batch: 58/91, Loss: 3.880943536758423\n",
            "Epoch: 28/30, Batch: 59/91, Loss: 4.2193145751953125\n",
            "Epoch: 28/30, Batch: 60/91, Loss: 4.059138774871826\n",
            "Epoch: 28/30, Batch: 61/91, Loss: 4.004943370819092\n",
            "Epoch: 28/30, Batch: 62/91, Loss: 3.924999952316284\n",
            "Epoch: 28/30, Batch: 63/91, Loss: 3.9968042373657227\n",
            "Epoch: 28/30, Batch: 64/91, Loss: 4.545164108276367\n",
            "Epoch: 28/30, Batch: 65/91, Loss: 4.41441535949707\n",
            "Epoch: 28/30, Batch: 66/91, Loss: 4.269312381744385\n",
            "Epoch: 28/30, Batch: 67/91, Loss: 3.604825496673584\n",
            "Epoch: 28/30, Batch: 68/91, Loss: 3.8158276081085205\n",
            "Epoch: 28/30, Batch: 69/91, Loss: 4.403167724609375\n",
            "Epoch: 28/30, Batch: 70/91, Loss: 4.692217826843262\n",
            "Epoch: 28/30, Batch: 71/91, Loss: 3.954716205596924\n",
            "Epoch: 28/30, Batch: 72/91, Loss: 3.9619264602661133\n",
            "Epoch: 28/30, Batch: 73/91, Loss: 4.283186912536621\n",
            "Epoch: 28/30, Batch: 74/91, Loss: 4.233555793762207\n",
            "Epoch: 28/30, Batch: 75/91, Loss: 3.966259002685547\n",
            "Epoch: 28/30, Batch: 76/91, Loss: 4.057923316955566\n",
            "Epoch: 28/30, Batch: 77/91, Loss: 4.324764728546143\n",
            "Epoch: 28/30, Batch: 78/91, Loss: 4.688494682312012\n",
            "Epoch: 28/30, Batch: 79/91, Loss: 3.9243619441986084\n",
            "Epoch: 28/30, Batch: 80/91, Loss: 3.739297389984131\n",
            "Epoch: 28/30, Batch: 81/91, Loss: 3.9207847118377686\n",
            "Epoch: 28/30, Batch: 82/91, Loss: 3.9997401237487793\n",
            "Epoch: 28/30, Batch: 83/91, Loss: 4.134061813354492\n",
            "Epoch: 28/30, Batch: 84/91, Loss: 3.3843321800231934\n",
            "Epoch: 28/30, Batch: 85/91, Loss: 3.428734540939331\n",
            "Epoch: 28/30, Batch: 86/91, Loss: 4.455149173736572\n",
            "Epoch: 28/30, Batch: 87/91, Loss: 4.2159528732299805\n",
            "Epoch: 28/30, Batch: 88/91, Loss: 4.251846790313721\n",
            "Epoch: 28/30, Batch: 89/91, Loss: 4.566164970397949\n",
            "Epoch: 28/30, Batch: 90/91, Loss: 3.9882633686065674\n",
            "Epoch: 29/30, Batch: 0/91, Loss: 3.8463854789733887\n",
            "Epoch: 29/30, Batch: 1/91, Loss: 3.8699541091918945\n",
            "Epoch: 29/30, Batch: 2/91, Loss: 5.167731761932373\n",
            "Epoch: 29/30, Batch: 3/91, Loss: 4.718429088592529\n",
            "Epoch: 29/30, Batch: 4/91, Loss: 4.682986259460449\n",
            "Epoch: 29/30, Batch: 5/91, Loss: 3.5698864459991455\n",
            "Epoch: 29/30, Batch: 6/91, Loss: 3.675443172454834\n",
            "Epoch: 29/30, Batch: 7/91, Loss: 3.4935319423675537\n",
            "Epoch: 29/30, Batch: 8/91, Loss: 3.188098907470703\n",
            "Epoch: 29/30, Batch: 9/91, Loss: 3.6321511268615723\n",
            "Epoch: 29/30, Batch: 10/91, Loss: 4.092345237731934\n",
            "Epoch: 29/30, Batch: 11/91, Loss: 4.890628814697266\n",
            "Epoch: 29/30, Batch: 12/91, Loss: 4.220089912414551\n",
            "Epoch: 29/30, Batch: 13/91, Loss: 4.256109237670898\n",
            "Epoch: 29/30, Batch: 14/91, Loss: 4.607555866241455\n",
            "Epoch: 29/30, Batch: 15/91, Loss: 3.8873867988586426\n",
            "Epoch: 29/30, Batch: 16/91, Loss: 3.8084166049957275\n",
            "Epoch: 29/30, Batch: 17/91, Loss: 4.358611106872559\n",
            "Epoch: 29/30, Batch: 18/91, Loss: 4.462006092071533\n",
            "Epoch: 29/30, Batch: 19/91, Loss: 3.9780750274658203\n",
            "Epoch: 29/30, Batch: 20/91, Loss: 4.351963520050049\n",
            "Epoch: 29/30, Batch: 21/91, Loss: 4.6742167472839355\n",
            "Epoch: 29/30, Batch: 22/91, Loss: 5.0214972496032715\n",
            "Epoch: 29/30, Batch: 23/91, Loss: 3.6493470668792725\n",
            "Epoch: 29/30, Batch: 24/91, Loss: 4.214109897613525\n",
            "Epoch: 29/30, Batch: 25/91, Loss: 4.317232608795166\n",
            "Epoch: 29/30, Batch: 26/91, Loss: 3.8658738136291504\n",
            "Epoch: 29/30, Batch: 27/91, Loss: 3.6391825675964355\n",
            "Epoch: 29/30, Batch: 28/91, Loss: 4.648470878601074\n",
            "Epoch: 29/30, Batch: 29/91, Loss: 4.792559623718262\n",
            "Epoch: 29/30, Batch: 30/91, Loss: 4.261783123016357\n",
            "Epoch: 29/30, Batch: 31/91, Loss: 4.210869312286377\n",
            "Epoch: 29/30, Batch: 32/91, Loss: 3.3947560787200928\n",
            "Epoch: 29/30, Batch: 33/91, Loss: 4.957640171051025\n",
            "Epoch: 29/30, Batch: 34/91, Loss: 4.653280258178711\n",
            "Epoch: 29/30, Batch: 35/91, Loss: 3.242469310760498\n",
            "Epoch: 29/30, Batch: 36/91, Loss: 3.986504077911377\n",
            "Epoch: 29/30, Batch: 37/91, Loss: 4.2221479415893555\n",
            "Epoch: 29/30, Batch: 38/91, Loss: 3.648371458053589\n",
            "Epoch: 29/30, Batch: 39/91, Loss: 4.225719928741455\n",
            "Epoch: 29/30, Batch: 40/91, Loss: 3.043208599090576\n",
            "Epoch: 29/30, Batch: 41/91, Loss: 3.918984889984131\n",
            "Epoch: 29/30, Batch: 42/91, Loss: 4.263885021209717\n",
            "Epoch: 29/30, Batch: 43/91, Loss: 4.551944732666016\n",
            "Epoch: 29/30, Batch: 44/91, Loss: 3.847930669784546\n",
            "Epoch: 29/30, Batch: 45/91, Loss: 4.564838886260986\n",
            "Epoch: 29/30, Batch: 46/91, Loss: 4.469815731048584\n",
            "Epoch: 29/30, Batch: 47/91, Loss: 3.8562605381011963\n",
            "Epoch: 29/30, Batch: 48/91, Loss: 4.02133321762085\n",
            "Epoch: 29/30, Batch: 49/91, Loss: 4.169004917144775\n",
            "Epoch: 29/30, Batch: 50/91, Loss: 4.4621663093566895\n",
            "Epoch: 29/30, Batch: 51/91, Loss: 4.476168155670166\n",
            "Epoch: 29/30, Batch: 52/91, Loss: 4.3576436042785645\n",
            "Epoch: 29/30, Batch: 53/91, Loss: 4.404967308044434\n",
            "Epoch: 29/30, Batch: 54/91, Loss: 4.2998127937316895\n",
            "Epoch: 29/30, Batch: 55/91, Loss: 3.563386917114258\n",
            "Epoch: 29/30, Batch: 56/91, Loss: 2.7660975456237793\n",
            "Epoch: 29/30, Batch: 57/91, Loss: 3.8360307216644287\n",
            "Epoch: 29/30, Batch: 58/91, Loss: 3.842731475830078\n",
            "Epoch: 29/30, Batch: 59/91, Loss: 3.649043083190918\n",
            "Epoch: 29/30, Batch: 60/91, Loss: 4.2025556564331055\n",
            "Epoch: 29/30, Batch: 61/91, Loss: 4.350757598876953\n",
            "Epoch: 29/30, Batch: 62/91, Loss: 3.8382322788238525\n",
            "Epoch: 29/30, Batch: 63/91, Loss: 4.648396968841553\n",
            "Epoch: 29/30, Batch: 64/91, Loss: 2.154222011566162\n",
            "Epoch: 29/30, Batch: 65/91, Loss: 3.630185604095459\n",
            "Epoch: 29/30, Batch: 66/91, Loss: 3.3966245651245117\n",
            "Epoch: 29/30, Batch: 67/91, Loss: 3.8479502201080322\n",
            "Epoch: 29/30, Batch: 68/91, Loss: 4.734582901000977\n",
            "Epoch: 29/30, Batch: 69/91, Loss: 4.142533779144287\n",
            "Epoch: 29/30, Batch: 70/91, Loss: 4.267664909362793\n",
            "Epoch: 29/30, Batch: 71/91, Loss: 4.106600761413574\n",
            "Epoch: 29/30, Batch: 72/91, Loss: 4.498390197753906\n",
            "Epoch: 29/30, Batch: 73/91, Loss: 4.223072528839111\n",
            "Epoch: 29/30, Batch: 74/91, Loss: 3.9760379791259766\n",
            "Epoch: 29/30, Batch: 75/91, Loss: 4.994499206542969\n",
            "Epoch: 29/30, Batch: 76/91, Loss: 3.685739755630493\n",
            "Epoch: 29/30, Batch: 77/91, Loss: 3.3817684650421143\n",
            "Epoch: 29/30, Batch: 78/91, Loss: 3.6742911338806152\n",
            "Epoch: 29/30, Batch: 79/91, Loss: 4.358604907989502\n",
            "Epoch: 29/30, Batch: 80/91, Loss: 3.9287490844726562\n",
            "Epoch: 29/30, Batch: 81/91, Loss: 4.323370933532715\n",
            "Epoch: 29/30, Batch: 82/91, Loss: 4.642742156982422\n",
            "Epoch: 29/30, Batch: 83/91, Loss: 4.62885046005249\n",
            "Epoch: 29/30, Batch: 84/91, Loss: 4.37022066116333\n",
            "Epoch: 29/30, Batch: 85/91, Loss: 3.7260067462921143\n",
            "Epoch: 29/30, Batch: 86/91, Loss: 4.741478443145752\n",
            "Epoch: 29/30, Batch: 87/91, Loss: 4.635552406311035\n",
            "Epoch: 29/30, Batch: 88/91, Loss: 3.745861530303955\n",
            "Epoch: 29/30, Batch: 89/91, Loss: 3.662163257598877\n",
            "Epoch: 29/30, Batch: 90/91, Loss: 2.83573842048645\n"
          ]
        }
      ],
      "source": [
        "train(**experiment_args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}